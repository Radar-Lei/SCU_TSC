---
phase: 01-grpo-core-infrastructure
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - grpo/reward.py
  - grpo/training.py
  - config/grpo_config.yaml
  - grpo/config.py
autonomous: true

must_haves:
  truths:
    - "reward函数链正确组合format和tsc rewards，权重可配置"
    - "完全不遵守format时直接返回format_reward，不计算tsc_reward"
    - "部分遵守或严格format时，final_reward = format_weight * format_reward + tsc_weight * tsc_reward"
    - "训练脚本集成了reward函数链，GRPOTrainer能正确调用"
    - "训练日志显示reward统计信息（format准确率、平均reward）"
  artifacts:
    - path: "grpo/reward.py"
      provides: "完整的reward函数链"
      exports: ["compute_reward", "batch_compute_reward", "RewardChainConfig", "RewardStats"]
    - path: "grpo/training.py"
      provides: "集成reward函数的训练脚本"
      exports: ["train_grpo", "create_reward_function"]
    - path: "config/grpo_config.yaml"
      provides: "完整配置文件"
      contains: ["reward", "format_reward", "tsc_reward"]
  key_links:
    - from: "grpo/training.py"
      to: "grpo/reward.py"
      via: "导入compute_reward并创建reward函数"
      pattern: "from.*reward.*import.*compute_reward|create_reward_function"
    - from: "grpo/training.py"
      to: "grpo/sumo_reward.py"
      via: "导入tsc_reward_fn"
      pattern: "from.*sumo_reward.*import.*tsc_reward_fn"
    - from: "grpo/training.py"
      to: "trl.GRPOTrainer"
      via: "传递reward函数给GRPOTrainer"
      pattern: "GRPOTrainer.*reward"
---

<objective>
实现reward函数链，组合format_reward_fn和tsc_reward_fn，完成GRPO训练脚本的集成。

**Purpose**: 将format验证和TSC评估组合成完整的reward信号，使模型能够同时学习格式正确性和交通控制能力。

**Output**:
- 更新`grpo/reward.py`: 添加reward函数链和批量处理
- 更新`grpo/training.py`: 集成reward函数，完成GRPOTrainer配置
- 更新`config/grpo_config.yaml`: 完善reward相关配置
- 更新`grpo/config.py`: 添加RewardChainConfig

**Reward函数链逻辑**（根据CONTEXT.md决策）:
1. **首先计算format_reward**:
   - 严格format: +1 → 继续计算tsc_reward
   - 部分format: -0.5 → 继续计算tsc_reward
   - 不遵守format: -10 → 直接返回，不计算tsc_reward

2. **然后计算tsc_reward**（仅当format可提取决策时）:
   - 从output提取决策（yes/no）
   - 运行SUMO仿真计算reward
   - 归一化到[-1, 1]

3. **最终组合**:
   - `final_reward = format_weight * format_reward + tsc_weight * tsc_reward`

4. **统计信息**（用于日志）:
   - format准确率（严格+部分/总数）
   - 平均format_reward
   - 平均tsc_reward
   - 平均final_reward
</objective>

<execution_context>
@/home/samuel/.claude/get-shit-done/workflows/execute-plan.md
@/home/samuel/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/01-grpo-core-infrastructure/01-CONTEXT.md
@grpo/reward.py
@grpo/sumo_reward.py
@grpo/training.py
@grpo/config.py
</context>

<tasks>

<task type="auto">
  <name>任务1: 实现reward函数链</name>
  <files>grpo/reward.py</files>
  <action>
在 `grpo/reward.py` 中添加reward函数链：

**数据类**:
```python
@dataclass
class RewardStats:
    """Reward统计信息"""
    total_count: int
    strict_format_count: int
    partial_format_count: int
    invalid_format_count: int
    avg_format_reward: float
    avg_tsc_reward: float
    avg_final_reward: float
    format_accuracy: float  # (strict + partial) / total

@dataclass
class RewardChainConfig:
    """Reward函数链配置"""
    format_weight: float = 1.0
    tsc_weight: float = 1.0
    # Format reward评分
    format_strict: float = 1.0
    format_partial: float = -0.5
    format_invalid: float = -10.0
    # 正则表达式
    extract_regex: str = r'\{["\s]*extend["\s]*:\s*["\s]*(yes|no)["\s]*\}'
```

**单样本reward计算**:
```python
def compute_reward(
    prompt: str,
    output: str,
    state_file: str,
    chain_config: RewardChainConfig,
    sumo_config: Any,
    tsc_reward_fn: Callable = None
) -> Tuple[float, Dict[str, Any]]:
    """
    计算单个样本的reward

    Args:
        prompt: 输入prompt
        output: 模型输出
        state_file: SUMO状态文件路径
        chain_config: Reward函数链配置
        sumo_config: SUMO配置
        tsc_reward_fn: TSC reward函数（可选，用于测试）

    Returns:
        (final_reward, info_dict)
    """
    # 1. 计算format reward
    format_result = format_reward_fn(
        output,
        regex=chain_config.extract_regex,
        strict_reward=chain_config.format_strict,
        partial_reward=chain_config.format_partial,
        invalid_reward=chain_config.format_invalid
    )

    # 2. 如果完全不遵守format，直接返回
    if not format_result.is_partial and not format_result.is_strict:
        return format_result.reward, {
            "format_reward": format_result.reward,
            "tsc_reward": 0.0,
            "reason": "invalid_format"
        }

    # 3. 计算TSC reward
    if tsc_reward_fn is None:
        from .sumo_reward import calculate_tsc_reward_single
        tsc_reward_fn = calculate_tsc_reward_single

    tsc_result = tsc_reward_fn(state_file, prompt, output, sumo_config)
    tsc_reward = tsc_result.reward if tsc_result.success else 0.0

    # 4. 组合reward
    final_reward = (
        chain_config.format_weight * format_result.reward +
        chain_config.tsc_weight * tsc_reward
    )

    return final_reward, {
        "format_reward": format_result.reward,
        "tsc_reward": tsc_reward,
        "is_strict": format_result.is_strict,
        "is_partial": format_result.is_partial,
    }
```

**批量reward计算**（GRPOTrainer接口）:
```python
def batch_compute_reward(
    prompts: List[str],
    outputs: List[str],
    state_files: List[str],
    chain_config: RewardChainConfig,
    sumo_config: Any
) -> Tuple[List[float], RewardStats]:
    """
    批量计算reward（GRPOTrainer调用接口）

    Args:
        prompts: 输入prompt列表
        outputs: 模型输出列表
        state_files: 状态文件路径列表
        chain_config: Reward函数链配置
        sumo_config: SUMO配置

    Returns:
        (rewards列表, 统计信息)
    """
    from .sumo_reward import ParallelSUMORewardCalculator

    # 先计算所有format reward
    format_results = [
        format_reward_fn(
            output,
            regex=chain_config.extract_regex,
            strict_reward=chain_config.format_strict,
            partial_reward=chain_config.format_partial,
            invalid_reward=chain_config.format_invalid
        )
        for output in outputs
    ]

    # 筛选需要计算TSC的样本
    needs_tsc_indices = [
        i for i, r in enumerate(format_results)
        if r.is_strict or r.is_partial
    ]

    # 批量计算TSC reward（使用并行）
    tsc_rewards = [0.0] * len(outputs)
    if needs_tsc_indices:
        calculator = ParallelSUMORewardCalculator(max_workers=sumo_config.max_workers)
        tsc_results = calculator.calculate_batch(
            prompts=[prompts[i] for i in needs_tsc_indices],
            outputs=[outputs[i] for i in needs_tsc_indices],
            state_files=[state_files[i] for i in needs_tsc_indices],
            config=sumo_config
        )
        for idx, reward in zip(needs_tsc_indices, tsc_results):
            tsc_rewards[idx] = reward

    # 组合最终reward
    final_rewards = []
    for i, format_result in enumerate(format_results):
        final_reward = (
            chain_config.format_weight * format_result.reward +
            chain_config.tsc_weight * tsc_rewards[i]
        )
        final_rewards.append(final_reward)

    # 计算统计信息
    stats = RewardStats(
        total_count=len(outputs),
        strict_format_count=sum(1 for r in format_results if r.is_strict),
        partial_format_count=sum(1 for r in format_results if r.is_partial),
        invalid_format_count=sum(1 for r in format_results if not r.is_strict and not r.is_partial),
        avg_format_reward=sum(r.reward for r in format_results) / len(outputs),
        avg_tsc_reward=sum(tsc_rewards) / len(outputs),
        avg_final_reward=sum(final_rewards) / len(outputs),
        format_accuracy=(sum(1 for r in format_results if r.is_strict or r.is_partial) / len(outputs))
    )

    return final_rewards, stats
```

**导入**:
```python
from typing import List, Tuple, Callable, Dict, Any
```
  </action>
  <verify>
```bash
# 测试函数可以导入
python3 -c "
from grpo.reward import compute_reward, batch_compute_reward, RewardChainConfig, RewardStats
print('Import OK')
"

# 测试单样本计算（不运行SUMO）
python3 -c "
from grpo.reward import compute_reward, RewardChainConfig
from types import SimpleNamespace

config = RewardChainConfig()
sumo_config = SimpleNamespace(max_workers=4, extend_seconds=5, reward_scale=10.0)

# Mock tsc_reward_fn
def mock_tsc_reward(state_file, prompt, output, config):
    from types import SimpleNamespace
    return SimpleNamespace(reward=0.5, success=True)

reward, info = compute_reward(
    prompt='{\"state\": {\"current_phase_id\": 0}}',
    output='{\"extend\": \"yes\"}',
    state_file='dummy.xml',
    chain_config=config,
    sumo_config=sumo_config,
    tsc_reward_fn=mock_tsc_reward
)
print(f'Reward: {reward}, Info: {info}')
assert 'format_reward' in info and 'tsc_reward' in info
"
```
  </verify>
  <done>
reward函数链实现完成，能够正确组合format和tsc rewards。
  </done>
</task>

<task type="auto">
  <name>任务2: 集成reward函数到训练脚本</name>
  <files>grpo/training.py</files>
  <action>
更新 `grpo/training.py`，集成reward函数链：

**1. 添加reward函数创建器**:
```python
def create_reward_function(
    chain_config: RewardChainConfig,
    sumo_config: SUMOConfig,
    dataset: Dataset
) -> Callable:
    """
    创建GRPOTrainer使用的reward函数

    GRPOTrainer期望的签名: (prompts, outputs, **kwargs) -> List[float]

    Args:
        chain_config: Reward函数链配置
        sumo_config: SUMO配置
        dataset: 数据集（用于获取state_files）

    Returns:
        reward函数
    """
    # 预加载state_files（按数据集顺序）
    state_files = dataset["state_file"]

    def reward_fn(prompts: List[str], outputs: List[str], **kwargs) -> List[float]:
        """
        GRPOTrainer调用的reward函数
        """
        # 确保prompts和state_files长度匹配
        # 注意: GRPO可能对每个prompt生成多个output，需要正确对齐
        n = len(outputs)
        aligned_state_files = state_files[:n] if len(state_files) >= n else state_files

        rewards, stats = batch_compute_reward(
            prompts=prompts[:n],
            outputs=outputs,
            state_files=aligned_state_files,
            chain_config=chain_config,
            sumo_config=sumo_config
        )

        # 打印统计信息
        print(f"\n{'='*50}")
        print(f"Reward Statistics:")
        print(f"  Total: {stats.total_count}")
        print(f"  Format accuracy: {stats.format_accuracy:.1%}")
        print(f"  Strict: {stats.strict_format_count}, Partial: {stats.partial_format_count}, Invalid: {stats.invalid_format_count}")
        print(f"  Avg format reward: {stats.avg_format_reward:.3f}")
        print(f"  Avg TSC reward: {stats.avg_tsc_reward:.3f}")
        print(f"  Avg final reward: {stats.avg_final_reward:.3f}")
        print(f"{'='*50}\n")

        return rewards

    return reward_fn
```

**2. 更新train_grpo函数**:
```python
def train_grpo(config: GRPOTrainingConfig):
    # ... 现有的模型加载代码 ...

    # 加载数据集
    train_dataset = load_grpo_dataset(config.dataset_path)
    print(f"加载了 {len(train_dataset)} 条训练数据")

    # 创建reward函数
    reward_fn = create_reward_function(
        chain_config=config.reward,
        sumo_config=config.sumo,
        dataset=train_dataset
    )

    # 配置GRPO
    grpo_config = GRPOConfig(
        output_dir=config.output_dir,
        learning_rate=config.learning_rate,
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        num_generations=config.num_generations,
        temperature=config.temperature,
        kl_coeff=config.kl_coeff,
        max_new_tokens=config.max_new_tokens,
        top_p=config.top_p,
        repetition_penalty=config.repetition_penalty,
        # 训练参数
        num_train_epochs=config.num_train_epochs,
        warmup_steps=config.warmup_steps,
        logging_steps=config.logging_steps,
        save_steps=config.save_steps,
        optim=config.optim,
        report_to="none",
        save_total_limit=2,
    )

    # 创建训练器
    trainer = GRPOTrainer(
        model=model,
        reward_funcs=reward_fn,  # 传入reward函数
        args=grpo_config,
        train_dataset=train_dataset,
    )

    # 开始训练
    print("\n开始GRPO训练...")
    trainer.train()

    # 保存模型
    print(f"\n正在保存模型到 {config.output_dir}...")
    model.save_pretrained(config.output_dir)
    tokenizer.save_pretrained(config.output_dir)

    # 保存merged模型
    merged_dir = os.path.join(config.output_dir, "merged")
    print(f"正在保存合并后的模型到 {merged_dir}...")
    model.save_pretrained_merged(merged_dir, tokenizer, save_method="merged_16bit")

    print("\nGRPO训练完成!")
    print(f"LoRA模型: {config.output_dir}")
    print(f"合并模型: {merged_dir}")
```

**3. 更新导入**:
```python
from grpo.reward import compute_reward, batch_compute_reward, RewardChainConfig, RewardStats
from grpo.sumo_reward import ParallelSUMORewardCalculator
from grpo.config import GRPOTrainingConfig, SUMOConfig, RewardChainConfig as ConfigRewardChainConfig
```
  </action>
  <verify>
```bash
# 测试更新后的训练脚本可以导入
python3 -c "
from grpo.training import train_grpo, create_reward_function, load_grpo_dataset
print('Import OK')
"

# 检查create_reward_function存在
python3 -c "
from grpo.training import create_reward_function
import inspect
sig = inspect.signature(create_reward_function)
print(f'create_reward_function params: {list(sig.parameters.keys())}')
"
```
  </verify>
  <done>
训练脚本更新完成，reward函数正确集成到GRPOTrainer中。
  </done>
</task>

<task type="auto">
  <name>任务3: 完善配置文件和配置类</name>
  <files>config/grpo_config.yaml, grpo/config.py</files>
  <action>
1. 完善 `config/grpo_config.yaml`：
```yaml
# Reward函数链配置
reward:
  format_weight: 1.0
  tsc_weight: 1.0

# Format reward配置
format_reward:
  strict: 1.0
  partial: -0.5
  invalid: -10.0
  extract_regex: '\{["\s]*extend["\s]*:\s*["\s]*(yes|no)["\s]*\}'

# SUMO仿真配置
sumo:
  max_workers: 4
  port_range: [10000, 60000]
  extend_seconds: 5
  reward_scale: 10.0

# ... 其他配置 ...
```

2. 更新 `grpo/config.py`：
- 确保 GRPOTrainingConfig 包含所有嵌套配置
- 确保 from_yaml() 正确解析所有配置段
- 添加参数验证

**完整配置类结构**:
```python
@dataclass
class FormatRewardConfig:
    strict: float = 1.0
    partial: float = -0.5
    invalid: float = -10.0
    extract_regex: str = r'\{["\s]*extend["\s]*:\s*["\s]*(yes|no)["\s]*\}'

@dataclass
class SUMOConfig:
    max_workers: int = 4
    port_range: List[int] = field(default_factory=lambda: [10000, 60000])
    extend_seconds: int = 5
    reward_scale: float = 10.0

@dataclass
class RewardChainConfig:
    format_weight: float = 1.0
    tsc_weight: float = 1.0

@dataclass
class GRPOTrainingConfig:
    # 模型配置
    model_path: str
    max_seq_length: int = 2048

    # GRPO参数
    learning_rate: float = 1e-5
    batch_size: int = 2
    gradient_accumulation_steps: int = 4
    num_generations: int = 4
    temperature: float = 0.9
    kl_coeff: float = 0.1

    # 生成参数
    max_new_tokens: int = 50
    top_p: float = 0.9
    repetition_penalty: float = 1.0

    # 训练参数
    num_train_epochs: int = 3
    warmup_steps: int = 10
    logging_steps: int = 5
    save_steps: int = 50
    optim: str = "adamw_8bit"

    # 数据路径
    dataset_path: str = "/home/samuel/SCU_TSC/data/grpo_datasets"
    output_dir: str = "/home/samuel/SCU_TSC/model/grpo_model"

    # 嵌套配置
    reward: RewardChainConfig = field(default_factory=RewardChainConfig)
    format_reward: FormatRewardConfig = field(default_factory=FormatRewardConfig)
    sumo: SUMOConfig = field(default_factory=SUMOConfig)

    @classmethod
    def from_yaml(cls, path: str) -> "GRPOTrainingConfig":
        """从YAML文件加载配置"""
        import yaml
        with open(path, 'r') as f:
            data = yaml.safe_load(f)

        # 处理嵌套配置
        reward_data = data.pop('reward', {})
        format_data = data.pop('format_reward', {})
        sumo_data = data.pop('sumo', {})

        return cls(
            reward=RewardChainConfig(**reward_data),
            format_reward=FormatRewardConfig(**format_data),
            sumo=SUMOConfig(**sumo_data),
            **data
        )
```
  </action>
  <verify>
```bash
# 检查配置文件完整性
python3 -c "
from grpo.config import GRPOTrainingConfig
config = GRPOTrainingConfig.from_yaml('config/grpo_config.yaml')
print(f'model_path: {config.model_path}')
print(f'reward.format_weight: {config.reward.format_weight}')
print(f'format_reward.strict: {config.format_reward.strict}')
print(f'sumo.max_workers: {config.sumo.max_workers}')
"
```
  </verify>
  <done>
配置文件和配置类完善，所有参数可以正确加载。
  </done>
</task>

</tasks>

<verification>
**整体验证**:
1. reward函数链能正确组合format和tsc rewards
2. 完全不遵守format时不计算tsc_reward
3. 训练脚本能创建GRPOTrainer所需的reward函数
4. 配置文件包含所有必需参数
5. 模块可以正确导入

**测试流程**:
```bash
# 1. 导入测试
python3 -c "from grpo.training import train_grpo; print('OK')"

# 2. 配置加载测试
python3 -c "from grpo.config import GRPOTrainingConfig; c = GRPOTrainingConfig.from_yaml('config/grpo_config.yaml'); print('OK')"

# 3. 帮助信息测试
python3 grpo/training.py --help
```
</verification>

<success_criteria>
1. `grpo/reward.py` 包含完整的reward函数链
2. `grpo/training.py` 集成reward函数到GRPOTrainer
3. `config/grpo_config.yaml` 包含所有配置段
4. `grpo/config.py` 包含所有配置类和from_yaml方法
5. `python grpo/training.py --help` 显示完整帮助信息
6. Phase 1 Success Criteria全部满足
</success_criteria>

<output>
完成此计划后，创建 `.planning/phases/01-grpo-core-infrastructure/01-04-SUMMARY.md`，记录:
- reward函数链的完整实现
- GRPOTrainer集成方式
- 配置文件结构
- Phase 1完成总结
</output>
