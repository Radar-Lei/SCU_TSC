---
phase: 02-max-pressure-config
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - config/training_config.yaml
  - grpo/config.py
  - grpo/sft_training.py
  - grpo/training.py
autonomous: true

must_haves:
  truths:
    - "training_config.yaml包含所有SFT和GRPO训练的超参数"
    - "配置按功能分组（training、simulation、reward、paths等分段）"
    - "SFT训练脚本能从配置文件加载参数"
    - "GRPO训练脚本能从配置文件加载参数"
  artifacts:
    - path: config/training_config.yaml
      provides: 中央训练配置文件
      contains: ["training:", "simulation:", "reward:", "paths:"]
      min_lines: 80
    - path: grpo/config.py
      provides: TrainingConfig配置类
      exports: ["TrainingConfig", "load_training_config"]
      contains: "class TrainingConfig"
  key_links:
    - from: grpo/sft_training.py
      to: config/training_config.yaml
      via: "TrainingConfig.from_yaml()加载配置"
      pattern: "TrainingConfig.from_yaml"
    - from: grpo/training.py
      to: config/training_config.yaml
      via: "TrainingConfig.from_yaml()加载配置"
      pattern: "TrainingConfig.from_yaml"
---

## Objective

创建中央训练配置文件training_config.yaml，统一管理SFT和GRPO训练的所有超参数。配置文件按功能分组，支持通过YAML配置所有训练参数，替代分散在多个文件中的硬编码值。

**Purpose:** 建立中央配置管理系统，便于实验不同超参数组合，提高可维护性

**Output:** config/training_config.yaml文件，包含training、simulation、reward、paths等分段

## Context

@/home/samuel/.claude/get-shit-done/workflows/execute-plan.md
@/home/samuel/.claude/get-shit-done/templates/summary.md

@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@config/grpo_config.yaml
@grpo/config.py
@grpo/sft_training.py
@grpo/training.py
@.planning/phases/01-grpo-core-infrastructure/01-01-SUMMARY.md

## Tasks

<task type="auto">
  <name>Task 1: 创建training_config.yaml配置文件</name>
  <files>config/training_config.yaml</files>
  <action>
创建config/training_config.yaml文件，包含以下分段结构：

```yaml
# TSC-GRPO 中央训练配置文件
# 包含SFT和GRPO训练的所有超参数

# ============== 训练配置 ==============
training:
  # SFT训练参数
  sft:
    model_name: "unsloth/Qwen2.5-0.5B-Instruct"
    max_seq_length: 2048
    lora_rank: 32
    num_epochs: 3
    batch_size: 2
    gradient_accumulation_steps: 4
    learning_rate: 2.0e-4
    max_steps: null  # null表示不限制
    logging_steps: 5
    save_steps: 50
    eval_percent: 0.05  # 验证集比例
    eval_limit: 100     # 验证集最大数量
    eval_steps: 30
    warmup_steps: 5
    optim: "adamw_8bit"
    weight_decay: 0.001
    lr_scheduler_type: "linear"
    seed: 3407

  # GRPO训练参数
  grpo:
    model_path: "/home/samuel/SCU_TSC/model/sft_model"  # 从SFT模型继续
    max_seq_length: 2048
    lora_rank: 32
    learning_rate: 1.0e-5
    batch_size: 2
    gradient_accumulation_steps: 4
    num_generations: 4
    temperature: 0.9
    kl_coeff: 0.1
    max_new_tokens: 50
    top_p: 0.9
    repetition_penalty: 1.0
    num_train_epochs: 3
    warmup_steps: 10
    logging_steps: 5
    save_steps: 50
    optim: "adamw_8bit"
    gradient_checkpointing: true
    seed: 3407

# ============== 仿真配置 ==============
simulation:
  # SUMO仿真参数
  sumo:
    time_step: 1.0
    max_time: 3600
    warmup_steps: 300
    extend_seconds: 5
    min_green_time: 10.0
    max_green_time: 60.0
    min_green_offset_range: 2.0
    max_green_offset_range: 5.0
    default_min_green: 10.0
    default_max_green: 60.0
    max_workers: 4
    port_range: [10000, 60000]

  # 场景配置
  scenarios:
    scenarios_dir: "/home/samuel/SCU_TSC/sumo_simulation/environments"
    num_workers: 0  # 0表示使用CPU核心数-1

# ============== Reward配置 ==============
reward:
  # Reward函数链权重
  chain:
    format_weight: 1.0
    tsc_weight: 1.0

  # Format Reward参数
  format:
    strict: 1.0
    partial: -0.5
    invalid: -10.0
    extract_regex: '\{["\s]*extend["\s]*:\s*["\s]*(yes|no)["\s]*(?:,|\})'

  # TSC Reward参数
  tsc:
    reward_scale: 10.0

  # Max Pressure配置（预留）
  max_pressure:
    min_green_offset: 0.0
    max_green_override: false
    pressure_threshold: 0.0

# ============== 路径配置 ==============
paths:
  # 数据路径
  data_dir: "/home/samuel/SCU_TSC/data"
  grpo_dataset_dir: "/home/samuel/SCU_TSC/data/grpo_datasets"
  sft_dataset_dir: "/home/samuel/SCU_TSC/data/sft_datasets"

  # 模型路径
  sft_model_dir: "/home/samuel/SCU_TSC/model/sft_model"
  grpo_model_dir: "/home/samuel/SCU_TSC/model/grpo_model"

  # 配置路径
  grpo_config: "/home/samuel/SCU_TSC/config/grpo_config.yaml"

# ============== 日志配置 ==============
logging:
  use_wandb: false
  wandb_project: "scu-tsc-grpo"
  wandb_run_name: null
```

注意事项：
- 使用嵌套结构清晰分组（training.sft, training.grpo, simulation.sumo等）
- 添加中文注释说明每个参数的用途
- 保持与现有grpo_config.yaml的兼容性（GRPOConfig用于数据生成）
- 路径配置支持相对路径和绝对路径
  </action>
  <verify>
1. 检查文件存在且YAML格式正确：
   python3 -c "import yaml; yaml.safe_load(open('config/training_config.yaml')); print('YAML format: OK')"

2. 检查必需的分段存在：
   python3 -c "
import yaml
config = yaml.safe_load(open('config/training_config.yaml'))
assert 'training' in config, 'Missing training section'
assert 'simulation' in config, 'Missing simulation section'
assert 'reward' in config, 'Missing reward section'
assert 'paths' in config, 'Missing paths section'
assert 'sft' in config['training'], 'Missing sft subsection'
assert 'grpo' in config['training'], 'Missing grpo subsection'
print('All sections present: OK')
"
  </verify>
  <done>
training_config.yaml创建完成，包含所有SFT和GRPO训练参数，按功能分组
  </done>
</task>

<task type="auto">
  <name>Task 2: 创建TrainingConfig配置类</name>
  <files>grpo/config.py</files>
  <action>
在grpo/config.py中添加TrainingConfig类和相关子类：

1. **SFTTrainingConfig数据类**:
   ```python
   @dataclass
   class SFTTrainingConfig:
       model_name: str = "unsloth/Qwen2.5-0.5B-Instruct"
       max_seq_length: int = 2048
       lora_rank: int = 32
       num_epochs: int = 3
       batch_size: int = 2
       gradient_accumulation_steps: int = 4
       learning_rate: float = 2.0e-4
       max_steps: Optional[int] = None
       logging_steps: int = 5
       save_steps: int = 50
       eval_percent: float = 0.05
       eval_limit: int = 100
       eval_steps: int = 30
       warmup_steps: int = 5
       optim: str = "adamw_8bit"
       weight_decay: float = 0.001
       lr_scheduler_type: str = "linear"
       seed: int = 3407
   ```

2. **GRPOTrainingConfig数据类**:
   - 已存在，扩展为支持从training_config.yaml加载
   - 添加from_yaml_training()类方法用于从training_config.yaml加载

3. **SimulationConfig数据类**:
   ```python
   @dataclass
   class SimulationConfig:
       time_step: float = 1.0
       max_time: int = 3600
       warmup_steps: int = 300
       extend_seconds: int = 5
       min_green_time: float = 10.0
       max_green_time: float = 60.0
       min_green_offset_range: float = 2.0
       max_green_offset_range: float = 5.0
       default_min_green: float = 10.0
       default_max_green: float = 60.0
       max_workers: int = 4
       port_range: List[int] = field(default_factory=lambda: [10000, 60000])
   ```

4. **RewardConfig数据类**:
   ```python
   @dataclass
   class RewardConfig:
       format_weight: float = 1.0
       tsc_weight: float = 1.0
       format: FormatRewardConfig = field(default_factory=FormatRewardConfig)
       tsc: SUMOConfig = field(default_factory=SUMOConfig)
   ```

5. **PathsConfig数据类**:
   ```python
   @dataclass
   class PathsConfig:
       data_dir: str = "/home/samuel/SCU_TSC/data"
       grpo_dataset_dir: str = "/home/samuel/SCU_TSC/data/grpo_datasets"
       sft_dataset_dir: str = "/home/samuel/SCU_TSC/data/sft_datasets"
       sft_model_dir: str = "/home/samuel/SCU_TSC/model/sft_model"
       grpo_model_dir: str = "/home/samuel/SCU_TSC/model/grpo_model"
       grpo_config: str = "/home/samuel/SCU_TSC/config/grpo_config.yaml"
   ```

6. **TrainingConfig主配置类**:
   ```python
   @dataclass
   class TrainingConfig:
       training: Dict[str, Any]  # 包含sft和grpo配置
       simulation: SimulationConfig
       reward: RewardConfig
       paths: PathsConfig
       logging: Dict[str, Any]

       @classmethod
       def from_yaml(cls, path: str) -> "TrainingConfig":
           # 从training_config.yaml加载配置
           # 解析嵌套结构并创建相应子类实例

       @property
       def sft(self) -> SFTTrainingConfig:
           # 返回SFT配置

       @property
       def grpo(self) -> GRPOTrainingConfig:
           # 返回GRPO配置
   ```

7. **load_training_config()便捷函数**:
   ```python
   def load_training_config(path: str = "config/training_config.yaml") -> TrainingConfig:
       return TrainingConfig.from_yaml(path)
   ```

注意事项：
- 保持与现有GRPOTrainingConfig的兼容性
- 使用from_yaml处理嵌套YAML结构
- 添加__post_init__进行参数验证
  </action>
  <verify>
python3 -c "
from grpo.config import TrainingConfig, load_training_config, SFTTrainingConfig, SimulationConfig, RewardConfig, PathsConfig

# 测试加载配置
config = load_training_config('config/training_config.yaml')
print('Config loaded: OK')

# 测试属性访问
assert hasattr(config, 'sft'), 'Missing sft property'
assert hasattr(config, 'grpo'), 'Missing grpo property'
assert hasattr(config, 'simulation'), 'Missing simulation property'
assert hasattr(config, 'reward'), 'Missing reward property'
assert hasattr(config, 'paths'), 'Missing paths property'
print('Properties: OK')

# 测试SFT配置
sft_config = config.sft
assert sft_config.learning_rate == 2.0e-4, f'SFT learning rate mismatch: {sft_config.learning_rate}'
print('SFT config: OK')

# 测试GRPO配置
grpo_config = config.grpo
assert grpo_config.learning_rate == 1.0e-5, f'GRPO learning rate mismatch: {grpo_config.learning_rate}'
print('GRPO config: OK')

print('All config tests passed!')
"
  </verify>
  <done>
TrainingConfig配置类实现完成，支持从training_config.yaml加载所有训练参数
  </done>
</task>

<task type="auto">
  <name>Task 3: 更新SFT和GRPO训练脚本支持配置文件</name>
  <files>grpo/sft_training.py grpo/training.py</files>
<action>
1. 更新grpo/sft_training.py：
   - 修改train_sft()函数，添加config参数
   - 支持从TrainingConfig加载参数（优先级低于直接参数）
   - 添加--config命令行参数
   - 更新parse_args()和main()

   ```python
   def train_sft(
       config: Optional[TrainingConfig] = None,
       model_name: str = "unsloth/Qwen2.5-0.5B-Instruct",
       dataset_path: str = ...,
       # ... 其他参数
   ):
       # 如果config提供，使用config.sft作为默认值
       if config is not None:
           sft_config = config.sft
           # 使用sft_config中的值作为默认值
   ```

2. 更新grpo/training.py：
   - 修改train_grpo()函数，添加config参数
   - 支持从TrainingConfig加载参数
   - 添加--config参数支持training_config.yaml
   - 保持与现有grpo_config.yaml的兼容性

注意事项：
- 命令行参数优先级最高，覆盖配置文件值
- 保持向后兼容，不传config时使用原有默认值
- 添加配置文件路径打印，便于调试
  </action>
  <verify>
# 测试SFT训练脚本配置加载（不实际训练）
python3 -c "
from grpo.sft_training import parse_args
import sys

# 模拟命令行参数
sys.argv = ['sft_training.py', '--config', 'config/training_config.yaml']
args = parse_args()
assert hasattr(args, 'config'), 'Missing config argument'
assert args.config == 'config/training_config.yaml', f'Config path mismatch: {args.config}'
print('SFT training config argument: OK')
"

# 测试GRPO训练脚本配置加载
python3 -c "
from grpo.training import parse_args
import sys

# 模拟命令行参数
sys.argv = ['training.py', '--config', 'config/training_config.yaml']
args = parse_args()
assert hasattr(args, 'config'), 'Missing config argument'
assert args.config == 'config/training_config.yaml', f'Config path mismatch: {args.config}'
print('GRPO training config argument: OK')
"
  </verify>
  <done>
SFT和GRPO训练脚本支持从training_config.yaml加载配置，保持命令行参数覆盖能力
  </done>
</task>

## Verification

整体验证：
1. training_config.yaml文件存在且格式正确
2. TrainingConfig类能正确加载配置
3. SFT和GRPO训练脚本能使用配置文件
4. 命令行参数能覆盖配置文件值

## Success Criteria

1. config/training_config.yaml包含所有训练和仿真超参数
2. 配置按功能分组（training、simulation、reward、paths）
3. TrainingConfig类能正确加载和解析配置
4. SFT和GRPO训练脚本支持配置文件加载

## Output

After completion, create `.planning/phases/02-max-pressure-config/02-02-SUMMARY.md`
