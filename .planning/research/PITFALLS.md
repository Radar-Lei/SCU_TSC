# 陷阱研究: TSC GRPO训练系统重构

**领域:** 离线强化学习重构 (SUMO交通信号控制 + GRPO训练)
**研究日期:** 2026-02-03
**置信度:** HIGH (基于现有代码分析 + RL领域知识 + SUMO/Docker实践)

---

## 关键陷阱

### 陷阱 1: 数据生成时状态不完整导致训练时无法重现

**出错表现:**
训练时从state文件恢复仿真后,车辆排队数、相位状态与数据生成时不一致,导致reward计算错误。

**发生原因:**
SUMO的`saveState()`只保存部分仿真状态。某些动态信息(如随机数生成器状态、未来车辆插入计划、部分传感器数据)不在state文件中。当前代码在`dataset_generator.py:347`保存状态,但缺少完整性验证。

**如何避免:**
1. **立即验证策略:** 数据生成时,每次保存state后立即reload并对比关键指标(排队数、相位、仿真时间)
2. **元数据冗余:** 在数据条目中同时保存state_file路径和关键状态值(queue_counts, phase_id, sim_time),训练时可对比
3. **测试用例:** 在Phase 1创建"state往返测试"(save → load → compare),用真实场景验证

**代码位置分析:**
- `dataset_generator.py:347` - 保存状态但无验证
- `sumo_interface.py:255-270` - saveState/loadState无完整性检查
- `training.py:73` - 加载state_file路径但未验证有效性

**预警信号:**
- Reward计算时出现异常低/高的值
- 同一state_file多次加载产生不同的排队数
- 训练loss不收敛,reward分布异常宽

**所属阶段:**
**Phase 1 (数据生成重构)** - 必须在此阶段建立验证机制,否则后续训练基于错误数据

**恢复成本:** HIGH - 需要重新生成所有数据集

---

### 陷阱 2: 延长/切换两个分支的SUMO环境不一致

**出错表现:**
为同一决策点生成"延长"和"切换"两个结果时,除了动作不同外,其他环境因素(随机种子、车辆到达)也不同,导致无法公平比较两个决策的效果。

**发生原因:**
当前设计是"前置SUMO仿真到数据生成阶段",为每个决策点运行两次仿真。如果两次仿真的初始状态完全相同,但后续随机性(如新车辆生成)不同,则无法分离"动作影响"和"环境随机性影响"。

**如何避免:**
1. **从同一state文件分叉:** 每个决策点保存state → 加载state后执行延长 → 再次加载同一state后执行切换
2. **固定随机种子:** 在sumocfg中设置`<random value="false"/>`或固定种子,确保仿真可重现
3. **数据格式验证:** 在`GRPODataEntry`中添加字段记录延长/切换的初始状态哈希,确认一致性

**代码位置分析:**
- `dataset_generator.py:20-38` - GRPODataEntry缺少"分支一致性"字段
- `sumo_interface.py:122-192` - start()方法未强制可重现性
- `config.py` - 缺少"随机种子控制"配置项

**预警信号:**
- 延长vs切换的reward差异过大(>2倍)或过小(<5%)
- 数据中出现"两个动作reward都很差"的异常情况
- 仿真时间相同但排队数差异巨大

**所属阶段:**
**Phase 1 (数据生成重构)** - 架构设计阶段必须确定分叉策略

**恢复成本:** HIGH - 需要重新设计数据生成流程并重新生成数据

---

### 陷阱 3: 排队长度作为唯一指标丢失重要动态信息

**出错表现:**
模型学会"让排队数短期下降"的策略,但忽略了排队趋势、车辆等待时间、通行量等,导致长期性能差。

**发生原因:**
`PROJECT.md`决策使用"排队长度作为唯一状态/reward指标"。这是合理的简化,但可能过度简化:
- 排队长度不反映车辆等待了多久
- 5秒决策间隔内,排队数可能只是噪声波动
- 忽略了"即将到达的车流"信息

当前代码`sumo_interface.py:418-423`只统计停止车辆数(halting),未考虑趋势。

**如何避免:**
1. **保守起步:** v1确实只用排队长度,但在数据中保留更多信息(waiting_time, throughput)作为备用
2. **趋势特征:** 不仅记录当前queue,也记录"过去N个决策点的queue变化率"
3. **多目标验证:** 训练后在测试集上评估多个指标(不仅reward,还有平均等待时间等),检测过拟合

**代码位置分析:**
- `sumo_interface.py:418-423` - `get_lane_queue_count()`只返回数量
- `dataset_generator.py:323` - `get_all_phases_queue()`只统计当前时刻
- `reward.py` - Reward计算基于排队数差异,无趋势信息

**预警信号:**
- 训练reward上升,但测试集上平均等待时间下降很慢
- 模型倾向总是"切换"或总是"延长"(极端策略)
- 不同路口/场景泛化性能差

**所属阶段:**
**Phase 2 (Reward函数设计)** - 在此阶段验证reward信号充分性
**Phase 4 (验证测试)** - 检测是否需要回退增加特征

**恢复成本:** MEDIUM - 可以在不重新生成数据的情况下,从现有state文件中提取补充特征

---

### 陷阱 4: GRPO训练时num_generations设置错误导致无效对比

**出错表现:**
GRPO需要为每个prompt生成多个候选(group),然后计算相对reward。如果`num_generations`设置为1,则无group对比,GRPO退化为普通策略梯度。

**发生原因:**
TRL的GRPOTrainer期望每个prompt生成多个输出,通过组内相对排序学习。当前`training.py:456`使用`config.num_generations`,但如果配置错误(=1或过小),训练无效。

GRPO原理: 对同一prompt生成N个输出 → 计算各自reward → 根据组内排名调整策略(高reward的增加概率,低reward的减少)。

**如何避免:**
1. **最小值断言:** 在`train_grpo()`开始时检查`num_generations >= 4`,否则报错
2. **配置文件注释:** 在`config/training_config.yaml`中明确说明GRPO的num_generations含义和推荐范围(4-8)
3. **训练日志监控:** 打印每个batch的生成数量,确保符合预期

**代码位置分析:**
- `training.py:461` - `num_generations=config.num_generations`直接使用,无验证
- `config.py` - GRPOTrainingConfig需要添加验证逻辑
- 当前代码缺少"GRPO特定参数"的健全性检查

**预警信号:**
- 训练过程中每个prompt只有1个completion
- Reward标准差为0(所有生成结果相同)
- 训练曲线类似SFT而非RL(无探索)

**所属阶段:**
**Phase 3 (GRPO训练集成)** - 在训练脚本中添加GRPO参数验证

**恢复成本:** LOW - 只需修改配置重新训练,无需重新生成数据

---

### 陷阱 5: Docker环境中SUMO并行执行端口冲突

**出错表现:**
使用`parallel_runner.py`并行生成数据时,多个SUMO实例因为端口冲突无法同时启动,导致部分worker失败或死锁。

**发生原因:**
TraCI需要为每个SUMO实例分配唯一端口。当前`sumo_interface.py:45-67`的`find_available_port()`在单机环境可用,但在Docker容器或多进程环境下存在竞态条件:
- 进程A检测端口10001可用
- 进程B也检测端口10001可用
- 两者同时尝试绑定 → 一个失败

Docker环境加剧此问题,因为容器网络隔离可能影响端口检测。

**如何避免:**
1. **预分配端口池:** 在并行启动前,主进程分配N个端口,每个worker使用指定端口(不自行查找)
2. **文件锁机制:** 使用文件锁或数据库记录已占用端口,原子性地分配
3. **重试+指数退避:** 当前代码`sumo_interface.py:157`有重试,但应增加随机延迟避免"同步重试"
4. **Docker端口映射:** 在`docker/publish.sh`中预定义端口映射范围

**代码位置分析:**
- `sumo_interface.py:45-67` - `find_available_port()`存在竞态
- `parallel_runner.py` - 并行调用但未协调端口分配
- `docker/publish.sh` - 缺少端口范围配置

**预警信号:**
- 并行任务中部分worker反复重启
- 日志中出现"Address already in use"错误
- 数据生成完成率<100%但无明显错误

**所属阶段:**
**Phase 1 (数据生成重构)** - 并行化时必须解决

**恢复成本:** MEDIUM - 修复代码后需重新运行失败的场景

---

### 陷阱 6: 相位min/max green随机偏移不一致导致训练时约束违反

**出错表现:**
数据生成时每个相位的min_green和max_green有随机偏移(±5秒),但训练时模型不知道这些偏移值,导致学习到的策略在实际部署时违反约束(如在已达最大绿时仍要延长)。

**发生原因:**
`dataset_generator.py:57-86`使用`_green_time_cache`缓存随机偏移后的min/max值,这是合理的(确保同一相位在整个episode中使用相同约束)。但问题是:
- 这些随机值只在数据生成时使用
- `GRPODataEntry`中保存了`min_green`和`max_green`,但训练时未使用
- 模型输入(prompt)不包含"当前约束是多少"

结果: 模型学习的是"平均"约束下的策略,遇到偏离平均的约束时表现差。

**如何避免:**
1. **约束作为输入:** 修改prompt,包含`min_green_time`和`max_green_time`字段,让模型知道当前约束
2. **标准化约束:** v1不使用随机偏移,所有相位使用固定min/max(简化问题)
3. **训练时验证:** 在reward函数中检查模型决策是否违反约束,违反时给予惩罚

**代码位置分析:**
- `dataset_generator.py:57-86` - 随机偏移逻辑
- `prompt_builder.py` - 需要检查是否包含约束信息
- `training.py:419-429` - Reward配置未考虑约束验证

**预警信号:**
- 模型在测试集上频繁选择"延长"但can_extend=False
- 不同场景(min/max偏移不同)的性能差异大
- 部署时出现"无效动作"需要fallback

**所属阶段:**
**Phase 2 (Reward函数设计)** - 决定是否简化约束
**Phase 3 (GRPO训练集成)** - 添加约束验证

**恢复成本:** MEDIUM - 可能需要重新生成数据(如果改prompt)或只修改reward函数

---

### 陷阱 7: 离线数据的分布偏差导致模型过拟合到数据生成策略

**出错表现:**
模型性能在训练集上很好,但在新的测试场景或不同时间段(早高峰vs晚高峰)上性能骤降。

**发生原因:**
离线RL的核心挑战: 模型只能学习到数据生成策略能覆盖的状态-动作空间。当前设计是"在决策点收集数据",但如果:
- 数据生成策略本身有偏(如总是在min_green时刻首次决策)
- 某些场景(如极端拥堵)在训练数据中出现少
- 决策点采样不均匀(当前代码`dataset_generator.py:326-340`基于排队数过滤,可能加剧偏差)

则模型学到的是"在数据生成策略诱导的分布下如何决策",而非"在真实部署分布下如何决策"。

**如何避免:**
1. **数据审查:** 在Phase 1后,统计数据集的状态分布(排队数范围、相位分布、时间分布),识别覆盖不足的区域
2. **平衡采样:** 当前代码`dataset_generator.py:326-340`按排队数概率过滤,应考虑分层采样(确保各排队数区间有足够样本)
3. **领域随机化:** 数据生成时使用多种场景配置(不同车流模式、路网结构)
4. **保守策略:** 训练时增加KL散度约束(限制策略偏离数据生成策略的程度)

**代码位置分析:**
- `dataset_generator.py:326-340` - 过滤逻辑可能引入采样偏差
- `parallel_runner.py` - 场景选择是否覆盖足够多样性
- `training.py:468` - `beta=config.kl_coeff`控制保守程度

**预警信号:**
- 训练集reward高,验证集reward低(典型过拟合)
- 数据分布统计显示某些状态覆盖<1%
- 模型在罕见状态下给出极端决策

**所属阶段:**
**Phase 1 (数据生成重构)** - 数据审查和采样策略
**Phase 4 (验证测试)** - 检测泛化性能

**恢复成本:** HIGH - 可能需要补充生成数据或重新设计采样策略

---

### 陷阱 8: Reward函数的格式检查过严阻碍探索

**出错表现:**
训练初期,模型生成的输出格式不符合要求(如JSON格式错误),导致大部分样本获得严厉的format惩罚,模型陷入局部最优(学会生成格式正确但内容无意义的输出)。

**发生原因:**
当前`reward.py`实现了多层级reward链:
1. Format reward (strict/partial/invalid)
2. TSC reward (基于SUMO仿真结果)

如果`format_weight`过高或`format_invalid`惩罚过严(如-1.0),模型可能:
- 优先优化格式而非决策质量
- 避免探索(因为探索可能导致格式错误)
- 收敛到"安全但低质量"的输出(如总是输出固定模板)

**如何避免:**
1. **渐进式惩罚:** 训练早期使用宽松的format reward,后期逐渐收紧
2. **Partial credit:** 当前代码似乎有`partial_format_count`,应确保"接近正确"的输出仍有正reward
3. **SFT预训练:** 在GRPO前先做SFT,确保模型初始策略已能生成正确格式
4. **监控指标:** 打印`format_accuracy`和`avg_tsc_reward`的比例,确保TSC reward有信号

**代码位置分析:**
- `reward.py` - Format/TSC权重平衡
- `training.py:360-361` - 打印format/TSC权重但无动态调整
- `config.py` - 缺少"课程学习"式的reward调度配置

**预警信号:**
- `format_accuracy`快速达到100%但`avg_tsc_reward`不增长
- 生成的输出高度一致(缺乏多样性)
- KL散度快速下降(策略过早收敛)

**所属阶段:**
**Phase 2 (Reward函数设计)** - 调整权重平衡
**Phase 3 (GRPO训练集成)** - 监控训练动态

**恢复成本:** LOW - 调整reward权重或添加课程学习,重新训练

---

## 技术债务模式

在重构过程中,这些"捷径"看似节省时间,实际埋下隐患:

| 捷径 | 即时好处 | 长期代价 | 何时可接受 |
|------|----------|----------|------------|
| 跳过state文件的保存/加载验证 | 数据生成快10% | 训练时发现数据损坏,需重新生成 | **绝不** - 验证成本远低于重新生成 |
| 不记录SUMO场景的sumocfg路径 | 数据结构简单 | 训练时无法重新加载state,无法计算TSC reward | **绝不** - 当前代码`training.py:74`已记录scenario_dir |
| 对所有场景使用相同的min/max green | 避免复杂的随机偏移逻辑 | 现实场景中约束多样,模型泛化差 | **MVP阶段** - v1可简化,v2增加 |
| 只使用单个SUMO场景训练 | 数据生成快 | 严重过拟合到特定路网和车流 | **调试阶段** - 正式训练需多场景 |
| 不区分train/val/test split | 代码简单 | 无法检测过拟合,模型部署风险高 | **绝不** - 必须有验证集 |
| Reward函数中不计算baseline对比 | 训练快(无额外仿真) | 不知道模型是否优于简单策略 | **早期实验** - 但应尽早添加 |
| 所有场景用相同的extend_seconds | 配置简单 | 不同场景的最优决策频率不同 | **v1可接受** - 但应记录此假设 |
| 不保存数据生成时的随机种子 | 元数据更少 | 无法重现数据生成过程,难以调试 | **绝不** - 种子是可重现性基础 |

---

## 集成陷阱

与外部系统集成时的常见错误:

| 集成点 | 常见错误 | 正确做法 |
|--------|----------|----------|
| SUMO Docker | 假设Docker内外环境一致,直接用绝对路径 | 使用容器内路径,通过volume映射 |
| SUMO TraCI | 未处理TraCIException,导致整个数据生成中断 | try-except包裹所有traci调用,失败时记录并跳过该场景 |
| SUMO saveState | 假设所有仿真状态都被保存 | 验证关键状态(排队数、相位)在load后一致 |
| GRPO Trainer | 假设prompts和completions长度总是一致 | 当前代码`training.py:219-220`已处理对齐,需保留 |
| HuggingFace Dataset | 期望所有列都能被自动处理 | `remove_unused_columns=False`(当前代码`training.py:487`已设置) |
| Unsloth FastLanguageModel | 假设所有模型路径都能加载 | 检查路径存在性,区分本地/HF hub路径 |
| Reward计算并行化 | 多个worker同时创建SUMO实例,端口冲突 | 预分配端口池或使用进程锁 |

---

## 性能陷阱

小规模时没问题,扩展后崩溃的模式:

| 陷阱 | 症状 | 预防 | 何时崩溃 |
|------|------|------|----------|
| 为每个数据点保存state文件 | 磁盘占用暴增 | `state_save_interval`控制保存频率(当前代码`dataset_generator.py:347`) | >10K样本,每个state文件~10MB |
| 加载整个数据集到内存 | 内存溢出 | 使用HuggingFace Dataset的streaming模式 | 数据集>10GB |
| 串行生成所有场景数据 | 时间过长(数天) | 并行化(当前有`parallel_runner.py`),但需解决端口冲突 | >50个场景 |
| 训练时同步计算reward | GPU闲置等待SUMO仿真 | 当前代码`sumo_reward.py`应使用异步或批处理 | batch_size \* num_generations > 20 |
| 每个epoch重新验证所有数据 | 验证时间>>训练时间 | 只在checkpoint时验证,或使用验证集子集 | 验证集>100K样本 |
| SUMO GUI模式 | 渲染窗口消耗资源 | 生产环境强制`gui=False`(当前代码`config.py`控制) | 并行>4实例 |
| 日志打印过度详细 | I/O成为瓶颈 | 分级日志(DEBUG/INFO),生产环境只WARNING+ | 每秒>1000条日志 |

---

## 数据完整性检查清单

这些看似完成,实则缺失关键部分的项目:

- [ ] **SUMO状态保存:** 保存了state文件,但未验证load后排队数一致性
- [ ] **数据集JSON:** 生成了grpo_dataset.json,但未检查必需字段(state_file, prompt)是否非空
- [ ] **Prompt构建:** 构建了JSON格式,但未验证是否可解析(可能有转义问题)
- [ ] **相位顺序:** 记录了phase_order,但未验证它与SUMO实际执行顺序一致
- [ ] **时间戳对齐:** 记录了simulation_time,但未验证state文件中的时间匹配
- [ ] **场景配置:** 使用了SUMO场景,但未记录车流文件(.rou.xml)的哈希(无法验证数据来源)
- [ ] **Reward可计算性:** 保存了state_file路径,但未在数据生成阶段验证该state能否被加载和执行
- [ ] **决策点一致性:** 判断了决策点,但未验证min_green时刻的判断在不同run中一致(浮点数精度问题)
- [ ] **Docker环境:** 在Docker内运行,但未记录镜像版本/SUMO版本(影响可重现性)
- [ ] **Baseline对比:** 计算了模型reward,但未预先计算简单策略(如固定时长、Max Pressure)的reward

---

## 恢复策略

当陷阱已发生,如何补救:

| 陷阱 | 恢复成本 | 恢复步骤 |
|------|----------|----------|
| 数据生成时未保存state文件 | **HIGH** | 1. 重新运行所有场景生成 2. 添加state保存逻辑 3. 验证state完整性 |
| State文件损坏或不可加载 | **HIGH** | 1. 检查哪些state可用 2. 过滤数据集保留可用数据 3. 补充生成缺失数据 |
| Reward函数设计错误(如符号反了) | **MEDIUM** | 1. 修正reward函数 2. 重新训练(数据可复用) 3. 对比新旧模型性能 |
| GRPO参数设置错误 | **LOW** | 1. 调整配置文件 2. 从checkpoint恢复或重新训练 3. 监控新指标 |
| Docker端口冲突导致部分数据缺失 | **MEDIUM** | 1. 识别失败的场景 2. 修复端口分配逻辑 3. 只重新生成失败场景 |
| 数据分布偏差严重 | **HIGH** | 1. 分析数据分布 2. 设计平衡采样策略 3. 补充生成数据或重新生成 |
| 约束信息缺失导致模型违反约束 | **MEDIUM** | 1. 修改prompt包含约束 2. 重新生成数据集 3. 或在reward中惩罚违反约束 |
| 过拟合到训练集 | **MEDIUM** | 1. 增加数据多样性 2. 增强正则化(KL系数) 3. 早停策略 4. 补充验证集 |

---

## 阶段-陷阱映射

如何在roadmap各阶段预防这些陷阱:

| 陷阱 | 预防阶段 | 验证方法 |
|------|----------|----------|
| State文件不完整 | Phase 1 (数据生成) | save→load→对比排队数测试 |
| 延长/切换分支不一致 | Phase 1 (数据生成) | 检查分叉前的state哈希一致 |
| 排队长度指标不足 | Phase 2 (Reward设计) | 训练后评估多指标(等待时间等) |
| GRPO num_generations错误 | Phase 3 (训练集成) | 启动时断言>=4,日志打印实际值 |
| Docker端口冲突 | Phase 1 (数据生成) | 并行测试,监控worker成功率 |
| 约束随机偏移不一致 | Phase 2 (Reward设计) | 决定是否简化或将约束加入prompt |
| 离线数据分布偏差 | Phase 1 & Phase 4 | 数据审查+测试集泛化评估 |
| Reward格式检查过严 | Phase 3 (训练集成) | 监控format_accuracy vs tsc_reward比例 |
| 缺少验证集 | Phase 1 (数据生成) | 生成时按8:1:1划分train/val/test |
| 缺少baseline对比 | Phase 4 (验证测试) | 实现Max Pressure baseline并对比 |
| SUMO版本不一致 | Phase 0 (环境准备) | 在Docker镜像中固定SUMO版本 |
| 日志记录不足 | 所有阶段 | 关键操作(save/load/决策)打印时间戳+参数 |

---

## 领域特定最佳实践

基于TSC+GRPO+SUMO的特殊性:

**SUMO仿真最佳实践:**
1. **始终使用headless模式**(sumo而非sumo-gui)在生产环境
2. **saveState前先执行一步仿真**,确保所有内部状态更新完成
3. **loadState后预热若干步**(如10步),让车辆状态稳定
4. **记录每个state文件对应的sumocfg路径**,否则无法重新加载

**GRPO训练最佳实践:**
1. **num_generations建议4-8**(太小无对比,太大计算慢)
2. **KL系数(beta)从小开始**(如0.01),避免策略偏离过快
3. **监控组内reward方差**,方差过小说明生成多样性不足
4. **早期使用SFT预训练**,确保初始策略能生成有效格式

**离线RL最佳实践:**
1. **数据审查是必需步骤**,统计状态-动作覆盖率
2. **保守策略优于激进策略**,宁可性能提升慢也不要崩溃
3. **Baseline对比必不可少**,否则不知道"学到东西"还是"记住数据"
4. **验证集必须与训练集分布不同**(不同时间段/场景),才能测泛化

**Docker集成最佳实践:**
1. **在Dockerfile中固定所有版本**(SUMO, Python, CUDA)
2. **使用docker-compose管理多容器**(如并行数据生成)
3. **Volume映射要用绝对路径**,避免上下文混淆
4. **容器内日志输出到stdout**,便于docker logs查看

---

## 研究信心度评估

| 陷阱类别 | 信心度 | 依据 |
|----------|--------|------|
| SUMO状态保存/加载 | **HIGH** | 代码分析 + SUMO官方文档已知限制 |
| 离线RL分布偏差 | **HIGH** | RL领域共识 + 当前代码采样逻辑分析 |
| GRPO训练参数 | **MEDIUM** | TRL文档 + GRPO论文原理(训练数据有限) |
| Docker端口冲突 | **HIGH** | 代码中find_available_port的竞态明显 |
| Reward设计权衡 | **MEDIUM** | TSC领域知识 + 当前代码只用排队数 |
| 约束一致性 | **HIGH** | 代码中随机偏移逻辑与训练输入不匹配 |
| 数据完整性验证 | **HIGH** | 当前代码缺少验证步骤(代码审查) |

---

## 未覆盖领域

以下是研究中信心度较低或未充分覆盖的领域,可能需要在具体阶段深入:

1. **Qwen模型特定问题:** 当前代码参考`Qwen3_(4B)_GRPO.ipynb`,但未研究Qwen在TSC任务上的特殊行为
2. **Unsloth框架限制:** 使用unsloth加速,但未研究其LoRA实现与标准PEFT的差异
3. **长序列训练:** 如果prompt+completion超过max_seq_length,truncation策略是什么
4. **多路口协同:** 当前代码似乎单路口独立决策,未考虑路口间影响
5. **在线fine-tuning:** 部署后如何持续学习(超出当前offline RL范围)

---

## 信息来源

**代码审查:**
- `/home/samuel/SCU_TSC/grpo/dataset_generator.py` - 数据生成流程和状态保存
- `/home/samuel/SCU_TSC/grpo/sumo_interface.py` - SUMO接口和端口管理
- `/home/samuel/SCU_TSC/grpo/training.py` - GRPO训练流程和reward函数
- `/home/samuel/SCU_TSC/.planning/PROJECT.md` - 项目设计决策和约束

**领域知识:**
- 离线强化学习的分布偏移问题(RL领域共识,2020-2025文献)
- GRPO算法原理(Group Relative Policy Optimization)
- SUMO仿真器的状态保存局限性(官方文档和社区讨论)
- Docker并发和端口管理(软件工程最佳实践)

**置信度说明:**
- **HIGH信心:** 基于实际代码分析 + 可验证的技术限制
- **MEDIUM信心:** 基于领域经验 + 间接证据(如设计决策的隐含假设)
- **LOW信心:** 基于一般经验,但缺少该项目的具体验证

---

*陷阱研究: TSC GRPO训练系统离线重构*
*研究日期: 2026-02-03*
*分析基础: 现有代码库 + RL/SUMO领域知识 + Docker工程实践*
