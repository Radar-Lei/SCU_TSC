{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSC äº¤é€šä¿¡å·æ§åˆ¶å¾®è°ƒ - Non-Thinking ç‰ˆæœ¬\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths! # To enable memory efficient GRPO with vLLM\n",
    "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess \n",
    "import os \n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "\tif '=' in line:\n",
    "\t\tvar, value = line.split('=', 1)\n",
    "\t\tos.environ[var] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 12-22 15:02:32 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM\n",
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM to enable standby.\n",
      "INFO:unsloth_zoo.log: Unsloth: Enabling vLLM standby mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:02:34 [vllm_utils.py:693] Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'\n",
      "==((====))==  Unsloth 2025.10.5: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading model/models/qwen3-4B-SFT with actual GPU utilization = 88.87%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.56 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 256.\n",
      "Unsloth: vLLM's KV Cache can use up to 13.99 GB. Also swap space = 6 GB.\n",
      "WARNING 12-22 15:02:39 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-22 15:02:39 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 1024, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8886516903419027, 'max_num_batched_tokens': 2048, 'max_num_seqs': 256, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'enable_sleep_mode': True, 'model': 'model/models/qwen3-4B-SFT'}\n",
      "INFO 12-22 15:02:39 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:02:39 [model.py:1510] Using max model len 1024\n",
      "INFO 12-22 15:02:40 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 12-22 15:02:40 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 12-22 15:02:40 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='model/models/qwen3-4B-SFT', speculative_config=None, tokenizer='model/models/qwen3-4B-SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=model/models/qwen3-4B-SFT, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-22 15:02:40 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 12-22 15:02:40 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 12-22 15:02:40 [gpu_model_runner.py:2602] Starting to load model model/models/qwen3-4B-SFT...\n",
      "INFO 12-22 15:02:41 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 12-22 15:02:41 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa9f1aec1ed442fba4e67829395727b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:02:44 [default_loader.py:267] Loading weights took 3.51 seconds\n",
      "INFO 12-22 15:02:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 12-22 15:02:45 [gpu_model_runner.py:2653] Model loading took 7.7296 GiB and 3.628204 seconds\n",
      "INFO 12-22 15:02:50 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/2fd496daae/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-22 15:02:50 [backends.py:559] Dynamo bytecode transform time: 5.10 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 23.07it/s, triton_poi_fused_view_6]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:02:52 [backends.py:197] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 37.57it/s, triton_poi_fused_view_10]                          \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1275.25it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1307.08it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1210.13it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1155.20it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1169.78it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1126.81it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1377.07it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1377.77it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1344.88it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1105.40it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1316.59it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1404.36it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1368.09it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1083.65it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1309.49it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1143.94it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1209.02it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1152.17it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1240.38it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1191.04it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1218.73it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1304.24it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1331.03it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1344.72it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1340.03it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1244.40it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1242.09it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1380.99it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1321.08it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1203.03it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1344.68it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1291.93it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1139.02it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1200.87it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 37.71it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:03:09 [backends.py:218] Compiling a graph for dynamic shape takes 18.11 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:03:19 [monitor.py:34] torch.compile takes 23.21 s in total\n",
      "INFO 12-22 15:03:20 [gpu_worker.py:298] Available KV cache memory: 11.79 GiB\n",
      "INFO 12-22 15:03:20 [kv_cache_utils.py:1087] GPU KV cache size: 85,824 tokens\n",
      "INFO 12-22 15:03:20 [kv_cache_utils.py:1091] Maximum concurrency for 1,024 tokens per request: 83.81x\n",
      "INFO 12-22 15:03:20 [vllm_utils.py:698] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 12-22 15:03:20 [gpu_model_runner.py:3643] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:08<00:00,  7.45it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:03:31 [gpu_model_runner.py:3480] Graph capturing finished in 11 secs, took 1.02 GiB\n",
      "INFO 12-22 15:03:31 [vllm_utils.py:705] Unsloth: Patched vLLM v1 graph capture finished in 11 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 15:03:32 [core.py:210] init engine (profile, create kv cache, warmup model) took 47.39 seconds\n",
      "INFO 12-22 15:03:32 [llm.py:306] Supported_tasks: ('generate',)\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_layernorm', 'attention_norm', 'post_attention_layernorm', 'layer_norm1', 'k_norm', 'ffn_norm', 'post_feedforward_layernorm', 'norm1', 'layer_norm2', 'norm2', 'q_norm', 'input_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88adb2185b044ee3a350ac4cf6893143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at model/models/qwen3-4B-SFT and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_layernorm', 'attention_norm', 'post_attention_layernorm', 'layer_norm1', 'k_norm', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'cross_attn_post_attention_layernorm', 'norm1', 'layer_norm2', 'norm2', 'q_norm', 'input_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.5 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 1024\n",
    "lora_rank = 32\n",
    "\n",
    "os.environ[\"HF_HOME\"] = 'model'\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = 'model'\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"model/models/qwen3-4B-SFT\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½ TSC æ•°æ®é›†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# åŠ è½½ TSC æ•°æ®é›†\n",
    "with open('./data_TSC/tsc_sft_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    tsc_data = json.load(f)\n",
    "\n",
    "print(f\"æ€»æ•°æ®é‡: {len(tsc_data)}\")\n",
    "\n",
    "# åˆ†å‰²æ•°æ®é›†ï¼š95% è®­ç»ƒï¼Œ5% æµ‹è¯•\n",
    "train_data, test_data = train_test_split(tsc_data, test_size=0.05, random_state=42)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_data)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–ç­”æ¡ˆå‡½æ•° - ä¸¥æ ¼æ ¼å¼è¦æ±‚\n",
    "def extract_phase_answer(text: str) -> str | None:\n",
    "    \"\"\"ä»è¾“å‡ºä¸­æå–ç›¸ä½æ•°å­—ï¼Œä¸¥æ ¼è¦æ±‚æ ¼å¼ä¸ºï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    \n",
    "    # è§„èŒƒåŒ–ï¼šå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦\n",
    "    text = text.strip().replace(' ', '')\n",
    "    \n",
    "    # å°è¯•å¤šç§åˆ†éš”ç¬¦å’Œæ ¼å¼\n",
    "    patterns = [\n",
    "        r'ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½[:ï¼š]\\d+',  # åŸå§‹æ ¼å¼\n",
    "        r'ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½[:ï¼š]\\s*(\\d+)',  # å¸¦ç©ºæ ¼\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            # æå–æ•°å­—\n",
    "            digits = re.findall(r'\\d+', match.group(0))\n",
    "            if digits:\n",
    "                return digits[-1]  # è¿”å›æœ€åä¸€ä¸ªæ•°å­—\n",
    "    \n",
    "    return None\n",
    "\n",
    "# def extract_phase_answer(text: str) -> str | None:\n",
    "#     \"\"\"ä»è¾“å‡ºä¸­æå–ç›¸ä½æ•°å­—ï¼Œä¸¥æ ¼è¦æ±‚æ ¼å¼ä¸ºï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\"\"\"\n",
    "#     # åªåŒ¹é…ä¸¥æ ¼æ ¼å¼ï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\n",
    "#     pattern = r'ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½[:ï¼š]\\s*(\\d+)'\n",
    "#     match = re.search(pattern, text)\n",
    "#     if match:\n",
    "#         return match.group(1)\n",
    "#     return None\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒæ•°æ®é›†\n",
    "def prepare_dataset(data):\n",
    "    dataset_list = []\n",
    "    for item in data:\n",
    "        # ä¿®æ”¹ç³»ç»Ÿæç¤ºï¼Œå¼ºåˆ¶æ ¼å¼ä¸º \"ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\"\n",
    "        system_prompt = \"ä½ æ˜¯ä¸€ä½äº¤é€šç®¡ç†ä¸“å®¶ã€‚ä½ å¯ä»¥è¿ç”¨ä½ çš„äº¤é€šå¸¸è¯†çŸ¥è¯†æ¥è§£å†³äº¤é€šä¿¡å·æ§åˆ¶ä»»åŠ¡ã€‚æ ¹æ®ç»™å®šçš„äº¤é€šåœºæ™¯å’ŒçŠ¶æ€ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ã€‚ä½ å¿…é¡»ç›´æ¥å›ç­”ï¼Œæ ¼å¼å¿…é¡»æ˜¯ï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼š{æ•°å­—}ï¼ˆå…¶ä¸­æ•°å­—æ˜¯0-9ä¹‹é—´çš„å•ä¸ªæ•°å­—ï¼‰\"\n",
    "        \n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": item[\"input\"]},\n",
    "        ]\n",
    "        answer = extract_phase_answer(item[\"output\"])\n",
    "        dataset_list.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"answer\": answer,\n",
    "        })\n",
    "    return Dataset.from_list(dataset_list)\n",
    "\n",
    "train_dataset = prepare_dataset(train_data)\n",
    "test_dataset = prepare_dataset(test_data)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†æ ·ä¾‹:\")\n",
    "print(f\"Prompt: {train_dataset[0]['prompt']}\")\n",
    "print(f\"Answer: {train_dataset[0]['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¾®è°ƒå‰æµ‹è¯•æ¨¡å‹å‡†ç¡®ç‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def evaluate_model(model, tokenizer, test_dataset, max_samples=100):\n",
    "#     \"\"\"è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡\"\"\"\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     # åªæµ‹è¯•å‰ max_samples ä¸ªæ ·æœ¬ä»¥èŠ‚çœæ—¶é—´\n",
    "#     test_samples = min(max_samples, len(test_dataset))\n",
    "    \n",
    "#     FastLanguageModel.for_inference(model)  # å¯ç”¨æ¨ç†æ¨¡å¼\n",
    "    \n",
    "#     for i in tqdm(range(test_samples), desc=\"è¯„ä¼°ä¸­\"):\n",
    "#         item = test_dataset[i]\n",
    "        \n",
    "#         # æ„å»ºè¾“å…¥\n",
    "#         messages = item['prompt']\n",
    "#         inputs = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=True,\n",
    "#             add_generation_prompt=True,\n",
    "#             return_tensors=\"pt\"\n",
    "#         ).to(model.device)\n",
    "        \n",
    "#         # ç”Ÿæˆå›ç­”\n",
    "#         outputs = model.generate(\n",
    "#             input_ids=inputs,\n",
    "#             max_new_tokens=128,\n",
    "#             temperature=0.7,\n",
    "#             do_sample=True,\n",
    "#             pad_token_id=tokenizer.pad_token_id,\n",
    "#         )\n",
    "        \n",
    "#         # è§£ç è¾“å‡º\n",
    "#         response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "#         # æå–é¢„æµ‹çš„ç›¸ä½\n",
    "#         predicted_phase = extract_phase_answer(response)\n",
    "#         true_phase = item['answer']\n",
    "        \n",
    "#         if predicted_phase == true_phase:\n",
    "#             correct += 1\n",
    "#         total += 1\n",
    "        \n",
    "#         # æ‰“å°å‰5ä¸ªæ ·ä¾‹\n",
    "#         if i < 5:\n",
    "#             print(f\"\\næ ·ä¾‹ {i+1}:\")\n",
    "#             print(f\"çœŸå®ç›¸ä½: {true_phase}\")\n",
    "#             print(f\"é¢„æµ‹ç›¸ä½: {predicted_phase}\")\n",
    "#             print(f\"æ¨¡å‹å›ç­”: {response[:200]}...\")\n",
    "    \n",
    "#     accuracy = correct / total if total > 0 else 0\n",
    "#     print(f\"\\nå‡†ç¡®ç‡: {accuracy:.2%} ({correct}/{total})\")\n",
    "#     return accuracy\n",
    "\n",
    "# print(\"=\"*50)\n",
    "# print(\"å¾®è°ƒå‰æ¨¡å‹å‡†ç¡®ç‡:\")\n",
    "# print(\"=\"*50)\n",
    "# accuracy_before = evaluate_model(model, tokenizer, test_dataset, max_samples=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰å¥–åŠ±å‡½æ•°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# éªŒè¯è¾“å‡ºæ ¼å¼\n",
    "def is_valid_format(text: str) -> bool:\n",
    "    \"\"\"éªŒè¯æ–‡æœ¬æ˜¯å¦æ»¡è¶³ä¸¥æ ¼æ ¼å¼ï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\"\"\"\n",
    "    pattern = r'^ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½[:ï¼š]\\s*\\d+\\s*$'\n",
    "    return bool(re.match(pattern, text.strip()))\n",
    "\n",
    "# å¥–åŠ±å‡½æ•°ï¼šæ£€æŸ¥é¢„æµ‹çš„ç›¸ä½æ˜¯å¦æ­£ç¡®\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    q = prompts[0][-1][\"content\"][:100]  # åªæ˜¾ç¤ºå‰100å­—ç¬¦\n",
    "    extracted_responses = [extract_phase_answer(r) for r in responses]\n",
    "    \n",
    "    print(\n",
    "        \"-\" * 20,\n",
    "        f\"\\né—®é¢˜:\\n{q}...\",\n",
    "        f\"\\næ­£ç¡®ç­”æ¡ˆ:\\n{answer[0]}\",\n",
    "        f\"\\næ¨¡å‹å›ç­”:\\n{responses[0][:150]}...\",\n",
    "        f\"\\næå–ç»“æœ:\\n{extracted_responses[0]}\",\n",
    "    )\n",
    "    result = [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "    del responses, extracted_responses\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# å¥–åŠ±å‡½æ•°ï¼šä¸¥æ ¼éªŒè¯æ ¼å¼ - ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\n",
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    # ä¸¥æ ¼æ£€æŸ¥æ ¼å¼æ˜¯å¦ä¸º \"ä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ï¼šæ•°å­—\"\n",
    "    rewards = []\n",
    "    for r in responses:\n",
    "        if is_valid_format(r):\n",
    "            rewards.append(0.5)  # æ ¼å¼æ­£ç¡®å¾—åˆ°1.0åˆ†\n",
    "        else:\n",
    "            rewards.append(0.0)  # æ ¼å¼ä¸æ­£ç¡®å¾—åˆ°0åˆ†\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def length_penalty_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Penalizes completions that are too long.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for r in responses:\n",
    "        # The ideal answer is ~7 tokens.\n",
    "        # Give 0 penalty for <= 15 tokens.\n",
    "        # Give an increasingly negative reward for anything longer.\n",
    "        if len(r) <= 15:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(-0.1 * (len(r) - 15)) # Penalize -0.1 for each token over 15\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é…ç½®å¹¶å¼€å§‹ GRPO è®­ç»ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_prompt_length = 896  # TSC çš„è¾“å…¥æ¯”è¾ƒé•¿\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    max_steps=5000,  # æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    save_steps=5000,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"outputs_tsc\",\n",
    "\n",
    "    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    dataloader_pin_memory=False,  # å…³é—­å›ºå®šå†…å­˜\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        correctness_reward_func,\n",
    "        format_reward_func,\n",
    "        length_penalty_func,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### æ£€æŸ¥è®­ç»ƒåçš„æƒé‡æ˜¯å¦è¢«ä¿®æ”¹\n",
    "import torch\n",
    "\n",
    "print(\"æ£€æŸ¥å¾®è°ƒåçš„æƒé‡...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# è·å– LoRA æ¨¡å—çš„æƒé‡\n",
    "lora_modules_with_weights = {}\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):\n",
    "        # æ£€æŸ¥æƒé‡æ˜¯å¦è¢«ä¿®æ”¹ï¼ˆéé›¶ï¼‰\n",
    "        A_norm = torch.norm(module.lora_A.default.weight if hasattr(module.lora_A, 'default') else module.lora_A.weight).item()\n",
    "        B_norm = torch.norm(module.lora_B.default.weight if hasattr(module.lora_B, 'default') else module.lora_B.weight).item()\n",
    "        \n",
    "        if A_norm > 0 or B_norm > 0:\n",
    "            lora_modules_with_weights[name] = (A_norm, B_norm)\n",
    "            print(f\"âœ“ {name}\")\n",
    "            print(f\"  - lora_A norm: {A_norm:.6f}\")\n",
    "            print(f\"  - lora_B norm: {B_norm:.6f}\")\n",
    "\n",
    "if not lora_modules_with_weights:\n",
    "    print(\"âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°éé›¶çš„ LoRA æƒé‡ï¼\")\n",
    "else:\n",
    "    print(f\"\\næ€»å…±æ‰¾åˆ° {len(lora_modules_with_weights)} ä¸ªæœ‰éé›¶æƒé‡çš„ LoRA æ¨¡å—\")\n",
    "\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿å­˜æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### æ”¹è¿›çš„æƒé‡ä¿å­˜æ–¹å¼ - æ–°çš„Cell\n",
    "from peft import get_peft_model_state_dict\n",
    "import os\n",
    "\n",
    "print(\"ä¿å­˜å¾®è°ƒåçš„æƒé‡...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# æ–¹æ³•1: ä¿å­˜ LoRA æƒé‡\n",
    "save_dir_1 = \"tsc_grpo_saved_lora_v2\"\n",
    "if os.path.exists(save_dir_1):\n",
    "    import shutil\n",
    "    shutil.rmtree(save_dir_1)\n",
    "\n",
    "model.save_pretrained(save_dir_1)\n",
    "print(f\"âœ“ å·²ä½¿ç”¨ save_pretrained ä¿å­˜æƒé‡åˆ°: {save_dir_1}\")\n",
    "\n",
    "# æ–¹æ³•2: ä¿å­˜å®Œæ•´çš„çŠ¶æ€å­—å…¸\n",
    "save_dir_2 = \"tsc_grpo_saved_lora_state_dict\"\n",
    "if os.path.exists(save_dir_2):\n",
    "    import shutil\n",
    "    shutil.rmtree(save_dir_2)\n",
    "os.makedirs(save_dir_2, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜ LoRA çŠ¶æ€å­—å…¸\n",
    "lora_state_dict = get_peft_model_state_dict(model)\n",
    "torch.save(lora_state_dict, os.path.join(save_dir_2, \"lora_state_dict.pt\"))\n",
    "print(f\"âœ“ å·²ä¿å­˜ LoRA çŠ¶æ€å­—å…¸åˆ°: {save_dir_2}\")\n",
    "\n",
    "# éªŒè¯ä¿å­˜çš„æƒé‡æ˜¯å¦éé›¶\n",
    "print(\"\\néªŒè¯ä¿å­˜çš„æƒé‡...\")\n",
    "for key in list(lora_state_dict.keys())[:5]:\n",
    "    val_norm = torch.norm(lora_state_dict[key]).item()\n",
    "    print(f\"  - {key}: norm = {val_norm:.6f}\")\n",
    "\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"tsc_grpo_saved_lora\")\n",
    "print(\"æ¨¡å‹å·²ä¿å­˜åˆ° tsc_grpo_saved_lora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¾®è°ƒåæµ‹è¯•æ¨¡å‹å‡†ç¡®ç‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# ============ æ–¹æ¡ˆAï¼šç›´æ¥ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼ˆæƒé‡å·²ä¿å­˜ï¼‰ ============\n",
    "# åŠ è½½å¾®è°ƒåçš„ LoRA æƒé‡å¹¶åˆå¹¶\n",
    "print(\"åŠ è½½å¾®è°ƒåçš„ LoRA æƒé‡...\")\n",
    "\n",
    "# é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ç”¨äºæ¨ç†\n",
    "base_model = model.get_base_model()\n",
    "\n",
    "# åŠ è½½ LoRA æƒé‡\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"tsc_grpo_saved_lora\",\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "print(\"LoRA æƒé‡å·²åˆå¹¶\")\n",
    "\n",
    "# ============ ä½¿ç”¨ Transformers è¿›è¡Œæ¨ç† ============\n",
    "def evaluate_with_vllm(tokenizer, test_dataset, merged_model, max_samples=100):\n",
    "    \"\"\"ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†å’Œå‡†ç¡®ç‡è¯„ä¼°\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_samples = min(max_samples, len(test_dataset))\n",
    "    \n",
    "    # å¯ç”¨æ¨ç†æ¨¡å¼\n",
    "    FastLanguageModel.for_inference(merged_model)\n",
    "    \n",
    "    for i in tqdm(range(test_samples), desc=\"æ¨ç†ä¸­\"):\n",
    "        item = test_dataset[i]\n",
    "        \n",
    "        # æ„å»ºè¾“å…¥\n",
    "        messages = item['prompt']\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(merged_model.device)\n",
    "        \n",
    "        # Transformers æ¨ç†å‚æ•°ï¼ˆå…¼å®¹ Unslothï¼‰\n",
    "        with torch.no_grad():\n",
    "            outputs = merged_model.generate(\n",
    "                input_ids=inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        # æå–ç”Ÿæˆçš„æ–‡æœ¬\n",
    "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # æå–é¢„æµ‹çš„ç›¸ä½\n",
    "        predicted_phase = extract_phase_answer(response)\n",
    "        true_phase = item['answer']\n",
    "        \n",
    "        if predicted_phase == true_phase:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # æ‰“å°å‰5ä¸ªæ ·ä¾‹\n",
    "        if i < 5:\n",
    "            print(f\"\\næ ·ä¾‹ {i+1}:\")\n",
    "            print(f\"çœŸå®ç›¸ä½: {true_phase}\")\n",
    "            print(f\"é¢„æµ‹ç›¸ä½: {predicted_phase}\")\n",
    "            print(f\"æ¨¡å‹å›ç­”: {response[:200]}...\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nå‡†ç¡®ç‡: {accuracy:.2%} ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"å¾®è°ƒåæ¨¡å‹å‡†ç¡®ç‡ï¼ˆvLLMï¼‰:\")\n",
    "print(\"=\"*50)\n",
    "accuracy_after = evaluate_with_vllm(tokenizer, test_dataset, merged_model, max_samples=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¹æ¯”å¾®è°ƒå‰åå‡†ç¡®ç‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"å‡†ç¡®ç‡å¯¹æ¯”\")\n",
    "print(\"=\"*50)\n",
    "print(f\"å¾®è°ƒå‰å‡†ç¡®ç‡: {accuracy_before:.2%}\")\n",
    "print(f\"å¾®è°ƒåå‡†ç¡®ç‡: {accuracy_after:.2%}\")\n",
    "print(f\"æå‡å¹…åº¦: {(accuracy_after - accuracy_before):.2%}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace å‘å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\n",
    "    \"DavidRay93/Qwen3-4B-TSC-GRPO-Test\",\n",
    "    tokenizer,\n",
    "    quantization_method=[\"f16\"],\n",
    "    token=\"YOUR_HUGGINGFACE_TOKEN_HERE\",\n",
    "    temporary_location=\"/root/autodl-tmp/saved_models\",  # æŒ‡å®šä¿å­˜å’Œè½¬æ¢çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
