{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TSC 交通信号控制微调 - Non-Thinking 版本\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths! # To enable memory efficient GRPO with vLLM\n",
        "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess \n",
        "import os \n",
        "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
        "output = result.stdout\n",
        "for line in output.splitlines():\n",
        "\tif '=' in line:\n",
        "\t\tvar, value = line.split('=', 1)\n",
        "\t\tos.environ[var] = value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 加载模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1024\n",
        "lora_rank = 32\n",
        "\n",
        "os.environ[\"HF_HOME\"] = 'model'\n",
        "os.environ[\"MODELSCOPE_CACHE\"] = 'model'\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Instruct-2507\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,\n",
        "    fast_inference = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.9,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 加载 TSC 数据集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 加载 TSC 数据集\n",
        "with open('./data_TSC/tsc_sft_dataset.json', 'r', encoding='utf-8') as f:\n",
        "    tsc_data = json.load(f)\n",
        "\n",
        "print(f\"总数据量: {len(tsc_data)}\")\n",
        "\n",
        "# 分割数据集：95% 训练，5% 测试\n",
        "train_data, test_data = train_test_split(tsc_data, test_size=0.05, random_state=42)\n",
        "\n",
        "print(f\"训练集大小: {len(train_data)}\")\n",
        "print(f\"测试集大小: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 提取答案函数 - 严格格式要求\n",
        "def extract_phase_answer(text: str) -> str | None:\n",
        "    \"\"\"从输出中提取相位数字，严格要求格式为：下一个信号相位：数字\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return None\n",
        "    \n",
        "    # 规范化：去除多余空格和特殊字符\n",
        "    text = text.strip().replace(' ', '')\n",
        "    \n",
        "    # 尝试多种分隔符和格式\n",
        "    patterns = [\n",
        "        r'下一个信号相位[:：]\\d+',  # 原始格式\n",
        "        r'下一个信号相位[:：]\\s*(\\d+)',  # 带空格\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            # 提取数字\n",
        "            digits = re.findall(r'\\d+', match.group(0))\n",
        "            if digits:\n",
        "                return digits[-1]  # 返回最后一个数字\n",
        "    \n",
        "    return None\n",
        "\n",
        "# def extract_phase_answer(text: str) -> str | None:\n",
        "#     \"\"\"从输出中提取相位数字，严格要求格式为：下一个信号相位：数字\"\"\"\n",
        "#     # 只匹配严格格式：下一个信号相位：数字\n",
        "#     pattern = r'下一个信号相位[:：]\\s*(\\d+)'\n",
        "#     match = re.search(pattern, text)\n",
        "#     if match:\n",
        "#         return match.group(1)\n",
        "#     return None\n",
        "\n",
        "# 准备训练数据集\n",
        "def prepare_dataset(data):\n",
        "    dataset_list = []\n",
        "    for item in data:\n",
        "        # 修改系统提示，强制格式为 \"下一个信号相位：数字\"\n",
        "        system_prompt = \"你是一位交通管理专家。你可以运用你的交通常识知识来解决交通信号控制任务。根据给定的交通场景和状态，预测下一个信号相位。你必须直接回答，格式必须是：下一个信号相位：{数字}（其中数字是0-9之间的单个数字）\"\n",
        "        \n",
        "        prompt = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": item[\"input\"]},\n",
        "        ]\n",
        "        answer = extract_phase_answer(item[\"output\"])\n",
        "        dataset_list.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"answer\": answer,\n",
        "        })\n",
        "    return Dataset.from_list(dataset_list)\n",
        "\n",
        "train_dataset = prepare_dataset(train_data)\n",
        "test_dataset = prepare_dataset(test_data)\n",
        "\n",
        "print(f\"训练集样例:\")\n",
        "print(f\"Prompt: {train_dataset[0]['prompt']}\")\n",
        "print(f\"Answer: {train_dataset[0]['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 微调前测试模型准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# def evaluate_model(model, tokenizer, test_dataset, max_samples=100):\n",
        "#     \"\"\"评估模型在测试集上的准确率\"\"\"\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "    \n",
        "#     # 只测试前 max_samples 个样本以节省时间\n",
        "#     test_samples = min(max_samples, len(test_dataset))\n",
        "    \n",
        "#     FastLanguageModel.for_inference(model)  # 启用推理模式\n",
        "    \n",
        "#     for i in tqdm(range(test_samples), desc=\"评估中\"):\n",
        "#         item = test_dataset[i]\n",
        "        \n",
        "#         # 构建输入\n",
        "#         messages = item['prompt']\n",
        "#         inputs = tokenizer.apply_chat_template(\n",
        "#             messages,\n",
        "#             tokenize=True,\n",
        "#             add_generation_prompt=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         ).to(model.device)\n",
        "        \n",
        "#         # 生成回答\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=inputs,\n",
        "#             max_new_tokens=128,\n",
        "#             temperature=0.7,\n",
        "#             do_sample=True,\n",
        "#             pad_token_id=tokenizer.pad_token_id,\n",
        "#         )\n",
        "        \n",
        "#         # 解码输出\n",
        "#         response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "        \n",
        "#         # 提取预测的相位\n",
        "#         predicted_phase = extract_phase_answer(response)\n",
        "#         true_phase = item['answer']\n",
        "        \n",
        "#         if predicted_phase == true_phase:\n",
        "#             correct += 1\n",
        "#         total += 1\n",
        "        \n",
        "#         # 打印前5个样例\n",
        "#         if i < 5:\n",
        "#             print(f\"\\n样例 {i+1}:\")\n",
        "#             print(f\"真实相位: {true_phase}\")\n",
        "#             print(f\"预测相位: {predicted_phase}\")\n",
        "#             print(f\"模型回答: {response[:200]}...\")\n",
        "    \n",
        "#     accuracy = correct / total if total > 0 else 0\n",
        "#     print(f\"\\n准确率: {accuracy:.2%} ({correct}/{total})\")\n",
        "#     return accuracy\n",
        "\n",
        "# print(\"=\"*50)\n",
        "# print(\"微调前模型准确率:\")\n",
        "# print(\"=\"*50)\n",
        "# accuracy_before = evaluate_model(model, tokenizer, test_dataset, max_samples=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 定义奖励函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "# 验证输出格式\n",
        "def is_valid_format(text: str) -> bool:\n",
        "    \"\"\"验证文本是否满足严格格式：下一个信号相位：数字\"\"\"\n",
        "    pattern = r'^下一个信号相位[:：]\\s*\\d+\\s*$'\n",
        "    return bool(re.match(pattern, text.strip()))\n",
        "\n",
        "# 奖励函数：检查预测的相位是否正确\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    q = prompts[0][-1][\"content\"][:100]  # 只显示前100字符\n",
        "    extracted_responses = [extract_phase_answer(r) for r in responses]\n",
        "    \n",
        "    print(\n",
        "        \"-\" * 20,\n",
        "        f\"\\n问题:\\n{q}...\",\n",
        "        f\"\\n正确答案:\\n{answer[0]}\",\n",
        "        f\"\\n模型回答:\\n{responses[0][:150]}...\",\n",
        "        f\"\\n提取结果:\\n{extracted_responses[0]}\",\n",
        "    )\n",
        "    result = [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "    del responses, extracted_responses\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "# 奖励函数：严格验证格式 - 下一个信号相位：数字\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    # 严格检查格式是否为 \"下一个信号相位：数字\"\n",
        "    rewards = []\n",
        "    for r in responses:\n",
        "        if is_valid_format(r):\n",
        "            rewards.append(0.5)  # 格式正确得到1.0分\n",
        "        else:\n",
        "            rewards.append(0.0)  # 格式不正确得到0分\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def length_penalty_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Penalizes completions that are too long.\"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = []\n",
        "    for r in responses:\n",
        "        # The ideal answer is ~7 tokens.\n",
        "        # Give 0 penalty for <= 15 tokens.\n",
        "        # Give an increasingly negative reward for anything longer.\n",
        "        if len(r) <= 15:\n",
        "            rewards.append(0.0)\n",
        "        else:\n",
        "            rewards.append(-0.1 * (len(r) - 15)) # Penalize -0.1 for each token over 15\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 配置并开始 GRPO 训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "max_prompt_length = 896  # TSC 的输入比较长\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=max_prompt_length,\n",
        "    max_completion_length=max_seq_length - max_prompt_length,\n",
        "    max_steps=5000,  # 根据需要调整\n",
        "    save_steps=5000,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"outputs_tsc\",\n",
        "\n",
        "    gradient_checkpointing=True,  # 启用梯度检查点\n",
        "    dataloader_pin_memory=False,  # 关闭固定内存\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        correctness_reward_func,\n",
        "        format_reward_func,\n",
        "        length_penalty_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"开始训练...\")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 检查训练后的权重是否被修改\n",
        "import torch\n",
        "\n",
        "print(\"检查微调后的权重...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 获取 LoRA 模块的权重\n",
        "lora_modules_with_weights = {}\n",
        "for name, module in model.named_modules():\n",
        "    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):\n",
        "        # 检查权重是否被修改（非零）\n",
        "        A_norm = torch.norm(module.lora_A.default.weight if hasattr(module.lora_A, 'default') else module.lora_A.weight).item()\n",
        "        B_norm = torch.norm(module.lora_B.default.weight if hasattr(module.lora_B, 'default') else module.lora_B.weight).item()\n",
        "        \n",
        "        if A_norm > 0 or B_norm > 0:\n",
        "            lora_modules_with_weights[name] = (A_norm, B_norm)\n",
        "            print(f\"✓ {name}\")\n",
        "            print(f\"  - lora_A norm: {A_norm:.6f}\")\n",
        "            print(f\"  - lora_B norm: {B_norm:.6f}\")\n",
        "\n",
        "if not lora_modules_with_weights:\n",
        "    print(\"⚠️  警告: 没有找到非零的 LoRA 权重！\")\n",
        "else:\n",
        "    print(f\"\\n总共找到 {len(lora_modules_with_weights)} 个有非零权重的 LoRA 模块\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 保存模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 改进的权重保存方式 - 新的Cell\n",
        "from peft import get_peft_model_state_dict\n",
        "import os\n",
        "\n",
        "print(\"保存微调后的权重...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 方法1: 保存 LoRA 权重\n",
        "save_dir_1 = \"tsc_grpo_saved_lora_v2\"\n",
        "if os.path.exists(save_dir_1):\n",
        "    import shutil\n",
        "    shutil.rmtree(save_dir_1)\n",
        "\n",
        "model.save_pretrained(save_dir_1)\n",
        "print(f\"✓ 已使用 save_pretrained 保存权重到: {save_dir_1}\")\n",
        "\n",
        "# 方法2: 保存完整的状态字典\n",
        "save_dir_2 = \"tsc_grpo_saved_lora_state_dict\"\n",
        "if os.path.exists(save_dir_2):\n",
        "    import shutil\n",
        "    shutil.rmtree(save_dir_2)\n",
        "os.makedirs(save_dir_2, exist_ok=True)\n",
        "\n",
        "# 保存 LoRA 状态字典\n",
        "lora_state_dict = get_peft_model_state_dict(model)\n",
        "torch.save(lora_state_dict, os.path.join(save_dir_2, \"lora_state_dict.pt\"))\n",
        "print(f\"✓ 已保存 LoRA 状态字典到: {save_dir_2}\")\n",
        "\n",
        "# 验证保存的权重是否非零\n",
        "print(\"\\n验证保存的权重...\")\n",
        "for key in list(lora_state_dict.keys())[:5]:\n",
        "    val_norm = torch.norm(lora_state_dict[key]).item()\n",
        "    print(f\"  - {key}: norm = {val_norm:.6f}\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_lora(\"tsc_grpo_saved_lora\")\n",
        "print(\"模型已保存到 tsc_grpo_saved_lora\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 微调后测试模型准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# ============ 方案A：直接使用微调后的模型（权重已保存） ============\n",
        "# 加载微调后的 LoRA 权重并合并\n",
        "print(\"加载微调后的 LoRA 权重...\")\n",
        "\n",
        "# 重新加载基础模型用于推理\n",
        "base_model = model.get_base_model()\n",
        "\n",
        "# 加载 LoRA 权重\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"tsc_grpo_saved_lora\",\n",
        "    is_trainable=False\n",
        ")\n",
        "\n",
        "# 合并 LoRA 权重到基础模型\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "print(\"LoRA 权重已合并\")\n",
        "\n",
        "# ============ 使用 Transformers 进行推理 ============\n",
        "def evaluate_with_vllm(tokenizer, test_dataset, merged_model, max_samples=100):\n",
        "    \"\"\"使用微调后的模型进行推理和准确率评估\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    test_samples = min(max_samples, len(test_dataset))\n",
        "    \n",
        "    # 启用推理模式\n",
        "    FastLanguageModel.for_inference(merged_model)\n",
        "    \n",
        "    for i in tqdm(range(test_samples), desc=\"推理中\"):\n",
        "        item = test_dataset[i]\n",
        "        \n",
        "        # 构建输入\n",
        "        messages = item['prompt']\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(merged_model.device)\n",
        "        \n",
        "        # Transformers 推理参数（兼容 Unsloth）\n",
        "        with torch.no_grad():\n",
        "            outputs = merged_model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=128,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "        \n",
        "        # 提取生成的文本\n",
        "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "        \n",
        "        # 提取预测的相位\n",
        "        predicted_phase = extract_phase_answer(response)\n",
        "        true_phase = item['answer']\n",
        "        \n",
        "        if predicted_phase == true_phase:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        # 打印前5个样例\n",
        "        if i < 5:\n",
        "            print(f\"\\n样例 {i+1}:\")\n",
        "            print(f\"真实相位: {true_phase}\")\n",
        "            print(f\"预测相位: {predicted_phase}\")\n",
        "            print(f\"模型回答: {response[:200]}...\")\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"\\n准确率: {accuracy:.2%} ({correct}/{total})\")\n",
        "    return accuracy\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"微调后模型准确率（vLLM）:\")\n",
        "print(\"=\"*50)\n",
        "accuracy_after = evaluate_with_vllm(tokenizer, test_dataset, merged_model, max_samples=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 对比微调前后准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"准确率对比\")\n",
        "print(\"=\"*50)\n",
        "print(f\"微调前准确率: {accuracy_before:.2%}\")\n",
        "print(f\"微调后准确率: {accuracy_after:.2%}\")\n",
        "print(f\"提升幅度: {(accuracy_after - accuracy_before):.2%}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace 发布"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\n",
        "    \"DavidRay93/Qwen3-4B-TSC-GRPO-Test\",\n",
        "    tokenizer,\n",
        "    quantization_method=[\"f16\"],\n",
        "    token=\"YOUR_HUGGINGFACE_TOKEN_HERE\",\n",
        "    temporary_location=\"/root/autodl-tmp/saved_models\",  # 指定保存和转换的文件夹路径\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
