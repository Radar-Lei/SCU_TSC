{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TSC 交通信号控制微调 - Non-Thinking 版本\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths! # To enable memory efficient GRPO with vLLM\n",
        "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess \n",
        "import os \n",
        "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
        "output = result.stdout\n",
        "for line in output.splitlines():\n",
        "\tif '=' in line:\n",
        "\t\tvar, value = line.split('=', 1)\n",
        "\t\tos.environ[var] = value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 加载模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "lora_rank = 32\n",
        "\n",
        "os.environ[\"HF_HOME\"] = 'model'\n",
        "os.environ[\"MODELSCOPE_CACHE\"] = 'model'\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Instruct-2507\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,\n",
        "    fast_inference = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.85,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 加载 TSC 数据集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 加载 TSC 数据集\n",
        "with open('./data_TSC/tsc_sft_dataset.json', 'r', encoding='utf-8') as f:\n",
        "    tsc_data = json.load(f)\n",
        "\n",
        "print(f\"总数据量: {len(tsc_data)}\")\n",
        "\n",
        "# 分割数据集：95% 训练，5% 测试\n",
        "train_data, test_data = train_test_split(tsc_data, test_size=0.05, random_state=42)\n",
        "\n",
        "print(f\"训练集大小: {len(train_data)}\")\n",
        "print(f\"测试集大小: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 提取答案函数 - 严格格式要求\n",
        "def extract_phase_answer(text: str) -> str | None:\n",
        "    \"\"\"从输出中提取相位数字，严格要求格式为：下一个信号相位：数字\"\"\"\n",
        "    # 只匹配严格格式：下一个信号相位：数字\n",
        "    pattern = r'下一个信号相位[:：]\\s*(\\d+)'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "# 准备训练数据集\n",
        "def prepare_dataset(data):\n",
        "    dataset_list = []\n",
        "    for item in data:\n",
        "        # 修改系统提示，强制格式为 \"下一个信号相位：数字\"\n",
        "        system_prompt = \"你是一位交通管理专家。你可以运用你的交通常识知识来解决交通信号控制任务。根据给定的交通场景和状态，预测下一个信号相位。你必须直接回答，格式必须是：下一个信号相位：{数字}（其中数字是0-9之间的单个数字）\"\n",
        "        \n",
        "        prompt = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": item[\"input\"]},\n",
        "        ]\n",
        "        answer = extract_phase_answer(item[\"output\"])\n",
        "        dataset_list.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"answer\": answer,\n",
        "        })\n",
        "    return Dataset.from_list(dataset_list)\n",
        "\n",
        "train_dataset = prepare_dataset(train_data)\n",
        "test_dataset = prepare_dataset(test_data)\n",
        "\n",
        "print(f\"训练集样例:\")\n",
        "print(f\"Prompt: {train_dataset[0]['prompt']}\")\n",
        "print(f\"Answer: {train_dataset[0]['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 微调前测试模型准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_dataset, max_samples=100):\n",
        "    \"\"\"评估模型在测试集上的准确率\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # 只测试前 max_samples 个样本以节省时间\n",
        "    test_samples = min(max_samples, len(test_dataset))\n",
        "    \n",
        "    FastLanguageModel.for_inference(model)  # 启用推理模式\n",
        "    \n",
        "    for i in tqdm(range(test_samples), desc=\"评估中\"):\n",
        "        item = test_dataset[i]\n",
        "        \n",
        "        # 构建输入\n",
        "        messages = item['prompt']\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "        \n",
        "        # 生成回答\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=128,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "        \n",
        "        # 解码输出\n",
        "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "        \n",
        "        # 提取预测的相位\n",
        "        predicted_phase = extract_phase_answer(response)\n",
        "        true_phase = item['answer']\n",
        "        \n",
        "        if predicted_phase == true_phase:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        # 打印前5个样例\n",
        "        if i < 5:\n",
        "            print(f\"\\n样例 {i+1}:\")\n",
        "            print(f\"真实相位: {true_phase}\")\n",
        "            print(f\"预测相位: {predicted_phase}\")\n",
        "            print(f\"模型回答: {response[:200]}...\")\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"\\n准确率: {accuracy:.2%} ({correct}/{total})\")\n",
        "    return accuracy\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"微调前模型准确率:\")\n",
        "print(\"=\"*50)\n",
        "accuracy_before = evaluate_model(model, tokenizer, test_dataset, max_samples=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 定义奖励函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 验证输出格式\n",
        "def is_valid_format(text: str) -> bool:\n",
        "    \"\"\"验证文本是否满足严格格式：下一个信号相位：数字\"\"\"\n",
        "    pattern = r'^下一个信号相位[:：]\\s*\\d+\\s*$'\n",
        "    return bool(re.match(pattern, text.strip()))\n",
        "\n",
        "# 奖励函数：检查预测的相位是否正确\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    q = prompts[0][-1][\"content\"][:100]  # 只显示前100字符\n",
        "    extracted_responses = [extract_phase_answer(r) for r in responses]\n",
        "    \n",
        "    print(\n",
        "        \"-\" * 20,\n",
        "        f\"\\n问题:\\n{q}...\",\n",
        "        f\"\\n正确答案:\\n{answer[0]}\",\n",
        "        f\"\\n模型回答:\\n{responses[0][:150]}...\",\n",
        "        f\"\\n提取结果:\\n{extracted_responses[0]}\",\n",
        "    )\n",
        "    \n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "# 奖励函数：严格验证格式 - 下一个信号相位：数字\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    # 严格检查格式是否为 \"下一个信号相位：数字\"\n",
        "    rewards = []\n",
        "    for r in responses:\n",
        "        if is_valid_format(r):\n",
        "            rewards.append(0.5)  # 格式正确得到1.0分\n",
        "        else:\n",
        "            rewards.append(0.0)  # 格式不正确得到0分\n",
        "    return rewards\n",
        "\n",
        "# 奖励函数：检查答案是否为数字\n",
        "def digit_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_responses = [extract_phase_answer(r) for r in responses]\n",
        "    return [0.1 if r and r.isdigit() else 0.0 for r in extracted_responses]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 配置并开始 GRPO 训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "max_prompt_length = 512  # TSC 的输入比较长\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_generations=6,\n",
        "    max_prompt_length=max_prompt_length,\n",
        "    max_completion_length=max_seq_length - max_prompt_length,\n",
        "    max_steps=100,  # 根据需要调整\n",
        "    save_steps=100,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"outputs_tsc\",\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        correctness_reward_func,\n",
        "        digit_reward_func,\n",
        "        format_reward_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"开始训练...\")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 检查训练后的权重是否被修改\n",
        "import torch\n",
        "\n",
        "print(\"检查微调后的权重...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 获取 LoRA 模块的权重\n",
        "lora_modules_with_weights = {}\n",
        "for name, module in model.named_modules():\n",
        "    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):\n",
        "        # 检查权重是否被修改（非零）\n",
        "        A_norm = torch.norm(module.lora_A.default.weight if hasattr(module.lora_A, 'default') else module.lora_A.weight).item()\n",
        "        B_norm = torch.norm(module.lora_B.default.weight if hasattr(module.lora_B, 'default') else module.lora_B.weight).item()\n",
        "        \n",
        "        if A_norm > 0 or B_norm > 0:\n",
        "            lora_modules_with_weights[name] = (A_norm, B_norm)\n",
        "            print(f\"✓ {name}\")\n",
        "            print(f\"  - lora_A norm: {A_norm:.6f}\")\n",
        "            print(f\"  - lora_B norm: {B_norm:.6f}\")\n",
        "\n",
        "if not lora_modules_with_weights:\n",
        "    print(\"⚠️  警告: 没有找到非零的 LoRA 权重！\")\n",
        "else:\n",
        "    print(f\"\\n总共找到 {len(lora_modules_with_weights)} 个有非零权重的 LoRA 模块\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 保存模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 改进的权重保存方式 - 新的Cell\n",
        "from peft import get_peft_model_state_dict\n",
        "import os\n",
        "\n",
        "print(\"保存微调后的权重...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 方法1: 保存 LoRA 权重\n",
        "save_dir_1 = \"tsc_grpo_saved_lora_v2\"\n",
        "if os.path.exists(save_dir_1):\n",
        "    import shutil\n",
        "    shutil.rmtree(save_dir_1)\n",
        "\n",
        "model.save_pretrained(save_dir_1)\n",
        "print(f\"✓ 已使用 save_pretrained 保存权重到: {save_dir_1}\")\n",
        "\n",
        "# 方法2: 保存完整的状态字典\n",
        "save_dir_2 = \"tsc_grpo_saved_lora_state_dict\"\n",
        "if os.path.exists(save_dir_2):\n",
        "    import shutil\n",
        "    shutil.rmtree(save_dir_2)\n",
        "os.makedirs(save_dir_2, exist_ok=True)\n",
        "\n",
        "# 保存 LoRA 状态字典\n",
        "lora_state_dict = get_peft_model_state_dict(model)\n",
        "torch.save(lora_state_dict, os.path.join(save_dir_2, \"lora_state_dict.pt\"))\n",
        "print(f\"✓ 已保存 LoRA 状态字典到: {save_dir_2}\")\n",
        "\n",
        "# 验证保存的权重是否非零\n",
        "print(\"\\n验证保存的权重...\")\n",
        "for key in list(lora_state_dict.keys())[:5]:\n",
        "    val_norm = torch.norm(lora_state_dict[key]).item()\n",
        "    print(f\"  - {key}: norm = {val_norm:.6f}\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_lora(\"tsc_grpo_saved_lora\")\n",
        "print(\"模型已保存到 tsc_grpo_saved_lora\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 改进的权重加载和评估方式\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "print(\"\\n改进的权重加载方式...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 获取原始基础模型\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 从保存的 LoRA 适配器加载\n",
        "lora_model_id = \"tsc_grpo_saved_lora_v2\"\n",
        "\n",
        "if os.path.exists(lora_model_id):\n",
        "    print(f\"从 {lora_model_id} 加载 LoRA 权重...\")\n",
        "    \n",
        "    # 获取基础模型（当前模型）\n",
        "    base_model = model.base_model\n",
        "    \n",
        "    # 使用 PeftModel 加载 LoRA 权重\n",
        "    model_with_lora = PeftModel.from_pretrained(base_model, lora_model_id)\n",
        "    print(f\"✓ 已成功加载 LoRA 权重\")\n",
        "    \n",
        "    # 合并权重到基础模型中\n",
        "    merged_model = model_with_lora.merge_and_unload()\n",
        "    print(f\"✓ 已将 LoRA 权重合并到基础模型\")\n",
        "    \n",
        "    # 现在使用合并后的模型进行评估\n",
        "    print(\"\\n使用合并后的模型进行评估...\")\n",
        "    \n",
        "    def evaluate_merged_model(model, tokenizer, test_dataset, max_samples=100):\n",
        "        \"\"\"使用合并后的模型评估准确率\"\"\"\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        test_samples = min(max_samples, len(test_dataset))\n",
        "        FastLanguageModel.for_inference(model)\n",
        "        \n",
        "        for i in tqdm(range(test_samples), desc=\"评估中\"):\n",
        "            item = test_dataset[i]\n",
        "            messages = item['prompt']\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=True,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "            \n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=128,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "            \n",
        "            response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "            predicted_phase = extract_phase_answer(response)\n",
        "            true_phase = item['answer']\n",
        "            \n",
        "            if predicted_phase == true_phase:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "            \n",
        "            if i < 1:\n",
        "                print(f\"\\n样例 {i+1}:\")\n",
        "                print(f\"真实相位: {true_phase}\")\n",
        "                print(f\"预测相位: {predicted_phase}\")\n",
        "                print(f\"模型回答: {response[:200]}...\")\n",
        "        \n",
        "        accuracy = correct / total if total > 0 else 0\n",
        "        print(f\"\\n准确率: {accuracy:.2%} ({correct}/{total})\")\n",
        "        return accuracy\n",
        "    \n",
        "    accuracy_after_merged = evaluate_merged_model(merged_model, tokenizer, test_dataset, max_samples=500)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"准确率对比（使用合并后的模型）\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"微调前准确率: {accuracy_before:.2%}\")\n",
        "    print(f\"微调后准确率: {accuracy_after_merged:.2%}\")\n",
        "    print(f\"提升幅度: {(accuracy_after_merged - accuracy_before):.2%}\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(f\"❌ 错误: 找不到 {lora_model_id} 目录\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 微调后测试模型准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 诊断：比较原始权重和微调后权重\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"诊断：分析权重变化\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. 检查保存的 LoRA 权重文件\n",
        "saved_lora_path = \"tsc_grpo_saved_lora/adapter_model.safetensors\"\n",
        "if os.path.exists(saved_lora_path):\n",
        "    print(f\"\\n✓ 发现保存的 LoRA 权重文件: {saved_lora_path}\")\n",
        "    \n",
        "    # 使用 safetensors 加载\n",
        "    from safetensors.torch import load_file\n",
        "    lora_weights = load_file(saved_lora_path)\n",
        "    \n",
        "    print(f\"  - 权重数量: {len(lora_weights)}\")\n",
        "    \n",
        "    # 检查权重是否非零\n",
        "    zero_count = 0\n",
        "    nonzero_count = 0\n",
        "    \n",
        "    for key, val in lora_weights.items():\n",
        "        if torch.sum(torch.abs(val)) == 0:\n",
        "            zero_count += 1\n",
        "        else:\n",
        "            nonzero_count += 1\n",
        "            norm = torch.norm(val).item()\n",
        "            if nonzero_count <= 3:\n",
        "                print(f\"  - {key}: norm = {norm:.6f}\")\n",
        "    \n",
        "    print(f\"\\n  非零权重数: {nonzero_count}\")\n",
        "    print(f\"  零权重数: {zero_count}\")\n",
        "    \n",
        "    if zero_count == len(lora_weights):\n",
        "        print(\"\\n⚠️  严重警告: 所有 LoRA 权重都是零！\")\n",
        "        print(\"  可能原因:\")\n",
        "        print(\"    1. 训练过程没有实际更新权重\")\n",
        "        print(\"    2. 学习率过低\")\n",
        "        print(\"    3. 优化器配置有问题\")\n",
        "        print(\"    4. 奖励信号不足以驱动学习\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n❌ 找不到文件: {saved_lora_path}\")\n",
        "\n",
        "# 2. 检查保存的不同 checkpoint 的权重\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"检查多个 checkpoints 的权重:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for checkpoint_dir in [\"outputs_tsc/checkpoint-250\", \"outputs_tsc/checkpoint-500\"]:\n",
        "    checkpoint_lora = f\"{checkpoint_dir}/adapter_model.safetensors\"\n",
        "    if os.path.exists(checkpoint_lora):\n",
        "        lora_weights = load_file(checkpoint_lora)\n",
        "        \n",
        "        nonzero_count = sum(1 for val in lora_weights.values() if torch.sum(torch.abs(val)) > 0)\n",
        "        total_params = sum(val.numel() for val in lora_weights.values())\n",
        "        \n",
        "        print(f\"\\n{checkpoint_dir}:\")\n",
        "        print(f\"  - 非零权重数: {nonzero_count}/{len(lora_weights)}\")\n",
        "        print(f\"  - 总参数数: {total_params:,}\")\n",
        "        \n",
        "        # 找到最大的权重值\n",
        "        max_norm = max((torch.norm(val).item() for val in lora_weights.values()), default=0)\n",
        "        print(f\"  - 最大权重范数: {max_norm:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "微调后模型准确率 (使用保存的LoRA权重):\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 使用 vLLM 方式加载保存的 LoRA 权重\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m lora_request = \u001b[43mmodel\u001b[49m.load_lora(\u001b[33m\"\u001b[39m\u001b[33mtsc_grpo_saved_lora\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ 已成功加载保存的 LoRA 权重\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 使用加载的 LoRA 进行评估\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"微调后模型准确率 (使用保存的LoRA权重):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 使用 vLLM 方式加载保存的 LoRA 权重\n",
        "lora_request = model.load_lora(\"tsc_grpo_saved_lora\")\n",
        "print(\"✓ 已成功加载保存的 LoRA 权重\")\n",
        "\n",
        "# 使用加载的 LoRA 进行评估\n",
        "from vllm import SamplingParams\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model_with_lora(model, tokenizer, test_dataset, lora_request, max_samples=100):\n",
        "    \"\"\"使用 vLLM 和加载的 LoRA 权重评估模型准确率\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # 只测试前 max_samples 个样本以节省时间\n",
        "    test_samples = min(max_samples, len(test_dataset))\n",
        "    \n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.7,\n",
        "        max_tokens=128,\n",
        "    )\n",
        "    \n",
        "    for i in tqdm(range(test_samples), desc=\"评估中\"):\n",
        "        item = test_dataset[i]\n",
        "        \n",
        "        # 构建输入\n",
        "        messages = item['prompt']\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "        \n",
        "        # 使用 fast_generate 和 lora_request 生成回答\n",
        "        output = model.fast_generate(\n",
        "            text,\n",
        "            sampling_params=sampling_params,\n",
        "            lora_request=lora_request,\n",
        "        )[0].outputs[0].text\n",
        "        \n",
        "        # 提取预测的相位\n",
        "        predicted_phase = extract_phase_answer(output)\n",
        "        true_phase = item['answer']\n",
        "        \n",
        "        if predicted_phase == true_phase:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        # 打印前5个样例\n",
        "        if i < 1:\n",
        "            print(f\"\\n样例 {i+1}:\")\n",
        "            print(f\"真实相位: {true_phase}\")\n",
        "            print(f\"预测相位: {predicted_phase}\")\n",
        "            print(f\"模型回答: {output[:200]}...\")\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"\\n准确率: {accuracy:.2%} ({correct}/{total})\")\n",
        "    return accuracy\n",
        "\n",
        "accuracy_after = evaluate_model_with_lora(model, tokenizer, test_dataset, lora_request, max_samples=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "【诊断】检查 LoRA 权重是否被微调\n",
            "======================================================================\n",
            "\n",
            "权重文件统计:\n",
            "  - 权重张量数: 504\n",
            "\n",
            "检查权重数值...\n",
            "\n",
            "权重范数排序（前10个）:\n",
            "  1. ✓ base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight: 3.28125000\n",
            "  2. ✓ base_model.model.model.layers.10.mlp.up_proj.lora_A.weight: 3.28125000\n",
            "  3. ✓ base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight: 3.28125000\n",
            "  4. ✓ base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight: 3.28125000\n",
            "  5. ✓ base_model.model.model.layers.26.mlp.up_proj.lora_A.weight: 3.28125000\n",
            "  6. ✓ base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight: 3.28125000\n",
            "  7. ✓ base_model.model.model.layers.32.self_attn.q_proj.lora_A.weight: 3.28125000\n",
            "  8. ✓ base_model.model.model.layers.33.self_attn.o_proj.lora_A.weight: 3.28125000\n",
            "  9. ✓ base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight: 3.28125000\n",
            "  10. ✓ base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight: 3.28125000\n",
            "\n",
            "统计结果:\n",
            "  ✓ 非零权重数: 504/504\n",
            "  ✗ 零权重数: 0/504\n",
            "\n",
            "✓ 权重已被正确更新！\n",
            "  非零权重占比: 100.0%\n",
            "  最大权重范数: 3.28125000\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "### 【快速诊断】检查权重是否真的被微调\n",
        "\n",
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"【诊断】检查 LoRA 权重是否被微调\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "lora_weights = load_file(\"tsc_grpo_saved_lora/adapter_model.safetensors\")\n",
        "\n",
        "print(f\"\\n权重文件统计:\")\n",
        "print(f\"  - 权重张量数: {len(lora_weights)}\")\n",
        "\n",
        "print(f\"\\n检查权重数值...\")\n",
        "nonzero_count = 0\n",
        "weight_norms = []\n",
        "\n",
        "for key, val in lora_weights.items():\n",
        "    norm = torch.norm(val).item()\n",
        "    weight_norms.append((key, norm))\n",
        "    \n",
        "    if norm > 0:\n",
        "        nonzero_count += 1\n",
        "\n",
        "# 排序并显示\n",
        "weight_norms.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\n权重范数排序（前10个）:\")\n",
        "for i, (key, norm) in enumerate(weight_norms[:10], 1):\n",
        "    status = \"✓\" if norm > 0 else \"✗\"\n",
        "    print(f\"  {i}. {status} {key}: {norm:.8f}\")\n",
        "\n",
        "print(f\"\\n统计结果:\")\n",
        "print(f\"  ✓ 非零权重数: {nonzero_count}/{len(lora_weights)}\")\n",
        "print(f\"  ✗ 零权重数: {len(lora_weights) - nonzero_count}/{len(lora_weights)}\")\n",
        "\n",
        "if nonzero_count == 0:\n",
        "    print(\"\\n\" + \"⚠️ \"*35)\n",
        "    print(\"严重问题: 所有 LoRA 权重都是零！\")\n",
        "    print(\"可能原因:\")\n",
        "    print(\"  1. 学习率过低（当前: 5e-6，建议: 1e-4）\")\n",
        "    print(\"  2. 奖励函数没有正确计算\")\n",
        "    print(\"  3. 梯度未能正确传播\")\n",
        "    print(\"  4. 训练中没有梯度更新\")\n",
        "    print(\"\\n建议: 需要重新训练，使用更大的学习率\")\n",
        "    print(\"⚠️ \"*35)\n",
        "else:\n",
        "    print(f\"\\n✓ 权重已被正确更新！\")\n",
        "    print(f\"  非零权重占比: {nonzero_count/len(lora_weights)*100:.1f}%\")\n",
        "    print(f\"  最大权重范数: {max(norm for _, norm in weight_norms):.8f}\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 对比微调前后准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"准确率对比\")\n",
        "print(\"=\"*50)\n",
        "print(f\"微调前准确率: {accuracy_before:.2%}\")\n",
        "print(f\"微调后准确率: {accuracy_after:.2%}\")\n",
        "print(f\"提升幅度: {(accuracy_after - accuracy_before):.2%}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace 发布"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\n",
        "    \"DavidRay93/Qwen3-4B-TSC-GRPO-Test\",\n",
        "    tokenizer,\n",
        "    quantization_method=[\"f16\"],\n",
        "    token=\"YOUR_HUGGINGFACE_TOKEN_HERE\",\n",
        "    temporary_location=\"/root/autodl-tmp/saved_models\",  # 指定保存和转换的文件夹路径\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
