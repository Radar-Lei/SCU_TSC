{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4885fa",
   "metadata": {},
   "source": [
    "# TSC æ ‡å‡† Unsloth GRPO è®­ç»ƒï¼ˆä¸¤å¤§åœºæ™¯ï¼‰\n",
    "\n",
    "ä½¿ç”¨æ ‡å‡† Unsloth GRPOTrainer + ç¦»çº¿ Dataset + Reward Function å›æº¯ SUMO è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149eb79c",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40758b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯å¢ƒå˜é‡å·²è®¾ç½®\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "print(\"ç¯å¢ƒå˜é‡å·²è®¾ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b9c5",
   "metadata": {},
   "source": [
    "## 1.5 ç”Ÿæˆ/æ£€æŸ¥ Datasetï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœ dataset ä¸å­˜åœ¨ï¼Œæ­¤ cell ä¼šè‡ªåŠ¨ç”Ÿæˆï¼›å¦‚æœå·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b377c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_grpo_dataset import main as generate_main, CONFIG\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print(f\"âš ï¸ Dataset ä¸å­˜åœ¨: {DATASET_PATH}\")\n",
    "    print(\"å¼€å§‹ç”Ÿæˆ dataset...\")\n",
    "\n",
    "    # å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥ä½¿ç”¨å°è§„æ¨¡dataset\n",
    "    # QUICK_VERIFY = True\n",
    "    QUICK_VERIFY = False  # æ­£å¼è®­ç»ƒè®¾ä¸º False\n",
    "\n",
    "    CONFIG.update({\n",
    "        \"output_dir\": DATASET_PATH,\n",
    "        \"state_dir\": \"grpo_states_two_scenarios\",\n",
    "        \"dataset_mode\": \"two_scenarios\",\n",
    "        \"steps_per_tl_signal_step\": 20,   # æ¯TLç”Ÿæˆ10ä¸ªsignal_stepæ ·æœ¬\n",
    "        \"steps_per_tl_extend_decision\": 20,  # æ¯TLç”Ÿæˆ100ä¸ªextend_decisionæ ·æœ¬\n",
    "        \"decision_lead_sec\": 10,\n",
    "        \"phase_duration_scale_range\": (0.7, 1.3),\n",
    "        \"extend_min_green_range\": (10, 45),\n",
    "        \"extend_max_green_range\": (80, 120),\n",
    "        \"extend_wait_time_range\": (2, 5),\n",
    "        \"max_extend_sec\": 8,  # extend_decision ä¸­ extend_sec çš„æœ€å¤§å€¼\n",
    "        \"max_tl_per_scenario\": 20,  \n",
    "        \"num_workers\": 4 if QUICK_VERIFY else 16,\n",
    "    })\n",
    "\n",
    "    print(\"å½“å‰é…ç½®:\")\n",
    "    print(f\"  - æ¨¡å¼: {'å¿«é€ŸéªŒè¯' if QUICK_VERIFY else 'æ­£å¼è®­ç»ƒ'}\")\n",
    "    print(f\"  - warmup_steps: {CONFIG['warmup_steps']}\")\n",
    "    print(f\"  - steps_per_tl_signal_step: {CONFIG['steps_per_tl_signal_step']}\")\n",
    "    print(f\"  - steps_per_tl_extend_decision: {CONFIG['steps_per_tl_extend_decision']}\")\n",
    "    print(f\"  - max_tl_per_scenario: {CONFIG['max_tl_per_scenario']}\")\n",
    "    print(f\"  - num_workers: {CONFIG['num_workers']}\")\n",
    "\n",
    "    generate_main()\n",
    "else:\n",
    "    print(f\"âœ“ Dataset å·²å­˜åœ¨: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502ee00",
   "metadata": {},
   "source": [
    "## 1.8 Generate Synthetic SFT Dataset\n",
    "\n",
    "Based on the GRPO dataset, generate synthetic responses to create an SFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03da1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from grpo_dataset_two_scenarios...\n",
      "Dataset has 4125 samples, limiting to 3000 for SFT...\n",
      "Generating synthetic responses...\n",
      "Generated 3000 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1920fcd2a234933ad6fe9436db957d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic SFT dataset to sft_dataset_synthetic\n",
      "\n",
      "Sample 0:\n",
      "{\n",
      "  \"content\": \"{\\\"extend\\\": \\\"\\\\u662f\\\", \\\"extend_sec\\\": 8}\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def extract_json_content(text, marker):\n",
    "    pattern = f\"ã€{marker}ã€‘(.*?)ã€/{marker}ã€‘\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    return None\n",
    "\n",
    "def generate_synthetic_response(messages):\n",
    "    user_content = messages[-1]['content']\n",
    "    \n",
    "    # Check task type\n",
    "    if \"ã€signal_step_input_jsonã€‘\" in user_content:\n",
    "        data = extract_json_content(user_content, \"signal_step_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for signal_step\n",
    "            scenario = data.get(\"scenario\", {})\n",
    "            phase_ids = scenario.get(\"phase_ids\", [1, 3])\n",
    "            \n",
    "            # Simple heuristic: pick valid phase, valid time\n",
    "            # Ideally pick a random one to prevent bias to one phase\n",
    "            next_phase_id = random.choice(phase_ids)\n",
    "            green_sec = random.randint(10, 60)\n",
    "            \n",
    "            return json.dumps({\n",
    "                \"next_phase_id\": next_phase_id,\n",
    "                \"green_sec\": green_sec\n",
    "            })\n",
    "            \n",
    "    elif \"ã€extend_decision_input_jsonã€‘\" in user_content:\n",
    "        data = extract_json_content(user_content, \"extend_decision_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for extend_decision\n",
    "            # Simple heuristic: 50% yes/no\n",
    "            should_extend = random.choice([\"æ˜¯\", \"å¦\"])\n",
    "            extend_sec = 0\n",
    "            if should_extend == \"æ˜¯\":\n",
    "                extend_sec = random.randint(5, 30)\n",
    "                \n",
    "            return json.dumps({\n",
    "                \"extend\": should_extend,\n",
    "                \"extend_sec\": extend_sec\n",
    "            })\n",
    "            \n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    INPUT_PATH = \"grpo_dataset_two_scenarios\"\n",
    "    OUTPUT_PATH = \"sft_dataset_synthetic\"\n",
    "    \n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        print(f\"Error: {INPUT_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading dataset from {INPUT_PATH}...\")\n",
    "    dataset = load_from_disk(INPUT_PATH)\n",
    "    \n",
    "    # Limit dataset to 3000 samples for SFT (doesn't need too much data)\n",
    "    MAX_SFT_SAMPLES = 3000\n",
    "    if len(dataset) > MAX_SFT_SAMPLES:\n",
    "        print(f\"Dataset has {len(dataset)} samples, limiting to {MAX_SFT_SAMPLES} for SFT...\")\n",
    "        indices = random.sample(range(len(dataset)), MAX_SFT_SAMPLES)\n",
    "        dataset = dataset.select(indices)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    print(\"Generating synthetic responses...\")\n",
    "    for item in dataset:\n",
    "        prompt_messages = item['prompt'] # List of {role, content}\n",
    "        \n",
    "        response_json = generate_synthetic_response(prompt_messages)\n",
    "        \n",
    "        if response_json:\n",
    "            # Create full conversation for SFT\n",
    "            # Clone messages to avoid modifying original reference if any\n",
    "            new_messages = [m.copy() for m in prompt_messages]\n",
    "            new_messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": response_json\n",
    "            })\n",
    "            \n",
    "            new_data.append({\n",
    "                \"messages\": new_messages\n",
    "            })\n",
    "    \n",
    "    if not new_data:\n",
    "        print(\"No valid samples generated!\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Generated {len(new_data)} samples.\")\n",
    "    \n",
    "    # Create new dataset\n",
    "    sft_dataset = Dataset.from_list(new_data)\n",
    "    sft_dataset.save_to_disk(OUTPUT_PATH)\n",
    "    print(f\"Saved synthetic SFT dataset to {OUTPUT_PATH}\")\n",
    "\n",
    "    # Verify one sample\n",
    "    print(\"\\nSample 0:\")\n",
    "    print(json.dumps(sft_dataset[0][\"messages\"][-1], ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab229777",
   "metadata": {},
   "source": [
    "## 1.9 SFT Training\n",
    "\n",
    "Train the model on the synthetic SFT dataset before GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b28ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-22 13:40:04 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Downloading Model from https://www.modelscope.cn to directory: model/models/unsloth/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 13:40:08,614 - modelscope - INFO - Creating symbolic link [model/models/unsloth/Qwen2.5-0.5B-Instruct].\n",
      "2026-01-22 13:40:08,614 - modelscope - WARNING - Failed to create symbolic link model/models/unsloth/Qwen2.5-0.5B-Instruct for /home/davidaray/SCU_TSC/model/models/unsloth/Qwen2___5-0___5B-Instruct.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.5: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.5 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 samples for SFT.\n",
      "Train samples: 2850\n",
      "Eval samples: 150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbedc88d3cc84549bd011b411c1afedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=28):   0%|          | 0/2850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc03a5bf38af44e1babd87878381bed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=28):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT Training with Early Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,850 | Num Epochs = 3 | Total steps = 1,071\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 17,596,416 of 511,629,184 (3.44% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1071' max='1071' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1071/1071 18:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.183300</td>\n",
       "      <td>0.726837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.102907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>0.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.086863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.085870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.082483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.080965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.079779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.078047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.077480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.075959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.076170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.074239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.072697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.072243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.070651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.070411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.069401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.073638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.066475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.064496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.064084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.062184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.061045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.060308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.059176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.058538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.057667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.057136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.056259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.055691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.055544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.055164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.054856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.054311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to checkpoints/sft_tsc_synthetic\n",
      "Done.\n",
      "Memory cleaned up for GRPO stage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# ==================== Config ====================\n",
    "max_seq_length = 1024\n",
    "lora_rank = 32\n",
    "# model_name = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "output_dir = \"checkpoints/sft_tsc_synthetic\"\n",
    "dataset_path = \"sft_dataset_synthetic\"\n",
    "\n",
    "# ==================== Load Model ====================\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=False,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.7,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# ==================== Load Dataset ====================\n",
    "if not os.path.isdir(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}. Run generate_synthetic_sft_dataset.py first.\")\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(f\"Loaded {len(dataset)} samples for SFT.\")\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# Limit eval dataset to 200 samples for speed\n",
    "if len(eval_dataset) > 500:\n",
    "    eval_dataset = eval_dataset.select(range(500))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# ==================== Configure Trainer ====================\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\", \n",
    ")\n",
    "\n",
    "# Formatting function for chat \n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    \n",
    "    # Logic to handle both batched and non-batched inputs\n",
    "    if isinstance(convos, list) and len(convos) > 0 and isinstance(convos[0], dict):\n",
    "        # Single conversation (list of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convos, tokenize=False, add_generation_prompt=False)]\n",
    "    else:\n",
    "        # Batch of conversations (list of lists of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "        \n",
    "    return texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False, \n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3, # Increase epochs relying on early stopping\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        fp16_full_eval=not torch.cuda.is_bf16_supported(),\n",
    "        bf16_full_eval=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=30, # Evaluate every 30 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=30,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 eval steps (90 steps)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting SFT Training with Early Stopping...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# Clean up memory for the next stage\n",
    "import gc\n",
    "try:\n",
    "    del model, tokenizer, trainer\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleaned up for GRPO stage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd3a3",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a017314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-22 14:58:54 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "âœ“ ä» SFT æ¨¡å‹å¼€å§‹è®­ç»ƒ: checkpoints/sft_tsc_synthetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM\n",
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM to enable standby.\n",
      "INFO:unsloth_zoo.log: Unsloth: Enabling vLLM standby mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 14:58:57 [vllm_utils.py:693] Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'\n",
      "==((====))==  Unsloth 2025.10.5: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Standby mode is enabled. Increasing `gpu_memory_utilization` to 0.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading model/models/unsloth/Qwen2___5-0___5B-Instruct with actual GPU utilization = 88.81%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.56 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
      "Unsloth: vLLM's KV Cache can use up to 19.97 GB. Also swap space = 6 GB.\n",
      "WARNING 01-22 14:59:02 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-22 14:59:02 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8881223979045927, 'max_num_batched_tokens': 2048, 'max_num_seqs': 288, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'enable_sleep_mode': True, 'model': 'model/models/unsloth/Qwen2___5-0___5B-Instruct'}\n",
      "INFO 01-22 14:59:02 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 14:59:02 [model.py:1510] Using max model len 2048\n",
      "INFO 01-22 14:59:03 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 01-22 14:59:03 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-22 14:59:03 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='model/models/unsloth/Qwen2___5-0___5B-Instruct', speculative_config=None, tokenizer='model/models/unsloth/Qwen2___5-0___5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=model/models/unsloth/Qwen2___5-0___5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 01-22 14:59:03 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-22 14:59:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-22 14:59:03 [gpu_model_runner.py:2602] Starting to load model model/models/unsloth/Qwen2___5-0___5B-Instruct...\n",
      "INFO 01-22 14:59:03 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 01-22 14:59:03 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b616b1e790f84b89b1cc2a5f4be8ff5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 14:59:04 [default_loader.py:267] Loading weights took 0.45 seconds\n",
      "INFO 01-22 14:59:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-22 14:59:04 [gpu_model_runner.py:2653] Model loading took 0.9606 GiB and 0.540062 seconds\n",
      "INFO 01-22 14:59:08 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/63cc1326d3/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-22 14:59:08 [backends.py:559] Dynamo bytecode transform time: 3.18 s\n",
      "INFO 01-22 14:59:09 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.948 s\n",
      "INFO 01-22 14:59:10 [monitor.py:34] torch.compile takes 3.18 s in total\n",
      "INFO 01-22 14:59:10 [gpu_worker.py:298] Available KV cache memory: 18.38 GiB\n",
      "INFO 01-22 14:59:10 [kv_cache_utils.py:1087] GPU KV cache size: 1,606,240 tokens\n",
      "INFO 01-22 14:59:10 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 784.30x\n",
      "INFO 01-22 14:59:10 [vllm_utils.py:698] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 01-22 14:59:10 [gpu_model_runner.py:3643] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 28.12it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:01<00:00, 28.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 14:59:14 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.71 GiB\n",
      "INFO 01-22 14:59:14 [vllm_utils.py:705] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 14:59:15 [core.py:210] init engine (profile, create kv cache, warmup model) took 10.45 seconds\n",
      "INFO 01-22 14:59:15 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at model/models/unsloth/Qwen2___5-0___5B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'attention_norm', 'norm2', 'q_norm', 'input_layernorm', 'k_norm', 'layer_norm2', 'ffn_norm', 'post_layernorm', 'post_attention_layernorm', 'layer_norm1', 'norm1', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'cross_attn_input_layernorm', 'attention_norm', 'norm2', 'q_norm', 'input_layernorm', 'k_norm', 'layer_norm2', 'ffn_norm', 'post_layernorm', 'cross_attn_post_attention_layernorm', 'post_attention_layernorm', 'layer_norm1', 'norm1', 'post_feedforward_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.5 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied qwen-2.5 chat template to match SFT training\n",
      "è®¾ç½® bos_token_id: 151644\n",
      "Tokenizer padding_side: left\n",
      "Tokenizer pad_token_id: 151654\n",
      "Tokenizer bos_token_id: 151644\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "\n",
    "# BASE_MODEL_DIR = \"model/models/qwen3-4B-SFT\"\n",
    "BASE_MODEL_DIR = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# BASE_MODEL_DIR = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "CHECKPOINT_DIR = \"checkpoints/grpo_tsc_two_scenarios_latest\"\n",
    "\n",
    "\n",
    "def _looks_like_checkpoint(path: str) -> bool:\n",
    "    if not os.path.isdir(path):\n",
    "        return False\n",
    "    marker_files = [\n",
    "        \"adapter_config.json\",\n",
    "        \"adapter_model.safetensors\",\n",
    "        \"adapter_model.bin\",\n",
    "        \"config.json\",\n",
    "    ]\n",
    "    return any(os.path.isfile(os.path.join(path, f)) for f in marker_files)\n",
    "\n",
    "\n",
    "SFT_CHECKPOINT_DIR = \"checkpoints/sft_tsc_synthetic\"\n",
    "GRPO_CHECKPOINT_DIR = \"checkpoints/grpo_tsc_two_scenarios\"\n",
    "\n",
    "def _find_latest_checkpoint(checkpoint_dir: str) -> str | None:\n",
    "    \"\"\"Find the latest valid checkpoint in a directory.\"\"\"\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        return None\n",
    "    candidates = []\n",
    "    for name in os.listdir(checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, name)\n",
    "        if _looks_like_checkpoint(path):\n",
    "            # Extract step number if possible (e.g., checkpoint-100)\n",
    "            try:\n",
    "                step = int(name.split('-')[-1])\n",
    "            except (ValueError, IndexError):\n",
    "                step = 0\n",
    "            candidates.append((step, path))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # Return the checkpoint with the highest step number\n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "resume_from = CHECKPOINT_DIR if _looks_like_checkpoint(CHECKPOINT_DIR) else BASE_MODEL_DIR\n",
    "\n",
    "# Priority: CHECKPOINT_DIR (latest symlink) > GRPO checkpoints > SFT checkpoint > base model\n",
    "if _looks_like_checkpoint(CHECKPOINT_DIR):\n",
    "    print(f\"âœ“ ä» checkpoint ç»§ç»­è®­ç»ƒ: {CHECKPOINT_DIR}\")\n",
    "    resume_from = CHECKPOINT_DIR\n",
    "else:\n",
    "    # Check for checkpoints in grpo_tsc_two_scenarios\n",
    "    grpo_latest = _find_latest_checkpoint(GRPO_CHECKPOINT_DIR)\n",
    "    if grpo_latest:\n",
    "        print(f\"âœ“ ä» GRPO checkpoint ç»§ç»­è®­ç»ƒ: {grpo_latest}\")\n",
    "        resume_from = grpo_latest\n",
    "    elif _looks_like_checkpoint(SFT_CHECKPOINT_DIR):\n",
    "        print(f\"âœ“ ä» SFT æ¨¡å‹å¼€å§‹è®­ç»ƒ: {SFT_CHECKPOINT_DIR}\")\n",
    "        resume_from = SFT_CHECKPOINT_DIR\n",
    "    else:\n",
    "        print(f\"â„¹ ä»åŸºç¡€æ¨¡å‹å¼€å§‹: {BASE_MODEL_DIR}\")\n",
    "        resume_from = BASE_MODEL_DIR\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=resume_from,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "if resume_from == BASE_MODEL_DIR:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=lora_rank * 2,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    _trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(_trainable) == 0:\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"lora\" in name.lower():\n",
    "                p.requires_grad = True\n",
    "        print(\"âš ï¸ checkpoint æœªæ£€æµ‹åˆ°å¯è®­ç»ƒå‚æ•°ï¼Œå·²å¼ºåˆ¶å¯ç”¨ LoRA å‚æ•°è®­ç»ƒ\")\n",
    "\n",
    "# Fix for GRPO generation: Must use left padding (applies to ALL cases)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Apply the same chat template as SFT training\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",  # Must match SFT training template\n",
    ")\n",
    "print(\"âœ“ Applied qwen-2.5 chat template to match SFT training\")\n",
    "\n",
    "# Ensure BOS token is preserved - this is critical for generation\n",
    "if tokenizer.bos_token_id is None:\n",
    "    # For Qwen, <|im_start|> acts as BOS in chat context\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "    print(f\"è®¾ç½® bos_token_id: {tokenizer.bos_token_id}\")\n",
    "\n",
    "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")\n",
    "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"Tokenizer bos_token_id: {tokenizer.bos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0673f7d",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43de6b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset åŠ è½½æˆåŠŸ: grpo_dataset_two_scenarios\n",
      "æ ·æœ¬æ•°: 4125\n",
      "âš ï¸ Stratified split failed (Stratifying by column is only supported for ClassLabel column, and column task_type is Value.), using random split\n",
      "âœ“ Train/Eval split: train=4120, eval=5\n",
      "  - eval task_type counts: Counter({'signal_step': 3, 'extend_decision': 2})\n",
      "ç¤ºä¾‹ Prompt (message list):\n",
      "[{'content': 'You are a traffic signal control expert. Output only valid JSON without any explanation.', 'role': 'system'}, {'content': 'ä½ æ˜¯äº¤é€šä¿¡å·æ§åˆ¶ä¼˜åŒ–ä¸“å®¶ã€‚ã€signal_step_input_jsonã€‘{\"crossing_id\":2030588924,\"as_of\":\"2026-01-22 10:53:44\",\"scenario\":{\"phase_ids\":[1,3,5,6,7,9],\"phase_lane_map\":{\"1\":[\"nt12_nt16_0\",\"np9_nt16_0\"],\"3\":[\"nt15_nt16_1\",\"np8_nt16_1\"],\"5\":[\"np8_nt16_0\",\"nt15_nt16_0\"],\"6\":[\"np8_nt16_0\"],\"7\":[\"np8_nt16_0\",\"np8_nt16_1\"],\"9\":[\"nt15_nt16_0\",\"nt15_nt16_1\"]}},\"state\":{\"current_phase_id\":1,\"current_phase_elapsed_sec\":0,\"current_phase_planned_green_sec\":10,\"phase_metrics_now\":[{\"phase_id\":1,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0},{\"phase_id\":3,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0},{\"phase_id\":5,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0},{\"phase_id\":6,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0},{\"phase_id\":7,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0},{\"phase_id\":9,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0}]}}ã€/signal_step_input_jsonã€‘\\n\\nå­—æ®µå«ä¹‰ï¼ˆä»…è¯´æ˜å«ä¹‰ï¼‰ï¼š\\n* state.phase_metrics_now[*].avg_queue_vehï¼šè¯¥ç›¸ä½åœ¨å½“å‰æ—¶åˆ»çš„å¹³å‡æ’é˜Ÿè½¦è¾†æ•°ï¼ˆè¾†ï¼‰ï¼›å¯ç”±è¯¥ç›¸ä½æ‰€æ§åˆ¶è½¦é“çš„æ’é˜Ÿè½¦è¾†æ•°å–å¹³å‡å¾—åˆ°ã€‚\\n* state.phase_metrics_now[*].avg_passed_veh_in_current_greenï¼šè¯¥ç›¸ä½åœ¨\"å½“å‰æ­£åœ¨æ‰§è¡Œçš„ç»¿ç¯ç›¸ä½\"å†…è‡³å½“å‰æ—¶åˆ»ç´¯è®¡é€šè¿‡è½¦è¾†æ•°ï¼ˆè¾†ï¼‰ï¼›åªæœ‰å½“å‰ç›¸ä½é€šå¸¸ä¼š >0ï¼Œéå½“å‰ç›¸ä½ä¸€èˆ¬ä¸º 0ã€‚\\n\\nä»»åŠ¡ï¼ˆå¿…é¡»å®Œæˆï¼‰ï¼š\\n1. åŸºäºè¾“å…¥ JSON çš„ scenario ä¸ stateï¼Œè‡ªè¡Œå†³å®šå†³ç­–ç­–ç•¥/å‚æ•°ï¼Œè¯„ä¼°å„ç›¸ä½å½“å‰éœ€æ±‚å¼ºåº¦å¹¶åšå‡º\"ä¸‹ä¸€æ­¥åŠ¨ä½œ\"ï¼ˆä»…åœ¨å†…éƒ¨ä½¿ç”¨ï¼Œä¸è¾“å‡ºä»»ä½•é¢„æµ‹è¿‡ç¨‹/ä¸­é—´å€¼ï¼‰ã€‚\\n2. è¾“å‡ºï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ next_phase_idï¼Œä»¥åŠè¯¥ç›¸ä½ç»¿ç¯æŒç»­æ—¶é—´ green_secï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚\\n\\nè¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š\\n1. ä½ å¿…é¡»æ˜¾å¼åˆ©ç”¨ avg_queue_veh ä¸ passed_veh_in_current_green æ¥å†³ç­–ï¼›ä¸å¾—æ— è§†è¾“å…¥éšæ„ç»™å‡ºç­”æ¡ˆã€‚\\n2. åœ¨ç¼ºå°‘ä»»ä½•é¢å¤–ç¡¬çº¦æŸï¼ˆä¸æä¾› cycle_constraints / phase_order / phase_limits / historyï¼‰çš„æƒ…å†µä¸‹ï¼š\\n   * next_phase_id å¿…é¡»æ¥è‡ª scenario.phase_idsï¼›\\n   * green_sec å¿…é¡»ä¸ºæ­£æ•´æ•°ç§’ï¼Œä¸”å¿…é¡»åœ¨ [1, 120] èŒƒå›´å†…ï¼›\\n   * green_sec å¿…é¡»\"åˆç†\"ï¼šé˜Ÿåˆ—æ›´å¤§/é€šè¿‡æ›´å°‘çš„ç›¸ä½å€¾å‘ç»™æ›´é•¿ç»¿ï¼›é˜Ÿåˆ—æ›´å°/é€šè¿‡æ›´å¤šçš„ç›¸ä½å€¾å‘ç»™æ›´çŸ­ç»¿ï¼›\\n   * è‹¥å¤šç›¸ä½éœ€æ±‚æ¥è¿‘ï¼Œå¯ä¼˜å…ˆåˆ‡æ¢åˆ°éå½“å‰ç›¸ä½ä»¥é™ä½å…¶ä»–ç›¸ä½ç­‰å¾…çš„ç´¯ç§¯é£é™©ã€‚\\n\\nè¾“å‡ºè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\\n1. åªè¾“å‡ºæœ€ç»ˆ JSONï¼ˆä¸è¦ä»»ä½•è¯´æ˜ã€ä¸è¦ Markdown ä»£ç å—ã€ä¸è¦é¢å¤–æ–‡æœ¬ã€ä¸è¦å¤è¿°è§„åˆ™ã€ä¸è¦è¾“å‡ºæ¨ç†è¿‡ç¨‹ï¼‰ã€‚\\n2. JSON å¿…é¡»ä¸ºå¯¹è±¡ï¼Œä¸”ä»…åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š\\n   {\"next_phase_id\": <int>, \"green_sec\": <int>}\\n3. green_sec å¿…é¡»åœ¨ [1, 120] èŒƒå›´å†…ã€‚\\n4. ä¸å…è®¸è¾“å‡ºå…¶å®ƒå­—æ®µï¼Œä¸å…è®¸æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡æœ¬ã€‚', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets import Dataset\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "EVAL_TEST_SIZE = 0.001\n",
    "EVAL_MAX_SAMPLES = 64\n",
    "EVAL_SEED = 42\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset ä¸å­˜åœ¨: {DATASET_PATH}\\n\"\n",
    "        \"è¯·å…ˆè¿è¡Œ generate_grpo_dataset.py ç”Ÿæˆç¦»çº¿ dataset\"\n",
    "    )\n",
    "\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "print(f\"âœ“ Dataset åŠ è½½æˆåŠŸ: {DATASET_PATH}\")\n",
    "print(f\"æ ·æœ¬æ•°: {len(dataset)}\")\n",
    "\n",
    "# Split out a small eval set to track real progress.\n",
    "# Prefer stratified split by task_type so both tasks appear in eval.\n",
    "try:\n",
    "    if isinstance(dataset, Dataset) and (\"task_type\" in dataset.column_names):\n",
    "        try:\n",
    "            # Try stratified split first\n",
    "            split = dataset.train_test_split(\n",
    "                test_size=EVAL_TEST_SIZE,\n",
    "                seed=EVAL_SEED,\n",
    "                stratify_by_column=\"task_type\",\n",
    "            )\n",
    "        except Exception as strat_err:\n",
    "            # Fallback to non-stratified split if stratification fails\n",
    "            print(f\"âš ï¸ Stratified split failed ({strat_err}), using random split\")\n",
    "            split = dataset.train_test_split(test_size=EVAL_TEST_SIZE, seed=EVAL_SEED)\n",
    "    else:\n",
    "        split = dataset.train_test_split(test_size=EVAL_TEST_SIZE, seed=EVAL_SEED)\n",
    "\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "    if len(eval_dataset) > EVAL_MAX_SAMPLES:\n",
    "        eval_dataset = eval_dataset.select(range(EVAL_MAX_SAMPLES))\n",
    "\n",
    "    print(f\"âœ“ Train/Eval split: train={len(train_dataset)}, eval={len(eval_dataset)}\")\n",
    "    if \"task_type\" in train_dataset.column_names:\n",
    "        from collections import Counter\n",
    "        print(\"  - eval task_type counts:\", Counter(eval_dataset[\"task_type\"]))\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Train/Eval split failed completely: {e}\")\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = None\n",
    "\n",
    "# NOTE: Do NOT apply chat template here!\n",
    "# GRPOTrainer expects prompt to be a list of messages, not a formatted string.\n",
    "# It will apply the chat template internally using processing_class (tokenizer).\n",
    "print(\"ç¤ºä¾‹ Prompt (message list):\")\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ada76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[60][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c127c",
   "metadata": {},
   "source": [
    "## 4. å¯¼å…¥ Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08728fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Reward function åŠ è½½æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "from tsc_reward_function import (\n",
    "    tsc_reward_sim_fn,\n",
    "    tsc_reward_format_fn,\n",
    "    tsc_reward_constraint_fn,  # æ–°å¢\n",
    "    cleanup_global_pool,\n",
    "    reward_diag_snapshot,\n",
    "    reward_diag_last,   \n",
    ")\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class RewardDiagnosticsCallback(TrainerCallback):\n",
    "    def __init__(self, kl_spike_threshold: float = 5.0):\n",
    "        self.kl_spike_threshold = float(kl_spike_threshold)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        if not getattr(state, \"is_world_process_zero\", True):\n",
    "            return\n",
    "\n",
    "        # Periodic window summary (aligned to logging)\n",
    "        if getattr(args, \"logging_steps\", None) and state.global_step % int(args.logging_steps) == 0:\n",
    "            snap = reward_diag_snapshot(reset=True)\n",
    "            total = snap.get(\"window_total\", 0)\n",
    "            invalid = snap.get(\"window_invalid\", 0)\n",
    "            rate = (invalid / total) if total else 0.0\n",
    "            print(\n",
    "                f\"[reward_diag] steps {snap.get('window_start_step')}..{state.global_step} \"\n",
    "                f\"invalid_rate={rate:.3f} ({invalid}/{total})\"\n",
    "            )\n",
    "            by_task_total = snap.get(\"window_total_by_task\", {}) or {}\n",
    "            by_task_invalid = snap.get(\"window_invalid_by_task\", {}) or {}\n",
    "            by_reason = snap.get(\"window_reason_by_task\", {}) or {}\n",
    "            for task, t_total in sorted(by_task_total.items()):\n",
    "                t_invalid = int(by_task_invalid.get(task, 0))\n",
    "                t_rate = (t_invalid / t_total) if t_total else 0.0\n",
    "                reasons = by_reason.get(task, {}) or {}\n",
    "                reasons = {k: v for k, v in reasons.items() if k != \"ok\"}\n",
    "                top = sorted(reasons.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
    "                top_str = \", \".join([f\"{k}:{v}\" for k, v in top]) if top else \"n/a\"\n",
    "                print(\n",
    "                    f\"[reward_diag]  - {task}: invalid_rate={t_rate:.3f} \"\n",
    "                    f\"({t_invalid}/{t_total}) top={top_str}\"\n",
    "                )\n",
    "\n",
    "        # KL spike dump\n",
    "        kl = logs.get(\"kl\", None)\n",
    "        try:\n",
    "            if kl is not None and float(kl) >= self.kl_spike_threshold:\n",
    "                batch = reward_diag_last(state.global_step) or {}\n",
    "                print(\n",
    "                    f\"[reward_diag] KL spike at step={state.global_step} kl={float(kl):.4f} batch={batch}\"\n",
    "                )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "diag_callback = RewardDiagnosticsCallback(kl_spike_threshold=5.0)\n",
    "\n",
    "print(\"âœ“ Reward function åŠ è½½æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f371e",
   "metadata": {},
   "source": [
    "## 5. é…ç½® GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe254525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥ä½¿ç”¨çŸ­è®­ç»ƒ\n",
    "# QUICK_VERIFY = True\n",
    "QUICK_VERIFY = False  # æ­£å¼è®­ç»ƒè®¾ä¸º False\n",
    "\n",
    "# Eval é…ç½®ï¼ševal_dataset ä¸ºç©ºåˆ™è‡ªåŠ¨ç¦ç”¨\n",
    "DO_EVAL = False\n",
    "EVAL_STEPS = 20\n",
    "EVAL_BATCH_SIZE = 4  # å¿…é¡»èƒ½æ•´é™¤ num_generations\n",
    "\n",
    "config = GRPOConfig(\n",
    "    output_dir=\"checkpoints/grpo_tsc_two_scenarios\",\n",
    "\n",
    "    # æ‰¹æ¬¡é…ç½®\n",
    "    per_device_train_batch_size=2,\n",
    "    num_generations=4,  # Keep at 4 for proper GRPO\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # ç”Ÿæˆé…ç½®\n",
    "    max_completion_length=128,  \n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    use_vllm=False,\n",
    "\n",
    "    # è®­ç»ƒé…ç½®\n",
    "    learning_rate=2e-6,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=-1,\n",
    "\n",
    "    # GRPO ç‰¹å®š\n",
    "    scale_rewards=True,\n",
    "\n",
    "    # Eval (track real progress on held-out states)\n",
    "    do_eval=DO_EVAL,\n",
    "    eval_strategy=\"steps\" if DO_EVAL else \"no\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_on_start=DO_EVAL,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "\n",
    "    # æ—¥å¿—ä¸ä¿å­˜\n",
    "    logging_steps=5,\n",
    "    save_steps=100,\n",
    "    # save_total_limit=1,\n",
    "\n",
    "    # ä¼˜åŒ–å™¨\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.01,             # KL ç³»æ•°ï¼ˆ0.0 é»˜è®¤ä¸åŠ è½½ refï¼‰:contentReference[oaicite:1]{index=1}\n",
    "    max_grad_norm=0.5,    # æ¢¯åº¦è£å‰ªï¼ˆé˜² KL spikeï¼‰\n",
    "\n",
    "    # å…¶ä»–\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ GRPOConfig é…ç½®å®Œæˆ (æ¨¡å¼: {'å¿«é€ŸéªŒè¯' if QUICK_VERIFY else 'æ­£å¼è®­ç»ƒ'})\")\n",
    "if QUICK_VERIFY:\n",
    "    print(f\"  - max_steps: {config.max_steps} (éªŒè¯æ¨¡å¼)\")\n",
    "else:\n",
    "    print(f\"  - max_steps: {config.max_steps} (å…¨epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5b7e",
   "metadata": {},
   "source": [
    "## 6. åˆ›å»º GRPOTrainer å¹¶å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists and is writable\n",
    "import os\n",
    "import stat\n",
    "\n",
    "output_dir = config.output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Verify we can write to the directory\n",
    "test_file = os.path.join(output_dir, '.write_test')\n",
    "try:\n",
    "    with open(test_file, 'w') as f:\n",
    "        f.write('test')\n",
    "    os.remove(test_file)\n",
    "    print(f\"âœ“ Output directory {output_dir} is writable\")\n",
    "except PermissionError:\n",
    "    print(f\"âš ï¸ Cannot write to {output_dir}, attempting to fix permissions...\")\n",
    "    # Try to change permissions if we own the directory\n",
    "    try:\n",
    "        os.chmod(output_dir, stat.S_IRWXU | stat.S_IRWXG | stat.S_IROTH | stat.S_IXOTH)\n",
    "        print(f\"âœ“ Permissions fixed for {output_dir}\")\n",
    "    except Exception as e:\n",
    "        raise PermissionError(\n",
    "            f\"Cannot write to checkpoint directory: {output_dir}\\n\"\n",
    "            f\"Please run: sudo chown -R $(whoami) {output_dir}\"\n",
    "        ) from e\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    reward_funcs=[tsc_reward_sim_fn, tsc_reward_format_fn, tsc_reward_constraint_fn],\n",
    "    callbacks=[diag_callback],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ“ GRPOTrainer åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}\")\n",
    "print(f\"è¯„ä¼°æ ·æœ¬æ•°: {0 if eval_dataset is None else len(eval_dataset)}\")\n",
    "print(f\"æ¯ epoch steps: {len(trainer.get_train_dataloader())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"å¼€å§‹ GRPO è®­ç»ƒ\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"è®­ç»ƒå®Œæˆ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222355eb",
   "metadata": {},
   "source": [
    "## 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad711919",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = \"checkpoints/grpo_tsc_two_scenarios_final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c1138",
   "metadata": {},
   "source": [
    "## 8. æ¸…ç†èµ„æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_global_pool()\n",
    "print(\"âœ“ Simulator æ± å·²æ¸…ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7aa60b",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•æ¨ç†ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4886c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_sample = dataset[0]\n",
    "test_prompt = test_sample[\"prompt\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"æµ‹è¯•ç”Ÿæˆ:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
