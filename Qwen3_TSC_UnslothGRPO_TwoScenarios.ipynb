{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4885fa",
   "metadata": {},
   "source": [
    "# TSC æ ‡å‡† Unsloth GRPO è®­ç»ƒï¼ˆä¸¤å¤§åœºæ™¯ï¼‰\n",
    "\n",
    "ä½¿ç”¨æ ‡å‡† Unsloth GRPOTrainer + ç¦»çº¿ Dataset + Reward Function å›æº¯ SUMO è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149eb79c",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40758b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯å¢ƒå˜é‡å·²è®¾ç½®\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "print(\"ç¯å¢ƒå˜é‡å·²è®¾ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b9c5",
   "metadata": {},
   "source": [
    "## 1.5 ç”Ÿæˆ/æ£€æŸ¥ Datasetï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœ dataset ä¸å­˜åœ¨ï¼Œæ­¤ cell ä¼šè‡ªåŠ¨ç”Ÿæˆï¼›å¦‚æœå·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b377c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Dataset ä¸å­˜åœ¨: grpo_dataset_two_scenarios\n",
      "å¼€å§‹ç”Ÿæˆ dataset...\n",
      "å½“å‰é…ç½®:\n",
      "  - æ¨¡å¼: æ­£å¼è®­ç»ƒ\n",
      "  - warmup_steps: 80\n",
      "  - steps_per_tl_signal_step: 100\n",
      "  - steps_per_tl_extend_decision: 100\n",
      "  - max_tl_per_scenario: 10\n",
      "  - num_workers: 16\n",
      "å‘ç°åœºæ™¯æ•°: 1\n",
      "æ€»è®­ç»ƒç»„åˆæ•°: 10\n",
      "  ä¼˜å…ˆåœºæ™¯: 0\n",
      "  å…¶ä»–åœºæ™¯: 10\n",
      "  å¹¶è¡Œ workers: 16\n",
      "\n",
      "å¼€å§‹å¹¶è¡Œç”Ÿæˆï¼ˆ16 workersï¼‰...\n",
      "DEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumoDEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 18143 (Attempt 1/10)\n",
      "\n",
      "Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 53754 (Attempt 1/10)\n",
      "Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 57235 (Attempt 1/10)Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 59123 (Attempt 1/10)Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 44159 (Attempt 1/10)Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 22638 (Attempt 1/10)Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 19526 (Attempt 1/10)\n",
      "Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 39725 (Attempt 1/10)Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 31028 (Attempt 1/10)Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0 on port 47731 (Attempt 1/10)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Retrying in 1 seconds Retrying in 1 seconds Retrying in 1 seconds Retrying in 1 seconds\n",
      " Retrying in 1 seconds Retrying in 1 seconds Retrying in 1 seconds\n",
      "\n",
      " Retrying in 1 seconds\n",
      "\n",
      "\n",
      "\n",
      " Retrying in 1 seconds\n",
      "\n",
      " Retrying in 1 seconds\n",
      "Successfully connected to SUMO on port 19526Successfully connected to SUMO on port 31028Successfully connected to SUMO on port 22638\n",
      "\n",
      "\n",
      "Successfully connected to SUMOSuccessfully connected to SUMOSuccessfully connected to SUMO\n",
      "\n",
      "\n",
      "Starting warmup phase...Successfully connected to SUMO on port 44159Starting warmup phase...Starting warmup phase...\n",
      "\n",
      "\n",
      "\n",
      "Successfully connected to SUMOWarmup progress: 0/300Warmup progress: 0/300Warmup progress: 0/300\n",
      "\n",
      "\n",
      "\n",
      "Starting warmup phase...Successfully connected to SUMO on port 57235\n",
      "\n",
      "Warmup progress: 0/300Successfully connected to SUMO\n",
      "\n",
      "Starting warmup phase...Successfully connected to SUMO on port 39725\n",
      "\n",
      "Warmup progress: 0/300Successfully connected to SUMO\n",
      "\n",
      "Starting warmup phase...\n",
      "Warmup progress: 0/300\n",
      "Successfully connected to SUMO on port 59123\n",
      "Successfully connected to SUMO\n",
      "Starting warmup phase...\n",
      "Warmup progress: 0/300\n",
      "Successfully connected to SUMO on port 18143\n",
      "Successfully connected to SUMO\n",
      "Starting warmup phase...\n",
      "Warmup progress: 0/300\n",
      "Successfully connected to SUMO on port 47731Successfully connected to SUMO on port 53754\n",
      "\n",
      "Successfully connected to SUMOSuccessfully connected to SUMO\n",
      "\n",
      "Starting warmup phase...Starting warmup phase...\n",
      "\n",
      "Warmup progress: 0/300Warmup progress: 0/300\n",
      "\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300Warmup progress: 200/300\n",
      "\n",
      "Warmup progress: 200/300\n",
      "Warmup progress: 200/300\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Step #2908.00 (17ms ~= 58.82*RT, ~108411.76UPS, TraCI: 0ms, vehicles TOT 8298 ACT 1843 BUF\n",
      "[1/10] âœ“ chengdu/1492574990: 166 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 166 ä¸ª\n",
      "Step #2936.00 (13ms ~= 76.92*RT, ~141076.92UPS, TraCI: 0ms, vehicles TOT 8361 ACT 1834 BUF\n",
      "[2/10] âœ“ chengdu/1916386562: 156 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 322 ä¸ª\n",
      "Step #3206.00 (9ms ~= 111.11*RT, ~185111.11UPS, TraCI: 0ms, vehicles TOT 8923 ACT 1666 BUF\n",
      "[3/10] âœ“ chengdu/1159176756: 169 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 491 ä¸ª\n",
      "Step #3554.00 (12ms ~= 83.33*RT, ~145166.67UPS, TraCI: 1ms, vehicles TOT 9610 ACT 1742 BUF\n",
      "[4/10] âœ“ chengdu/1492574988: 197 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 688 ä¸ª\n",
      "Step #3649.00 (10ms ~= 100.00*RT, ~146100.00UPS, TraCI: 0ms, vehicles TOT 9721 ACT 1461 BU\n",
      "[5/10] âœ“ chengdu/2852225599: 196 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 884 ä¸ª\n",
      "Step #3709.00 (7ms ~= 142.86*RT, ~224714.29UPS, TraCI: 1ms, vehicles TOT 9708 ACT 1573 BUF\n",
      "[6/10] âœ“ chengdu/314622964: 199 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 1083 ä¸ª\n",
      "Step #4645.00 (3ms ~= 333.33*RT, ~254000.00UPS, TraCI: 0ms, vehicles TOT 9735 ACT 762 BUF  tep #0.00 (0ms ?*RT. ?UPS, TraCI: 3ms, vehicles TOT 0 ACT 0 BUF 0)                       Step #0.00 (0ms ?*RT. ?UPS, TraCI: 3ms, vehicles TOT 0 ACT 0 BUF 0)                       tep #4500.00 (3ms ~= 333.33*RT, ~271333.33UPS, TraCI: 0ms, vehicles TOT 9731 ACT 814 BUF \n",
      "[7/10] âœ“ chengdu/1916386518: 156 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 1239 ä¸ª\n",
      "Step #5833.00 (1ms ~= 1000.00*RT, ~485000.00UPS, TraCI: 0ms, vehicles TOT 9767 ACT 485 BUF\n",
      "[8/10] âœ“ chengdu/1388442962: 151 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 1390 ä¸ª\n",
      "Step #6942.00 (1ms ~= 1000.00*RT, ~421000.00UPS, TraCI: 0ms, vehicles TOT 9784 ACT 421 BUF\n",
      "[9/10] âœ“ chengdu/276556799: 200 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 1590 ä¸ª\n",
      "Step #12977.00 (0ms ?*RT. ?UPS, TraCI: 1ms, vehicles TOT 9866 ACT 56 BUF 32)              ep #9000.00 (1ms ~= 1000.00*RT, ~320000.00UPS, TraCI: 0ms, vehicles TOT 9827 ACT 320 BUF\n",
      "[10/10] âœ“ chengdu/1492645720: 119 ä¸ªæ ·æœ¬ï¼Œç´¯è®¡ 1709 ä¸ª\n",
      "\n",
      "ç”Ÿæˆ Datasetï¼Œæ€»æ ·æœ¬æ•°: 1709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4849e59d03ae41729d8e9f4375de4e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset å·²ä¿å­˜åˆ°: grpo_dataset_two_scenarios\n",
      "\n",
      "=== Dataset ç»Ÿè®¡ ===\n",
      "æ€»æ ·æœ¬æ•°: 1709\n",
      "åœºæ™¯æ•°: 1\n",
      "ä¿¡å·ç¯æ•°: 10\n"
     ]
    }
   ],
   "source": [
    "from generate_grpo_dataset import main as generate_main, CONFIG\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print(f\"âš ï¸ Dataset ä¸å­˜åœ¨: {DATASET_PATH}\")\n",
    "    print(\"å¼€å§‹ç”Ÿæˆ dataset...\")\n",
    "\n",
    "    # å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥ä½¿ç”¨å°è§„æ¨¡dataset\n",
    "    # QUICK_VERIFY = True\n",
    "    QUICK_VERIFY = False  # æ­£å¼è®­ç»ƒè®¾ä¸º False\n",
    "\n",
    "    CONFIG.update({\n",
    "        \"output_dir\": DATASET_PATH,\n",
    "        \"state_dir\": \"grpo_states_two_scenarios\",\n",
    "        \"dataset_mode\": \"two_scenarios\",\n",
    "        \"steps_per_tl_signal_step\": 100,   # æ¯TLç”Ÿæˆ100ä¸ªsignal_stepæ ·æœ¬\n",
    "        \"steps_per_tl_extend_decision\": 100,  # æ¯TLç”Ÿæˆ100ä¸ªextend_decisionæ ·æœ¬\n",
    "        \"decision_lead_sec\": 10,\n",
    "        \"phase_duration_scale_range\": (0.7, 1.3),\n",
    "        \"extend_min_green_range\": (5, 20),\n",
    "        \"extend_max_green_range\": (25, 120),\n",
    "        \"extend_wait_time_range\": (5, 25),\n",
    "        \"max_tl_per_scenario\": 10,  # æ¯åœºæ™¯æœ€å¤š10ä¸ªTL\n",
    "        \"num_workers\": 4 if QUICK_VERIFY else 16,\n",
    "    })\n",
    "\n",
    "    print(\"å½“å‰é…ç½®:\")\n",
    "    print(f\"  - æ¨¡å¼: {'å¿«é€ŸéªŒè¯' if QUICK_VERIFY else 'æ­£å¼è®­ç»ƒ'}\")\n",
    "    print(f\"  - warmup_steps: {CONFIG['warmup_steps']}\")\n",
    "    print(f\"  - steps_per_tl_signal_step: {CONFIG['steps_per_tl_signal_step']}\")\n",
    "    print(f\"  - steps_per_tl_extend_decision: {CONFIG['steps_per_tl_extend_decision']}\")\n",
    "    print(f\"  - max_tl_per_scenario: {CONFIG['max_tl_per_scenario']}\")\n",
    "    print(f\"  - num_workers: {CONFIG['num_workers']}\")\n",
    "\n",
    "    generate_main()\n",
    "else:\n",
    "    print(f\"âœ“ Dataset å·²å­˜åœ¨: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502ee00",
   "metadata": {},
   "source": [
    "## 1.8 Generate Synthetic SFT Dataset\n",
    "\n",
    "Based on the GRPO dataset, generate synthetic responses to create an SFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03da1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def extract_json_content(text, marker):\n",
    "    pattern = f\"ã€{marker}ã€‘(.*?)ã€/{marker}ã€‘\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    return None\n",
    "\n",
    "def generate_synthetic_response(messages):\n",
    "    user_content = messages[-1]['content']\n",
    "    \n",
    "    # Check task type\n",
    "    if \"ã€signal_step_input_jsonã€‘\" in user_content:\n",
    "        data = extract_json_content(user_content, \"signal_step_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for signal_step\n",
    "            scenario = data.get(\"scenario\", {})\n",
    "            phase_ids = scenario.get(\"phase_ids\", [1, 3])\n",
    "            \n",
    "            # Simple heuristic: pick valid phase, valid time\n",
    "            # Ideally pick a random one to prevent bias to one phase\n",
    "            next_phase_id = random.choice(phase_ids)\n",
    "            green_sec = random.randint(10, 60)\n",
    "            \n",
    "            return json.dumps({\n",
    "                \"next_phase_id\": next_phase_id,\n",
    "                \"green_sec\": green_sec\n",
    "            })\n",
    "            \n",
    "    elif \"ã€extend_decision_input_jsonã€‘\" in user_content:\n",
    "        data = extract_json_content(user_content, \"extend_decision_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for extend_decision\n",
    "            # Simple heuristic: 50% yes/no\n",
    "            should_extend = random.choice([\"æ˜¯\", \"å¦\"])\n",
    "            extend_sec = 0\n",
    "            if should_extend == \"æ˜¯\":\n",
    "                extend_sec = random.randint(5, 30)\n",
    "                \n",
    "            return json.dumps({\n",
    "                \"extend\": should_extend,\n",
    "                \"extend_sec\": extend_sec\n",
    "            })\n",
    "            \n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    INPUT_PATH = \"grpo_dataset_two_scenarios\"\n",
    "    OUTPUT_PATH = \"sft_dataset_synthetic\"\n",
    "    \n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        print(f\"Error: {INPUT_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading dataset from {INPUT_PATH}...\")\n",
    "    dataset = load_from_disk(INPUT_PATH)\n",
    "    \n",
    "    # Limit dataset to 2000 samples for SFT (doesn't need too much data)\n",
    "    MAX_SFT_SAMPLES = 2000\n",
    "    if len(dataset) > MAX_SFT_SAMPLES:\n",
    "        print(f\"Dataset has {len(dataset)} samples, limiting to {MAX_SFT_SAMPLES} for SFT...\")\n",
    "        indices = random.sample(range(len(dataset)), MAX_SFT_SAMPLES)\n",
    "        dataset = dataset.select(indices)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    print(\"Generating synthetic responses...\")\n",
    "    for item in dataset:\n",
    "        prompt_messages = item['prompt'] # List of {role, content}\n",
    "        \n",
    "        response_json = generate_synthetic_response(prompt_messages)\n",
    "        \n",
    "        if response_json:\n",
    "            # Create full conversation for SFT\n",
    "            # Clone messages to avoid modifying original reference if any\n",
    "            new_messages = [m.copy() for m in prompt_messages]\n",
    "            new_messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": response_json\n",
    "            })\n",
    "            \n",
    "            new_data.append({\n",
    "                \"messages\": new_messages\n",
    "            })\n",
    "    \n",
    "    if not new_data:\n",
    "        print(\"No valid samples generated!\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Generated {len(new_data)} samples.\")\n",
    "    \n",
    "    # Create new dataset\n",
    "    sft_dataset = Dataset.from_list(new_data)\n",
    "    sft_dataset.save_to_disk(OUTPUT_PATH)\n",
    "    print(f\"Saved synthetic SFT dataset to {OUTPUT_PATH}\")\n",
    "\n",
    "    # Verify one sample\n",
    "    print(\"\\nSample 0:\")\n",
    "    print(json.dumps(sft_dataset[0][\"messages\"][-1], ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab229777",
   "metadata": {},
   "source": [
    "## 1.9 SFT Training\n",
    "\n",
    "Train the model on the synthetic SFT dataset before GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b28ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# ==================== Config ====================\n",
    "max_seq_length = 1024\n",
    "lora_rank = 32\n",
    "# model_name = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "output_dir = \"checkpoints/sft_tsc_synthetic\"\n",
    "dataset_path = \"sft_dataset_synthetic\"\n",
    "\n",
    "# ==================== Load Model ====================\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=False,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.7,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# ==================== Load Dataset ====================\n",
    "if not os.path.isdir(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}. Run generate_synthetic_sft_dataset.py first.\")\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(f\"Loaded {len(dataset)} samples for SFT.\")\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# Limit eval dataset to 200 samples for speed\n",
    "if len(eval_dataset) > 500:\n",
    "    eval_dataset = eval_dataset.select(range(500))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# ==================== Configure Trainer ====================\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\", \n",
    ")\n",
    "\n",
    "# Formatting function for chat \n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    \n",
    "    # Logic to handle both batched and non-batched inputs\n",
    "    if isinstance(convos, list) and len(convos) > 0 and isinstance(convos[0], dict):\n",
    "        # Single conversation (list of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convos, tokenize=False, add_generation_prompt=False)]\n",
    "    else:\n",
    "        # Batch of conversations (list of lists of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "        \n",
    "    return texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False, \n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3, # Increase epochs relying on early stopping\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        fp16_full_eval=not torch.cuda.is_bf16_supported(),\n",
    "        bf16_full_eval=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=30, # Evaluate every 30 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=30,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 eval steps (90 steps)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting SFT Training with Early Stopping...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# Clean up memory for the next stage\n",
    "import gc\n",
    "try:\n",
    "    del model, tokenizer, trainer\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleaned up for GRPO stage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd3a3",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a017314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-19 22:58:59 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "âœ“ ä» SFT æ¨¡å‹å¼€å§‹è®­ç»ƒ: checkpoints/sft_tsc_synthetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM\n",
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM to enable standby.\n",
      "INFO:unsloth_zoo.log: Unsloth: Enabling vLLM standby mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 22:59:02 [vllm_utils.py:693] Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'\n",
      "==((====))==  Unsloth 2025.10.5: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Standby mode is enabled. Increasing `gpu_memory_utilization` to 0.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading model/models/unsloth/Qwen2___5-0___5B-Instruct with actual GPU utilization = 87.63%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.56 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
      "Unsloth: vLLM's KV Cache can use up to 19.69 GB. Also swap space = 6 GB.\n",
      "WARNING 01-19 22:59:07 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-19 22:59:07 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8762611176024063, 'max_num_batched_tokens': 2048, 'max_num_seqs': 288, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'enable_sleep_mode': True, 'model': 'model/models/unsloth/Qwen2___5-0___5B-Instruct'}\n",
      "INFO 01-19 22:59:07 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 22:59:07 [model.py:1510] Using max model len 2048\n",
      "INFO 01-19 22:59:08 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 01-19 22:59:08 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-19 22:59:08 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='model/models/unsloth/Qwen2___5-0___5B-Instruct', speculative_config=None, tokenizer='model/models/unsloth/Qwen2___5-0___5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=model/models/unsloth/Qwen2___5-0___5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 01-19 22:59:08 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-19 22:59:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-19 22:59:08 [gpu_model_runner.py:2602] Starting to load model model/models/unsloth/Qwen2___5-0___5B-Instruct...\n",
      "INFO 01-19 22:59:09 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 01-19 22:59:09 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3d4e708fb14fb99c92711d864a7004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 22:59:09 [default_loader.py:267] Loading weights took 0.47 seconds\n",
      "INFO 01-19 22:59:09 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-19 22:59:10 [gpu_model_runner.py:2653] Model loading took 0.9606 GiB and 0.556545 seconds\n",
      "INFO 01-19 22:59:13 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/17a02a4990/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-19 22:59:13 [backends.py:559] Dynamo bytecode transform time: 3.21 s\n",
      "INFO 01-19 22:59:14 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.963 s\n",
      "INFO 01-19 22:59:15 [monitor.py:34] torch.compile takes 3.21 s in total\n",
      "INFO 01-19 22:59:16 [gpu_worker.py:298] Available KV cache memory: 18.10 GiB\n",
      "INFO 01-19 22:59:16 [kv_cache_utils.py:1087] GPU KV cache size: 1,581,824 tokens\n",
      "INFO 01-19 22:59:16 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 772.38x\n",
      "INFO 01-19 22:59:16 [vllm_utils.py:698] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 01-19 22:59:16 [gpu_model_runner.py:3643] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.57it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:01<00:00, 27.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 22:59:20 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.71 GiB\n",
      "INFO 01-19 22:59:20 [vllm_utils.py:705] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 22:59:20 [core.py:210] init engine (profile, create kv cache, warmup model) took 10.71 seconds\n",
      "INFO 01-19 22:59:21 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at model/models/unsloth/Qwen2___5-0___5B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'post_layernorm', 'norm1', 'input_layernorm', 'attention_norm', 'post_attention_layernorm', 'pre_feedforward_layernorm', 'layer_norm1', 'ffn_norm', 'k_norm', 'layer_norm2', 'norm2']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'post_layernorm', 'norm1', 'input_layernorm', 'attention_norm', 'cross_attn_input_layernorm', 'cross_attn_post_attention_layernorm', 'post_attention_layernorm', 'pre_feedforward_layernorm', 'layer_norm1', 'ffn_norm', 'k_norm', 'layer_norm2', 'norm2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.5 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied qwen-2.5 chat template to match SFT training\n",
      "è®¾ç½® bos_token_id: 151644\n",
      "Tokenizer padding_side: left\n",
      "Tokenizer pad_token_id: 151654\n",
      "Tokenizer bos_token_id: 151644\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "\n",
    "# BASE_MODEL_DIR = \"model/models/qwen3-4B-SFT\"\n",
    "BASE_MODEL_DIR = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# BASE_MODEL_DIR = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "CHECKPOINT_DIR = \"checkpoints/grpo_tsc_two_scenarios_latest\"\n",
    "\n",
    "\n",
    "def _looks_like_checkpoint(path: str) -> bool:\n",
    "    if not os.path.isdir(path):\n",
    "        return False\n",
    "    marker_files = [\n",
    "        \"adapter_config.json\",\n",
    "        \"adapter_model.safetensors\",\n",
    "        \"adapter_model.bin\",\n",
    "        \"config.json\",\n",
    "    ]\n",
    "    return any(os.path.isfile(os.path.join(path, f)) for f in marker_files)\n",
    "\n",
    "\n",
    "\n",
    "SFT_CHECKPOINT_DIR = \"checkpoints/sft_tsc_synthetic\"\n",
    "\n",
    "resume_from = CHECKPOINT_DIR if _looks_like_checkpoint(CHECKPOINT_DIR) else BASE_MODEL_DIR\n",
    "\n",
    "if _looks_like_checkpoint(CHECKPOINT_DIR):\n",
    "    print(f\"âœ“ ä» checkpoint ç»§ç»­è®­ç»ƒ: {CHECKPOINT_DIR}\")\n",
    "    resume_from = CHECKPOINT_DIR\n",
    "elif _looks_like_checkpoint(SFT_CHECKPOINT_DIR):\n",
    "    print(f\"âœ“ ä» SFT æ¨¡å‹å¼€å§‹è®­ç»ƒ: {SFT_CHECKPOINT_DIR}\")\n",
    "    resume_from = SFT_CHECKPOINT_DIR\n",
    "else:\n",
    "    print(f\"â„¹ ä»åŸºç¡€æ¨¡å‹å¼€å§‹: {BASE_MODEL_DIR}\")\n",
    "    resume_from = BASE_MODEL_DIR\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=resume_from,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "if resume_from == BASE_MODEL_DIR:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=lora_rank * 2,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    _trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(_trainable) == 0:\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"lora\" in name.lower():\n",
    "                p.requires_grad = True\n",
    "        print(\"âš ï¸ checkpoint æœªæ£€æµ‹åˆ°å¯è®­ç»ƒå‚æ•°ï¼Œå·²å¼ºåˆ¶å¯ç”¨ LoRA å‚æ•°è®­ç»ƒ\")\n",
    "\n",
    "# Fix for GRPO generation: Must use left padding (applies to ALL cases)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Apply the same chat template as SFT training\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",  # Must match SFT training template\n",
    ")\n",
    "print(\"âœ“ Applied qwen-2.5 chat template to match SFT training\")\n",
    "\n",
    "# Ensure BOS token is preserved - this is critical for generation\n",
    "if tokenizer.bos_token_id is None:\n",
    "    # For Qwen, <|im_start|> acts as BOS in chat context\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "    print(f\"è®¾ç½® bos_token_id: {tokenizer.bos_token_id}\")\n",
    "\n",
    "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")\n",
    "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"Tokenizer bos_token_id: {tokenizer.bos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0673f7d",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43de6b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset åŠ è½½æˆåŠŸ: grpo_dataset_two_scenarios\n",
      "æ ·æœ¬æ•°: 1709\n",
      "âš ï¸ Stratified split failed (Stratifying by column is only supported for ClassLabel column, and column task_type is Value.), using random split\n",
      "âœ“ Train/Eval split: train=1674, eval=35\n",
      "  - eval task_type counts: Counter({'signal_step': 20, 'extend_decision': 15})\n",
      "ç¤ºä¾‹ Prompt (message list):\n",
      "[{'content': 'You are a traffic signal control expert. Output only valid JSON without any explanation.', 'role': 'system'}, {'content': 'ä½ æ˜¯äº¤é€šä¿¡å·æ§åˆ¶ä¼˜åŒ–ä¸“å®¶ã€‚ã€signal_step_input_jsonã€‘{\"crossing_id\":399226822,\"as_of\":\"2026-01-19 22:57:47\",\"scenario\":{\"phase_ids\":[1,3],\"phase_lane_map\":{\"1\":[\"441579299#14_1\",\"441579299#14_0\",\"441579299#14_2\",\"441579299#14_3\"],\"3\":[\"476664219#4_1\",\"476664219#4_0\"]}},\"state\":{\"current_phase_id\":1,\"current_phase_elapsed_sec\":11,\"current_phase_planned_green_sec\":11,\"phase_metrics_now\":[{\"phase_id\":1,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":4.0},{\"phase_id\":3,\"avg_queue_veh\":0.0,\"avg_passed_veh_in_current_green\":0.0}]}}ã€/signal_step_input_jsonã€‘\\n\\nå­—æ®µå«ä¹‰ï¼ˆä»…è¯´æ˜å«ä¹‰ï¼‰ï¼š\\n* state.phase_metrics_now[*].avg_queue_vehï¼šè¯¥ç›¸ä½åœ¨å½“å‰æ—¶åˆ»çš„å¹³å‡æ’é˜Ÿè½¦è¾†æ•°ï¼ˆè¾†ï¼‰ï¼›å¯ç”±è¯¥ç›¸ä½æ‰€æ§åˆ¶è½¦é“çš„æ’é˜Ÿè½¦è¾†æ•°å–å¹³å‡å¾—åˆ°ã€‚\\n* state.phase_metrics_now[*].avg_passed_veh_in_current_greenï¼šè¯¥ç›¸ä½åœ¨\"å½“å‰æ­£åœ¨æ‰§è¡Œçš„ç»¿ç¯ç›¸ä½\"å†…è‡³å½“å‰æ—¶åˆ»ç´¯è®¡é€šè¿‡è½¦è¾†æ•°ï¼ˆè¾†ï¼‰ï¼›åªæœ‰å½“å‰ç›¸ä½é€šå¸¸ä¼š >0ï¼Œéå½“å‰ç›¸ä½ä¸€èˆ¬ä¸º 0ã€‚\\n\\nä»»åŠ¡ï¼ˆå¿…é¡»å®Œæˆï¼‰ï¼š\\n1. åŸºäºè¾“å…¥ JSON çš„ scenario ä¸ stateï¼Œè‡ªè¡Œå†³å®šå†³ç­–ç­–ç•¥/å‚æ•°ï¼Œè¯„ä¼°å„ç›¸ä½å½“å‰éœ€æ±‚å¼ºåº¦å¹¶åšå‡º\"ä¸‹ä¸€æ­¥åŠ¨ä½œ\"ï¼ˆä»…åœ¨å†…éƒ¨ä½¿ç”¨ï¼Œä¸è¾“å‡ºä»»ä½•é¢„æµ‹è¿‡ç¨‹/ä¸­é—´å€¼ï¼‰ã€‚\\n2. è¾“å‡ºï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ next_phase_idï¼Œä»¥åŠè¯¥ç›¸ä½ç»¿ç¯æŒç»­æ—¶é—´ green_secï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚\\n\\nè¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š\\n1. ä½ å¿…é¡»æ˜¾å¼åˆ©ç”¨ avg_queue_veh ä¸ passed_veh_in_current_green æ¥å†³ç­–ï¼›ä¸å¾—æ— è§†è¾“å…¥éšæ„ç»™å‡ºç­”æ¡ˆã€‚\\n2. åœ¨ç¼ºå°‘ä»»ä½•é¢å¤–ç¡¬çº¦æŸï¼ˆä¸æä¾› cycle_constraints / phase_order / phase_limits / historyï¼‰çš„æƒ…å†µä¸‹ï¼š\\n   * next_phase_id å¿…é¡»æ¥è‡ª scenario.phase_idsï¼›\\n   * green_sec å¿…é¡»ä¸ºæ­£æ•´æ•°ç§’ï¼Œä¸”å¿…é¡»åœ¨ [1, 120] èŒƒå›´å†…ï¼›\\n   * green_sec å¿…é¡»\"åˆç†\"ï¼šé˜Ÿåˆ—æ›´å¤§/é€šè¿‡æ›´å°‘çš„ç›¸ä½å€¾å‘ç»™æ›´é•¿ç»¿ï¼›é˜Ÿåˆ—æ›´å°/é€šè¿‡æ›´å¤šçš„ç›¸ä½å€¾å‘ç»™æ›´çŸ­ç»¿ï¼›\\n   * è‹¥å¤šç›¸ä½éœ€æ±‚æ¥è¿‘ï¼Œå¯ä¼˜å…ˆåˆ‡æ¢åˆ°éå½“å‰ç›¸ä½ä»¥é™ä½å…¶ä»–ç›¸ä½ç­‰å¾…çš„ç´¯ç§¯é£é™©ã€‚\\n\\nè¾“å‡ºè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\\n1. åªè¾“å‡ºæœ€ç»ˆ JSONï¼ˆä¸è¦ä»»ä½•è¯´æ˜ã€ä¸è¦ Markdown ä»£ç å—ã€ä¸è¦é¢å¤–æ–‡æœ¬ã€ä¸è¦å¤è¿°è§„åˆ™ã€ä¸è¦è¾“å‡ºæ¨ç†è¿‡ç¨‹ï¼‰ã€‚\\n2. JSON å¿…é¡»ä¸ºå¯¹è±¡ï¼Œä¸”ä»…åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š\\n   {\"next_phase_id\": <int>, \"green_sec\": <int>}\\n3. green_sec å¿…é¡»åœ¨ [1, 120] èŒƒå›´å†…ã€‚\\n4. ä¸å…è®¸è¾“å‡ºå…¶å®ƒå­—æ®µï¼Œä¸å…è®¸æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡æœ¬ã€‚', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets import Dataset\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "EVAL_TEST_SIZE = 0.02\n",
    "EVAL_MAX_SAMPLES = 64\n",
    "EVAL_SEED = 42\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset ä¸å­˜åœ¨: {DATASET_PATH}\\n\"\n",
    "        \"è¯·å…ˆè¿è¡Œ generate_grpo_dataset.py ç”Ÿæˆç¦»çº¿ dataset\"\n",
    "    )\n",
    "\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "print(f\"âœ“ Dataset åŠ è½½æˆåŠŸ: {DATASET_PATH}\")\n",
    "print(f\"æ ·æœ¬æ•°: {len(dataset)}\")\n",
    "\n",
    "# Split out a small eval set to track real progress.\n",
    "# Prefer stratified split by task_type so both tasks appear in eval.\n",
    "try:\n",
    "    if isinstance(dataset, Dataset) and (\"task_type\" in dataset.column_names):\n",
    "        try:\n",
    "            # Try stratified split first\n",
    "            split = dataset.train_test_split(\n",
    "                test_size=EVAL_TEST_SIZE,\n",
    "                seed=EVAL_SEED,\n",
    "                stratify_by_column=\"task_type\",\n",
    "            )\n",
    "        except Exception as strat_err:\n",
    "            # Fallback to non-stratified split if stratification fails\n",
    "            print(f\"âš ï¸ Stratified split failed ({strat_err}), using random split\")\n",
    "            split = dataset.train_test_split(test_size=EVAL_TEST_SIZE, seed=EVAL_SEED)\n",
    "    else:\n",
    "        split = dataset.train_test_split(test_size=EVAL_TEST_SIZE, seed=EVAL_SEED)\n",
    "\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "    if len(eval_dataset) > EVAL_MAX_SAMPLES:\n",
    "        eval_dataset = eval_dataset.select(range(EVAL_MAX_SAMPLES))\n",
    "\n",
    "    print(f\"âœ“ Train/Eval split: train={len(train_dataset)}, eval={len(eval_dataset)}\")\n",
    "    if \"task_type\" in train_dataset.column_names:\n",
    "        from collections import Counter\n",
    "        print(\"  - eval task_type counts:\", Counter(eval_dataset[\"task_type\"]))\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Train/Eval split failed completely: {e}\")\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = None\n",
    "\n",
    "# NOTE: Do NOT apply chat template here!\n",
    "# GRPOTrainer expects prompt to be a list of messages, not a formatted string.\n",
    "# It will apply the chat template internally using processing_class (tokenizer).\n",
    "print(\"ç¤ºä¾‹ Prompt (message list):\")\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c127c",
   "metadata": {},
   "source": [
    "## 4. å¯¼å…¥ Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08728fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Reward function åŠ è½½æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "from tsc_reward_function import (\n",
    "    tsc_reward_fn,\n",
    "    cleanup_global_pool,\n",
    "    reward_diag_snapshot,\n",
    "    reward_diag_last,\n",
    ")\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class RewardDiagnosticsCallback(TrainerCallback):\n",
    "    def __init__(self, kl_spike_threshold: float = 5.0):\n",
    "        self.kl_spike_threshold = float(kl_spike_threshold)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        if not getattr(state, \"is_world_process_zero\", True):\n",
    "            return\n",
    "\n",
    "        # Periodic window summary (aligned to logging)\n",
    "        if getattr(args, \"logging_steps\", None) and state.global_step % int(args.logging_steps) == 0:\n",
    "            snap = reward_diag_snapshot(reset=True)\n",
    "            total = snap.get(\"window_total\", 0)\n",
    "            invalid = snap.get(\"window_invalid\", 0)\n",
    "            rate = (invalid / total) if total else 0.0\n",
    "            print(\n",
    "                f\"[reward_diag] steps {snap.get('window_start_step')}..{state.global_step} \"\n",
    "                f\"invalid_rate={rate:.3f} ({invalid}/{total})\"\n",
    "            )\n",
    "            by_task_total = snap.get(\"window_total_by_task\", {}) or {}\n",
    "            by_task_invalid = snap.get(\"window_invalid_by_task\", {}) or {}\n",
    "            by_reason = snap.get(\"window_reason_by_task\", {}) or {}\n",
    "            for task, t_total in sorted(by_task_total.items()):\n",
    "                t_invalid = int(by_task_invalid.get(task, 0))\n",
    "                t_rate = (t_invalid / t_total) if t_total else 0.0\n",
    "                reasons = by_reason.get(task, {}) or {}\n",
    "                reasons = {k: v for k, v in reasons.items() if k != \"ok\"}\n",
    "                top = sorted(reasons.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
    "                top_str = \", \".join([f\"{k}:{v}\" for k, v in top]) if top else \"n/a\"\n",
    "                print(\n",
    "                    f\"[reward_diag]  - {task}: invalid_rate={t_rate:.3f} \"\n",
    "                    f\"({t_invalid}/{t_total}) top={top_str}\"\n",
    "                )\n",
    "\n",
    "        # KL spike dump\n",
    "        kl = logs.get(\"kl\", None)\n",
    "        try:\n",
    "            if kl is not None and float(kl) >= self.kl_spike_threshold:\n",
    "                batch = reward_diag_last(state.global_step) or {}\n",
    "                print(\n",
    "                    f\"[reward_diag] KL spike at step={state.global_step} kl={float(kl):.4f} batch={batch}\"\n",
    "                )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "diag_callback = RewardDiagnosticsCallback(kl_spike_threshold=5.0)\n",
    "\n",
    "print(\"âœ“ Reward function åŠ è½½æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f371e",
   "metadata": {},
   "source": [
    "## 5. é…ç½® GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe254525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 2 to the `num_generations` of 4\n",
      "âœ“ GRPOConfig é…ç½®å®Œæˆ (æ¨¡å¼: æ­£å¼è®­ç»ƒ)\n",
      "  - max_steps: -1 (å…¨epoch)\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥ä½¿ç”¨çŸ­è®­ç»ƒ\n",
    "# QUICK_VERIFY = True\n",
    "QUICK_VERIFY = False  # æ­£å¼è®­ç»ƒè®¾ä¸º False\n",
    "\n",
    "# Eval é…ç½®ï¼ševal_dataset ä¸ºç©ºåˆ™è‡ªåŠ¨ç¦ç”¨\n",
    "DO_EVAL = False\n",
    "EVAL_STEPS = 20\n",
    "EVAL_BATCH_SIZE = 4  # å¿…é¡»èƒ½æ•´é™¤ num_generations\n",
    "\n",
    "config = GRPOConfig(\n",
    "    output_dir=\"checkpoints/grpo_tsc_two_scenarios\",\n",
    "\n",
    "    # æ‰¹æ¬¡é…ç½®\n",
    "    per_device_train_batch_size=2,\n",
    "    num_generations=4,  # Keep at 4 for proper GRPO\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # ç”Ÿæˆé…ç½®\n",
    "    max_completion_length=128,  \n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    use_vllm=False,\n",
    "\n",
    "    # è®­ç»ƒé…ç½®\n",
    "    learning_rate=2e-6,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=20 if QUICK_VERIFY else -1,  # å¿«é€ŸéªŒè¯ï¼š20æ­¥ï¼›æ­£å¼è®­ç»ƒï¼šå…¨epoch\n",
    "\n",
    "    # GRPO ç‰¹å®š\n",
    "    scale_rewards=True,\n",
    "\n",
    "    # Eval (track real progress on held-out states)\n",
    "    do_eval=DO_EVAL,\n",
    "    eval_strategy=\"steps\" if DO_EVAL else \"no\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_on_start=DO_EVAL,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "\n",
    "    # æ—¥å¿—ä¸ä¿å­˜\n",
    "    logging_steps=5,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # ä¼˜åŒ–å™¨\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.01,             # KL ç³»æ•°ï¼ˆ0.0 é»˜è®¤ä¸åŠ è½½ refï¼‰:contentReference[oaicite:1]{index=1}\n",
    "    max_grad_norm=0.5,    # æ¢¯åº¦è£å‰ªï¼ˆé˜² KL spikeï¼‰\n",
    "\n",
    "    # å…¶ä»–\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ GRPOConfig é…ç½®å®Œæˆ (æ¨¡å¼: {'å¿«é€ŸéªŒè¯' if QUICK_VERIFY else 'æ­£å¼è®­ç»ƒ'})\")\n",
    "if QUICK_VERIFY:\n",
    "    print(f\"  - max_steps: {config.max_steps} (éªŒè¯æ¨¡å¼)\")\n",
    "else:\n",
    "    print(f\"  - max_steps: {config.max_steps} (å…¨epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5b7e",
   "metadata": {},
   "source": [
    "## 6. åˆ›å»º GRPOTrainer å¹¶å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 151644, 'pad_token_id': 151654}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GRPOTrainer åˆ›å»ºæˆåŠŸ\n",
      "è®­ç»ƒæ ·æœ¬æ•°: 1674\n",
      "è¯„ä¼°æ ·æœ¬æ•°: 35\n",
      "æ¯ epoch steps: 1672\n",
      "\n",
      "============================================================\n",
      "å¼€å§‹ GRPO è®­ç»ƒ\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,674 | Num Epochs = 1 | Total steps = 418\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 17,596,416 of 511,629,184 (3.44% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-19 22:59:25 [processor.py:215] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n",
      "\n",
      "======================================================================\n",
      "[DEBUG] Completion #1\n",
      "======================================================================\n",
      "åŸå§‹è¾“å‡º:\n",
      "{\"extend\": \"æ˜¯\", \"extend_sec\": 28}\n",
      "======================================================================\n",
      "\n",
      "[tsc_reward_function] åˆå§‹åŒ– spawn è¿›ç¨‹æ± ï¼Œworkers=16ï¼Œç«¯å£èŒƒå›´=20000-21600\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 66/418 08:19 < 45:45, 0.13 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / tsc_reward_fn / mean</th>\n",
       "      <th>rewards / tsc_reward_fn / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>-2.284631</td>\n",
       "      <td>0.480154</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996547</td>\n",
       "      <td>-2.284631</td>\n",
       "      <td>1.765561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>-2.673727</td>\n",
       "      <td>0.368349</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.839054</td>\n",
       "      <td>-2.673727</td>\n",
       "      <td>3.155528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>-3.156652</td>\n",
       "      <td>0.559691</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.910482</td>\n",
       "      <td>-3.156652</td>\n",
       "      <td>3.720711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>-2.665663</td>\n",
       "      <td>0.885089</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.031524</td>\n",
       "      <td>-2.665663</td>\n",
       "      <td>2.740815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>-2.715567</td>\n",
       "      <td>0.758484</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.873310</td>\n",
       "      <td>-2.715567</td>\n",
       "      <td>2.761288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>-2.451863</td>\n",
       "      <td>0.771672</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.922809</td>\n",
       "      <td>-2.451863</td>\n",
       "      <td>3.238644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>-2.959561</td>\n",
       "      <td>0.769093</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.704161</td>\n",
       "      <td>-2.959561</td>\n",
       "      <td>2.827923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>-2.619528</td>\n",
       "      <td>0.537927</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.755447</td>\n",
       "      <td>-2.619528</td>\n",
       "      <td>2.773231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>-2.189375</td>\n",
       "      <td>0.689579</td>\n",
       "      <td>17.837500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.837500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.867572</td>\n",
       "      <td>-2.189375</td>\n",
       "      <td>3.173761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>-3.222269</td>\n",
       "      <td>1.144348</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.839834</td>\n",
       "      <td>-3.222269</td>\n",
       "      <td>3.347281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>-2.888550</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>17.762500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.762500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.791559</td>\n",
       "      <td>-2.888550</td>\n",
       "      <td>2.304205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>-3.722628</td>\n",
       "      <td>0.727349</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.932860</td>\n",
       "      <td>-3.722628</td>\n",
       "      <td>3.206720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[DEBUG] Completion #2\n",
      "======================================================================\n",
      "åŸå§‹è¾“å‡º:\n",
      "{\"next_phase_id\": 3, \"green_sec\": 56}\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[DEBUG] Completion #3\n",
      "======================================================================\n",
      "åŸå§‹è¾“å‡º:\n",
      "{\"next_phase_id\": 1, \"green_sec\": 52}\n",
      "======================================================================\n",
      "\n",
      "[reward_diag] steps 0..5 invalid_rate=0.175 (14/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.292 (14/48) top=extend_decision_final_green_out_of_bounds:10, extend_decision_extend_when_at_max_green:4\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/32) top=n/a\n",
      "[reward_diag] steps 5..10 invalid_rate=0.050 (4/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.111 (4/36) top=extend_decision_extend_when_at_max_green:3, extend_decision_final_green_out_of_bounds:1\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/44) top=n/a\n",
      "[reward_diag] steps 10..15 invalid_rate=0.100 (8/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.200 (8/40) top=extend_decision_final_green_out_of_bounds:4, extend_decision_extend_when_at_max_green:4\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/40) top=n/a\n",
      "[reward_diag] steps 15..20 invalid_rate=0.225 (18/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.375 (18/48) top=extend_decision_final_green_out_of_bounds:15, extend_decision_extend_when_at_max_green:3\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/32) top=n/a\n",
      "[reward_diag] steps 20..25 invalid_rate=0.087 (7/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.175 (7/40) top=extend_decision_final_green_out_of_bounds:7\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/40) top=n/a\n",
      "[reward_diag] steps 25..30 invalid_rate=0.113 (9/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.205 (9/44) top=extend_decision_extend_when_at_max_green:5, extend_decision_final_green_out_of_bounds:4\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/36) top=n/a\n",
      "[reward_diag] steps 30..35 invalid_rate=0.037 (3/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.107 (3/28) top=extend_decision_final_green_out_of_bounds:3\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/52) top=n/a\n",
      "[reward_diag] steps 35..40 invalid_rate=0.062 (5/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.156 (5/32) top=extend_decision_final_green_out_of_bounds:4, extend_decision_extend_when_at_max_green:1\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/48) top=n/a\n",
      "[reward_diag] steps 40..45 invalid_rate=0.087 (7/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.175 (7/40) top=extend_decision_extend_when_at_max_green:4, extend_decision_final_green_out_of_bounds:3\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/40) top=n/a\n",
      "[reward_diag] steps 45..50 invalid_rate=0.100 (8/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.200 (8/40) top=extend_decision_final_green_out_of_bounds:8\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/40) top=n/a\n",
      "[reward_diag] steps 50..55 invalid_rate=0.100 (8/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.222 (8/36) top=extend_decision_final_green_out_of_bounds:8\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/44) top=n/a\n",
      "[reward_diag] steps 55..60 invalid_rate=0.062 (5/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.104 (5/48) top=extend_decision_final_green_out_of_bounds:3, extend_decision_extend_when_at_max_green:2\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/32) top=n/a\n",
      "[reward_diag] steps 60..65 invalid_rate=0.138 (11/80)\n",
      "[reward_diag]  - extend_decision: invalid_rate=0.275 (11/40) top=extend_decision_final_green_out_of_bounds:8, extend_decision_extend_when_at_max_green:3\n",
      "[reward_diag]  - signal_step: invalid_rate=0.000 (0/40) top=n/a\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    reward_funcs=tsc_reward_fn,\n",
    "    callbacks=[diag_callback],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ“ GRPOTrainer åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}\")\n",
    "print(f\"è¯„ä¼°æ ·æœ¬æ•°: {0 if eval_dataset is None else len(eval_dataset)}\")\n",
    "print(f\"æ¯ epoch steps: {len(trainer.get_train_dataloader())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"å¼€å§‹ GRPO è®­ç»ƒ\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"è®­ç»ƒå®Œæˆ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222355eb",
   "metadata": {},
   "source": [
    "## 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad711919",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = \"checkpoints/grpo_tsc_two_scenarios_final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c1138",
   "metadata": {},
   "source": [
    "## 8. æ¸…ç†èµ„æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_global_pool()\n",
    "print(\"âœ“ Simulator æ± å·²æ¸…ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7aa60b",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•æ¨ç†ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4886c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_sample = dataset[0]\n",
    "test_prompt = test_sample[\"prompt\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"æµ‹è¯•ç”Ÿæˆ:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
