{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4885fa",
   "metadata": {},
   "source": [
    "# TSC 标准 Unsloth GRPO 训练（两大场景）\n",
    "\n",
    "使用标准 Unsloth GRPOTrainer + 离线 Dataset + Reward Function 回溯 SUMO 评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149eb79c",
   "metadata": {},
   "source": [
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40758b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "print(\"环境变量已设置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b9c5",
   "metadata": {},
   "source": [
    "## 1.5 生成/检查 Dataset（可选）\n",
    "\n",
    "如果 dataset 不存在，此 cell 会自动生成；如果已存在，则跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b377c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_grpo_dataset import main as generate_main, CONFIG\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print(f\"⚠️ Dataset 不存在: {DATASET_PATH}\")\n",
    "    print(\"开始生成 dataset...\")\n",
    "\n",
    "    # 快速验证模式：取消下面注释以使用小规模dataset\n",
    "    # QUICK_VERIFY = True\n",
    "    QUICK_VERIFY = False  # 正式训练设为 False\n",
    "\n",
    "    CONFIG.update({\n",
    "        \"output_dir\": DATASET_PATH,\n",
    "        \"state_dir\": \"grpo_states_two_scenarios\",\n",
    "        \"dataset_mode\": \"two_scenarios\",\n",
    "        \"steps_per_tl_signal_step\": 2,   # 每TL生成10个signal_step样本\n",
    "        \"steps_per_tl_extend_decision\": 2,  # 每TL生成100个extend_decision样本\n",
    "        \"decision_lead_sec\": 10,\n",
    "        \"phase_duration_scale_range\": (0.7, 1.3),\n",
    "        \"extend_min_green_range\": (5, 20),\n",
    "        \"extend_max_green_range\": (45, 120),\n",
    "        \"extend_wait_time_range\": (10, 20),\n",
    "        \"max_extend_sec\": 8,  # extend_decision 中 extend_sec 的最大值\n",
    "        \"max_tl_per_scenario\": 1,  \n",
    "        \"num_workers\": 4 if QUICK_VERIFY else 16,\n",
    "    })\n",
    "\n",
    "    print(\"当前配置:\")\n",
    "    print(f\"  - 模式: {'快速验证' if QUICK_VERIFY else '正式训练'}\")\n",
    "    print(f\"  - warmup_steps: {CONFIG['warmup_steps']}\")\n",
    "    print(f\"  - steps_per_tl_signal_step: {CONFIG['steps_per_tl_signal_step']}\")\n",
    "    print(f\"  - steps_per_tl_extend_decision: {CONFIG['steps_per_tl_extend_decision']}\")\n",
    "    print(f\"  - max_tl_per_scenario: {CONFIG['max_tl_per_scenario']}\")\n",
    "    print(f\"  - num_workers: {CONFIG['num_workers']}\")\n",
    "\n",
    "    generate_main()\n",
    "else:\n",
    "    print(f\"✓ Dataset 已存在: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502ee00",
   "metadata": {},
   "source": [
    "## 1.8 Generate Synthetic SFT Dataset\n",
    "\n",
    "Based on the GRPO dataset, generate synthetic responses to create an SFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03da1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def extract_json_content(text, marker):\n",
    "    pattern = f\"【{marker}】(.*?)【/{marker}】\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    return None\n",
    "\n",
    "def generate_synthetic_response(messages):\n",
    "    user_content = messages[-1]['content']\n",
    "    \n",
    "    # Check task type\n",
    "    if \"【signal_step_input_json】\" in user_content:\n",
    "        data = extract_json_content(user_content, \"signal_step_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for signal_step\n",
    "            scenario = data.get(\"scenario\", {})\n",
    "            phase_ids = scenario.get(\"phase_ids\", [1, 3])\n",
    "            \n",
    "            # Simple heuristic: pick valid phase, valid time\n",
    "            # Ideally pick a random one to prevent bias to one phase\n",
    "            next_phase_id = random.choice(phase_ids)\n",
    "            green_sec = random.randint(10, 60)\n",
    "            \n",
    "            return json.dumps({\n",
    "                \"next_phase_id\": next_phase_id,\n",
    "                \"green_sec\": green_sec\n",
    "            })\n",
    "            \n",
    "    elif \"【extend_decision_input_json】\" in user_content:\n",
    "        data = extract_json_content(user_content, \"extend_decision_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for extend_decision\n",
    "            # Simple heuristic: 50% yes/no\n",
    "            should_extend = random.choice([\"是\", \"否\"])\n",
    "            extend_sec = 0\n",
    "            if should_extend == \"是\":\n",
    "                extend_sec = random.randint(5, 30)\n",
    "                \n",
    "            return json.dumps({\n",
    "                \"extend\": should_extend,\n",
    "                \"extend_sec\": extend_sec\n",
    "            })\n",
    "            \n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    INPUT_PATH = \"grpo_dataset_two_scenarios\"\n",
    "    OUTPUT_PATH = \"sft_dataset_synthetic\"\n",
    "    \n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        print(f\"Error: {INPUT_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading dataset from {INPUT_PATH}...\")\n",
    "    dataset = load_from_disk(INPUT_PATH)\n",
    "    \n",
    "    # Limit dataset to 2000 samples for SFT (doesn't need too much data)\n",
    "    MAX_SFT_SAMPLES = 2000\n",
    "    if len(dataset) > MAX_SFT_SAMPLES:\n",
    "        print(f\"Dataset has {len(dataset)} samples, limiting to {MAX_SFT_SAMPLES} for SFT...\")\n",
    "        indices = random.sample(range(len(dataset)), MAX_SFT_SAMPLES)\n",
    "        dataset = dataset.select(indices)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    print(\"Generating synthetic responses...\")\n",
    "    for item in dataset:\n",
    "        prompt_messages = item['prompt'] # List of {role, content}\n",
    "        \n",
    "        response_json = generate_synthetic_response(prompt_messages)\n",
    "        \n",
    "        if response_json:\n",
    "            # Create full conversation for SFT\n",
    "            # Clone messages to avoid modifying original reference if any\n",
    "            new_messages = [m.copy() for m in prompt_messages]\n",
    "            new_messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": response_json\n",
    "            })\n",
    "            \n",
    "            new_data.append({\n",
    "                \"messages\": new_messages\n",
    "            })\n",
    "    \n",
    "    if not new_data:\n",
    "        print(\"No valid samples generated!\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Generated {len(new_data)} samples.\")\n",
    "    \n",
    "    # Create new dataset\n",
    "    sft_dataset = Dataset.from_list(new_data)\n",
    "    sft_dataset.save_to_disk(OUTPUT_PATH)\n",
    "    print(f\"Saved synthetic SFT dataset to {OUTPUT_PATH}\")\n",
    "\n",
    "    # Verify one sample\n",
    "    print(\"\\nSample 0:\")\n",
    "    print(json.dumps(sft_dataset[0][\"messages\"][-1], ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab229777",
   "metadata": {},
   "source": [
    "## 1.9 SFT Training\n",
    "\n",
    "Train the model on the synthetic SFT dataset before GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b28ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# ==================== Config ====================\n",
    "max_seq_length = 1024\n",
    "lora_rank = 32\n",
    "# model_name = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "output_dir = \"checkpoints/sft_tsc_synthetic\"\n",
    "dataset_path = \"sft_dataset_synthetic\"\n",
    "\n",
    "# ==================== Load Model ====================\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=False,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.7,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# ==================== Load Dataset ====================\n",
    "if not os.path.isdir(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}. Run generate_synthetic_sft_dataset.py first.\")\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(f\"Loaded {len(dataset)} samples for SFT.\")\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# Limit eval dataset to 200 samples for speed\n",
    "if len(eval_dataset) > 500:\n",
    "    eval_dataset = eval_dataset.select(range(500))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# ==================== Configure Trainer ====================\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\", \n",
    ")\n",
    "\n",
    "# Formatting function for chat \n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    \n",
    "    # Logic to handle both batched and non-batched inputs\n",
    "    if isinstance(convos, list) and len(convos) > 0 and isinstance(convos[0], dict):\n",
    "        # Single conversation (list of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convos, tokenize=False, add_generation_prompt=False)]\n",
    "    else:\n",
    "        # Batch of conversations (list of lists of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "        \n",
    "    return texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False, \n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3, # Increase epochs relying on early stopping\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        fp16_full_eval=not torch.cuda.is_bf16_supported(),\n",
    "        bf16_full_eval=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=30, # Evaluate every 30 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=30,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 eval steps (90 steps)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting SFT Training with Early Stopping...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# Clean up memory for the next stage\n",
    "import gc\n",
    "try:\n",
    "    del model, tokenizer, trainer\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleaned up for GRPO stage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd3a3",
   "metadata": {},
   "source": [
    "## 2. 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "\n",
    "# BASE_MODEL_DIR = \"model/models/qwen3-4B-SFT\"\n",
    "BASE_MODEL_DIR = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# BASE_MODEL_DIR = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "CHECKPOINT_DIR = \"checkpoints/grpo_tsc_two_scenarios_latest\"\n",
    "\n",
    "\n",
    "def _looks_like_checkpoint(path: str) -> bool:\n",
    "    if not os.path.isdir(path):\n",
    "        return False\n",
    "    marker_files = [\n",
    "        \"adapter_config.json\",\n",
    "        \"adapter_model.safetensors\",\n",
    "        \"adapter_model.bin\",\n",
    "        \"config.json\",\n",
    "    ]\n",
    "    return any(os.path.isfile(os.path.join(path, f)) for f in marker_files)\n",
    "\n",
    "\n",
    "SFT_CHECKPOINT_DIR = \"checkpoints/sft_tsc_synthetic\"\n",
    "GRPO_CHECKPOINT_DIR = \"checkpoints/grpo_tsc_two_scenarios\"\n",
    "\n",
    "def _find_latest_checkpoint(checkpoint_dir: str) -> str | None:\n",
    "    \"\"\"Find the latest valid checkpoint in a directory.\"\"\"\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        return None\n",
    "    candidates = []\n",
    "    for name in os.listdir(checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, name)\n",
    "        if _looks_like_checkpoint(path):\n",
    "            # Extract step number if possible (e.g., checkpoint-100)\n",
    "            try:\n",
    "                step = int(name.split('-')[-1])\n",
    "            except (ValueError, IndexError):\n",
    "                step = 0\n",
    "            candidates.append((step, path))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # Return the checkpoint with the highest step number\n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "resume_from = CHECKPOINT_DIR if _looks_like_checkpoint(CHECKPOINT_DIR) else BASE_MODEL_DIR\n",
    "\n",
    "# Priority: CHECKPOINT_DIR (latest symlink) > GRPO checkpoints > SFT checkpoint > base model\n",
    "if _looks_like_checkpoint(CHECKPOINT_DIR):\n",
    "    print(f\"✓ 从 checkpoint 继续训练: {CHECKPOINT_DIR}\")\n",
    "    resume_from = CHECKPOINT_DIR\n",
    "else:\n",
    "    # Check for checkpoints in grpo_tsc_two_scenarios\n",
    "    grpo_latest = _find_latest_checkpoint(GRPO_CHECKPOINT_DIR)\n",
    "    if grpo_latest:\n",
    "        print(f\"✓ 从 GRPO checkpoint 继续训练: {grpo_latest}\")\n",
    "        resume_from = grpo_latest\n",
    "    elif _looks_like_checkpoint(SFT_CHECKPOINT_DIR):\n",
    "        print(f\"✓ 从 SFT 模型开始训练: {SFT_CHECKPOINT_DIR}\")\n",
    "        resume_from = SFT_CHECKPOINT_DIR\n",
    "    else:\n",
    "        print(f\"ℹ 从基础模型开始: {BASE_MODEL_DIR}\")\n",
    "        resume_from = BASE_MODEL_DIR\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=resume_from,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "if resume_from == BASE_MODEL_DIR:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=lora_rank * 2,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    _trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(_trainable) == 0:\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"lora\" in name.lower():\n",
    "                p.requires_grad = True\n",
    "        print(\"⚠️ checkpoint 未检测到可训练参数，已强制启用 LoRA 参数训练\")\n",
    "\n",
    "# Fix for GRPO generation: Must use left padding (applies to ALL cases)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Apply the same chat template as SFT training\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",  # Must match SFT training template\n",
    ")\n",
    "print(\"✓ Applied qwen-2.5 chat template to match SFT training\")\n",
    "\n",
    "# Ensure BOS token is preserved - this is critical for generation\n",
    "if tokenizer.bos_token_id is None:\n",
    "    # For Qwen, <|im_start|> acts as BOS in chat context\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "    print(f\"设置 bos_token_id: {tokenizer.bos_token_id}\")\n",
    "\n",
    "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")\n",
    "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"Tokenizer bos_token_id: {tokenizer.bos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0673f7d",
   "metadata": {},
   "source": [
    "## 3. 加载 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets import Dataset\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "EVAL_TEST_SIZE = 0.001\n",
    "EVAL_MAX_SAMPLES = 64\n",
    "EVAL_SEED = 42\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset 不存在: {DATASET_PATH}\\n\"\n",
    "        \"请先运行 generate_grpo_dataset.py 生成离线 dataset\"\n",
    "    )\n",
    "\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "print(f\"✓ Dataset 加载成功: {DATASET_PATH}\")\n",
    "print(f\"样本数: {len(dataset)}\")\n",
    "\n",
    "# Split out a small eval set to track real progress.\n",
    "# Prefer stratified split by task_type so both tasks appear in eval.\n",
    "try:\n",
    "    if isinstance(dataset, Dataset) and (\"task_type\" in dataset.column_names):\n",
    "        try:\n",
    "            # Try stratified split first\n",
    "            split = dataset.train_test_split(\n",
    "                test_size=EVAL_TEST_SIZE,\n",
    "                seed=EVAL_SEED,\n",
    "                stratify_by_column=\"task_type\",\n",
    "            )\n",
    "        except Exception as strat_err:\n",
    "            # Fallback to non-stratified split if stratification fails\n",
    "            print(f\"⚠️ Stratified split failed ({strat_err}), using random split\")\n",
    "            split = dataset.train_test_split(test_size=EVAL_TEST_SIZE, seed=EVAL_SEED)\n",
    "    else:\n",
    "        split = dataset.train_test_split(test_size=EVAL_TEST_SIZE, seed=EVAL_SEED)\n",
    "\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "    if len(eval_dataset) > EVAL_MAX_SAMPLES:\n",
    "        eval_dataset = eval_dataset.select(range(EVAL_MAX_SAMPLES))\n",
    "\n",
    "    print(f\"✓ Train/Eval split: train={len(train_dataset)}, eval={len(eval_dataset)}\")\n",
    "    if \"task_type\" in train_dataset.column_names:\n",
    "        from collections import Counter\n",
    "        print(\"  - eval task_type counts:\", Counter(eval_dataset[\"task_type\"]))\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Train/Eval split failed completely: {e}\")\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = None\n",
    "\n",
    "# NOTE: Do NOT apply chat template here!\n",
    "# GRPOTrainer expects prompt to be a list of messages, not a formatted string.\n",
    "# It will apply the chat template internally using processing_class (tokenizer).\n",
    "print(\"示例 Prompt (message list):\")\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ada76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[50][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c127c",
   "metadata": {},
   "source": [
    "## 4. 导入 Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08728fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsc_reward_function import (\n",
    "    tsc_reward_sim_fn,\n",
    "    tsc_reward_format_fn,\n",
    "    cleanup_global_pool,\n",
    "    reward_diag_snapshot,\n",
    "    reward_diag_last,\n",
    ")\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class RewardDiagnosticsCallback(TrainerCallback):\n",
    "    def __init__(self, kl_spike_threshold: float = 5.0):\n",
    "        self.kl_spike_threshold = float(kl_spike_threshold)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        if not getattr(state, \"is_world_process_zero\", True):\n",
    "            return\n",
    "\n",
    "        # Periodic window summary (aligned to logging)\n",
    "        if getattr(args, \"logging_steps\", None) and state.global_step % int(args.logging_steps) == 0:\n",
    "            snap = reward_diag_snapshot(reset=True)\n",
    "            total = snap.get(\"window_total\", 0)\n",
    "            invalid = snap.get(\"window_invalid\", 0)\n",
    "            rate = (invalid / total) if total else 0.0\n",
    "            print(\n",
    "                f\"[reward_diag] steps {snap.get('window_start_step')}..{state.global_step} \"\n",
    "                f\"invalid_rate={rate:.3f} ({invalid}/{total})\"\n",
    "            )\n",
    "            by_task_total = snap.get(\"window_total_by_task\", {}) or {}\n",
    "            by_task_invalid = snap.get(\"window_invalid_by_task\", {}) or {}\n",
    "            by_reason = snap.get(\"window_reason_by_task\", {}) or {}\n",
    "            for task, t_total in sorted(by_task_total.items()):\n",
    "                t_invalid = int(by_task_invalid.get(task, 0))\n",
    "                t_rate = (t_invalid / t_total) if t_total else 0.0\n",
    "                reasons = by_reason.get(task, {}) or {}\n",
    "                reasons = {k: v for k, v in reasons.items() if k != \"ok\"}\n",
    "                top = sorted(reasons.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
    "                top_str = \", \".join([f\"{k}:{v}\" for k, v in top]) if top else \"n/a\"\n",
    "                print(\n",
    "                    f\"[reward_diag]  - {task}: invalid_rate={t_rate:.3f} \"\n",
    "                    f\"({t_invalid}/{t_total}) top={top_str}\"\n",
    "                )\n",
    "\n",
    "        # KL spike dump\n",
    "        kl = logs.get(\"kl\", None)\n",
    "        try:\n",
    "            if kl is not None and float(kl) >= self.kl_spike_threshold:\n",
    "                batch = reward_diag_last(state.global_step) or {}\n",
    "                print(\n",
    "                    f\"[reward_diag] KL spike at step={state.global_step} kl={float(kl):.4f} batch={batch}\"\n",
    "                )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "diag_callback = RewardDiagnosticsCallback(kl_spike_threshold=5.0)\n",
    "\n",
    "print(\"✓ Reward function 加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f371e",
   "metadata": {},
   "source": [
    "## 5. 配置 GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe254525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# 快速验证模式：取消下面注释以使用短训练\n",
    "# QUICK_VERIFY = True\n",
    "QUICK_VERIFY = False  # 正式训练设为 False\n",
    "\n",
    "# Eval 配置：eval_dataset 为空则自动禁用\n",
    "DO_EVAL = False\n",
    "EVAL_STEPS = 20\n",
    "EVAL_BATCH_SIZE = 4  # 必须能整除 num_generations\n",
    "\n",
    "config = GRPOConfig(\n",
    "    output_dir=\"checkpoints/grpo_tsc_two_scenarios\",\n",
    "\n",
    "    # 批次配置\n",
    "    per_device_train_batch_size=2,\n",
    "    num_generations=4,  # Keep at 4 for proper GRPO\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # 生成配置\n",
    "    max_completion_length=128,  \n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    use_vllm=False,\n",
    "\n",
    "    # 训练配置\n",
    "    learning_rate=2e-6,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=20 if QUICK_VERIFY else -1,  # 快速验证：20步；正式训练：全epoch\n",
    "\n",
    "    # GRPO 特定\n",
    "    scale_rewards=True,\n",
    "\n",
    "    # Eval (track real progress on held-out states)\n",
    "    do_eval=DO_EVAL,\n",
    "    eval_strategy=\"steps\" if DO_EVAL else \"no\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_on_start=DO_EVAL,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "\n",
    "    # 日志与保存\n",
    "    logging_steps=5,\n",
    "    save_steps=300,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # 优化器\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.01,             # KL 系数（0.0 默认不加载 ref）:contentReference[oaicite:1]{index=1}\n",
    "    max_grad_norm=0.5,    # 梯度裁剪（防 KL spike）\n",
    "\n",
    "    # 其他\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"✓ GRPOConfig 配置完成 (模式: {'快速验证' if QUICK_VERIFY else '正式训练'})\")\n",
    "if QUICK_VERIFY:\n",
    "    print(f\"  - max_steps: {config.max_steps} (验证模式)\")\n",
    "else:\n",
    "    print(f\"  - max_steps: {config.max_steps} (全epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5b7e",
   "metadata": {},
   "source": [
    "## 6. 创建 GRPOTrainer 并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    reward_funcs=[tsc_reward_sim_fn, tsc_reward_format_fn],\n",
    "    callbacks=[diag_callback],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"✓ GRPOTrainer 创建成功\")\n",
    "print(f\"训练样本数: {len(train_dataset)}\")\n",
    "print(f\"评估样本数: {0 if eval_dataset is None else len(eval_dataset)}\")\n",
    "print(f\"每 epoch steps: {len(trainer.get_train_dataloader())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"开始 GRPO 训练\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"训练完成\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222355eb",
   "metadata": {},
   "source": [
    "## 7. 保存最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad711919",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = \"checkpoints/grpo_tsc_two_scenarios_final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"✓ 最终模型已保存到: {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c1138",
   "metadata": {},
   "source": [
    "## 8. 清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_global_pool()\n",
    "print(\"✓ Simulator 池已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7aa60b",
   "metadata": {},
   "source": [
    "## 9. 测试推理（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4886c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_sample = dataset[0]\n",
    "test_prompt = test_sample[\"prompt\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"测试生成:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
