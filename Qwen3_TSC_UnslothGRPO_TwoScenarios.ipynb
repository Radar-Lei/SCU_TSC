{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4885fa",
   "metadata": {},
   "source": [
    "# TSC æ ‡å‡† Unsloth GRPO è®­ç»ƒï¼ˆä¸¤å¤§åœºæ™¯ï¼‰\n",
    "\n",
    "ä½¿ç”¨æ ‡å‡† Unsloth GRPOTrainer + ç¦»çº¿ Dataset + Reward Function å›æº¯ SUMO è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149eb79c",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40758b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯å¢ƒå˜é‡å·²è®¾ç½®\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"UNSLOTH_USE_MODELSCOPE\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "print(\"ç¯å¢ƒå˜é‡å·²è®¾ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b9c5",
   "metadata": {},
   "source": [
    "## 1.5 ç”Ÿæˆ/æ£€æŸ¥ Datasetï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœ dataset ä¸å­˜åœ¨ï¼Œæ­¤ cell ä¼šè‡ªåŠ¨ç”Ÿæˆï¼›å¦‚æœå·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b377c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_grpo_dataset import main as generate_main, CONFIG\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    print(f\"âš ï¸ Dataset ä¸å­˜åœ¨: {DATASET_PATH}\")\n",
    "    print(\"å¼€å§‹ç”Ÿæˆ dataset...\")\n",
    "\n",
    "    # å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥ä½¿ç”¨å°è§„æ¨¡dataset\n",
    "    # QUICK_VERIFY = True\n",
    "    QUICK_VERIFY = False  # æ­£å¼è®­ç»ƒè®¾ä¸º False\n",
    "\n",
    "    CONFIG.update({\n",
    "        \"output_dir\": DATASET_PATH,\n",
    "        \"state_dir\": \"grpo_states_two_scenarios\",\n",
    "        \"dataset_mode\": \"two_scenarios\",\n",
    "        \"steps_per_tl_signal_step\": 100,   # æ¯TLç”Ÿæˆ100ä¸ªsignal_stepæ ·æœ¬\n",
    "        \"steps_per_tl_extend_decision\": 100,  # æ¯TLç”Ÿæˆ100ä¸ªextend_decisionæ ·æœ¬\n",
    "        \"decision_lead_sec\": 10,\n",
    "        \"phase_duration_scale_range\": (0.7, 1.3),\n",
    "        \"extend_min_green_range\": (5, 20),\n",
    "        \"extend_max_green_range\": (25, 120),\n",
    "        \"extend_wait_time_range\": (5, 25),\n",
    "        \"max_tl_per_scenario\": 10,  # æ¯åœºæ™¯æœ€å¤š10ä¸ªTL\n",
    "        \"num_workers\": 4 if QUICK_VERIFY else 16,\n",
    "    })\n",
    "\n",
    "    print(\"å½“å‰é…ç½®:\")\n",
    "    print(f\"  - æ¨¡å¼: {'å¿«é€ŸéªŒè¯' if QUICK_VERIFY else 'æ­£å¼è®­ç»ƒ'}\")\n",
    "    print(f\"  - warmup_steps: {CONFIG['warmup_steps']}\")\n",
    "    print(f\"  - steps_per_tl_signal_step: {CONFIG['steps_per_tl_signal_step']}\")\n",
    "    print(f\"  - steps_per_tl_extend_decision: {CONFIG['steps_per_tl_extend_decision']}\")\n",
    "    print(f\"  - max_tl_per_scenario: {CONFIG['max_tl_per_scenario']}\")\n",
    "    print(f\"  - num_workers: {CONFIG['num_workers']}\")\n",
    "\n",
    "    generate_main()\n",
    "else:\n",
    "    print(f\"âœ“ Dataset å·²å­˜åœ¨: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502ee00",
   "metadata": {},
   "source": [
    "## 1.8 Generate Synthetic SFT Dataset\n",
    "\n",
    "Based on the GRPO dataset, generate synthetic responses to create an SFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03da1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def extract_json_content(text, marker):\n",
    "    pattern = f\"ã€{marker}ã€‘(.*?)ã€/{marker}ã€‘\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    return None\n",
    "\n",
    "def generate_synthetic_response(messages):\n",
    "    user_content = messages[-1]['content']\n",
    "    \n",
    "    # Check task type\n",
    "    if \"ã€signal_step_input_jsonã€‘\" in user_content:\n",
    "        data = extract_json_content(user_content, \"signal_step_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for signal_step\n",
    "            scenario = data.get(\"scenario\", {})\n",
    "            phase_ids = scenario.get(\"phase_ids\", [1, 3])\n",
    "            \n",
    "            # Simple heuristic: pick valid phase, valid time\n",
    "            # Ideally pick a random one to prevent bias to one phase\n",
    "            next_phase_id = random.choice(phase_ids)\n",
    "            green_sec = random.randint(10, 60)\n",
    "            \n",
    "            return json.dumps({\n",
    "                \"next_phase_id\": next_phase_id,\n",
    "                \"green_sec\": green_sec\n",
    "            })\n",
    "            \n",
    "    elif \"ã€extend_decision_input_jsonã€‘\" in user_content:\n",
    "        data = extract_json_content(user_content, \"extend_decision_input_json\")\n",
    "        if data:\n",
    "            # Heuristic for extend_decision\n",
    "            # Simple heuristic: 50% yes/no\n",
    "            should_extend = random.choice([\"æ˜¯\", \"å¦\"])\n",
    "            extend_sec = 0\n",
    "            if should_extend == \"æ˜¯\":\n",
    "                extend_sec = random.randint(5, 30)\n",
    "                \n",
    "            return json.dumps({\n",
    "                \"extend\": should_extend,\n",
    "                \"extend_sec\": extend_sec\n",
    "            })\n",
    "            \n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    INPUT_PATH = \"grpo_dataset_two_scenarios\"\n",
    "    OUTPUT_PATH = \"sft_dataset_synthetic\"\n",
    "    \n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        print(f\"Error: {INPUT_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading dataset from {INPUT_PATH}...\")\n",
    "    dataset = load_from_disk(INPUT_PATH)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    print(\"Generating synthetic responses...\")\n",
    "    for item in dataset:\n",
    "        prompt_messages = item['prompt'] # List of {role, content}\n",
    "        \n",
    "        response_json = generate_synthetic_response(prompt_messages)\n",
    "        \n",
    "        if response_json:\n",
    "            # Create full conversation for SFT\n",
    "            # Clone messages to avoid modifying original reference if any\n",
    "            new_messages = [m.copy() for m in prompt_messages]\n",
    "            new_messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": response_json\n",
    "            })\n",
    "            \n",
    "            new_data.append({\n",
    "                \"messages\": new_messages\n",
    "            })\n",
    "    \n",
    "    if not new_data:\n",
    "        print(\"No valid samples generated!\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Generated {len(new_data)} samples.\")\n",
    "    \n",
    "    # Create new dataset\n",
    "    sft_dataset = Dataset.from_list(new_data)\n",
    "    sft_dataset.save_to_disk(OUTPUT_PATH)\n",
    "    print(f\"Saved synthetic SFT dataset to {OUTPUT_PATH}\")\n",
    "\n",
    "    # Verify one sample\n",
    "    print(\"\\nSample 0:\")\n",
    "    print(json.dumps(sft_dataset[0][\"messages\"][-1], ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab229777",
   "metadata": {},
   "source": [
    "## 1.9 SFT Training\n",
    "\n",
    "Train the model on the synthetic SFT dataset before GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b28ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-19 12:30:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/unsloth/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 12:30:52,908 - modelscope - INFO - Got 31 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab903210e2f42a99e5994a173be565e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 31 items:   0%|          | 0.00/31.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5d7f86e7d4683a7762e70669f8d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [added_tokens.json]:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab63878b29f452696c071d70c854877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/config.json.metadata]:   0%|          | 0.00/101 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3b081404414edba7578d2dbd28dfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd39cae2c744d078db59f63e81ad1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/config.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704e1f36e3c24fd28b50522812f95b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/761 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ae01e07afd43cc90a9f5d3d7f5ca16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58d8e6bf5d745f0854090be0bd1ce41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/added_tokens.json.metadata]:   0%|          | 0.00/100 [00:00<?, ?B/sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42848a39977e451db46e2ee24ccd5a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/added_tokens.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8ae0af9c4b4b4db012d698148feaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/generation_config.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea0d1621791423382939560d2145625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/generation_config.json.metadata]:   0%|          | 0.00/101 [00:00<?,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0950e3d61e4795a31c9fecec516914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9bdb3aa9a64b569edfd424a3333c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/merges.txt.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2122c674c7d4116804caa0e9b4730a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/merges.txt.metadata]:   0%|          | 0.00/101 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f65f50a15d4d6c8303ad5e81f0282f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/942M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e11e548d8ec46cca2f04d416ce57f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/model.safetensors.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746ccaf8412549a69407a48728ce8f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/model.safetensors.metadata]:   0%|          | 0.00/125 [00:00<?, ?B/sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a992ae19f94b5db002c0af075f272d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/7.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9512dc4febc4fe284bbef1fd5feb27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/README.md.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e624773f834dbca34a7b892a136a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/README.md.metadata]:   0%|          | 0.00/100 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d69742b6b5f4af395c378f671bdf5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [special_tokens_map.json]:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd948fdf1b04ef4a4f1487c99426c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/special_tokens_map.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02f6dfc688741b581660c9f033581a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/special_tokens_map.json.metadata]:   0%|          | 0.00/101 [00:00<?â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d771303d1de34a51829659d655b54f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/10.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90d475911cf42889985a147c77766ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/tokenizer.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d575993bc2f645f8abe63b4de00837ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/tokenizer.json.metadata]:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf0846b48df485485e8530c8a10b281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/7.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa951b961a047459199adb91ca83cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/tokenizer_config.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b0b898c29348e5a8cd5210f30da91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/tokenizer_config.json.metadata]:   0%|          | 0.00/101 [00:00<?, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca9915d72894e42af8f96300b83789c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6230ae36f6849dabb36e33dbc4844b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/vocab.json.lock]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f875a3d58141ed97a7616021ee34ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [.cache/huggingface/download/vocab.json.metadata]:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 12:32:44,226 - modelscope - INFO - Download model 'unsloth/Qwen2.5-0.5B-Instruct' successfully.\n",
      "2026-01-19 12:32:44,226 - modelscope - INFO - Creating symbolic link [/root/.cache/modelscope/hub/models/unsloth/Qwen2.5-0.5B-Instruct].\n",
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:32:44 [vllm_utils.py:693] Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'\n",
      "==((====))==  Unsloth 2025.10.5: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading /root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct with actual GPU utilization = 68.09%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.56 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 256.\n",
      "Unsloth: vLLM's KV Cache can use up to 15.09 GB. Also swap space = 6 GB.\n",
      "WARNING 01-19 12:32:49 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-19 12:32:49 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6808672312798099, 'max_num_batched_tokens': 2048, 'max_num_seqs': 256, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': '/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct'}\n",
      "INFO 01-19 12:32:49 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:32:49 [model.py:1510] Using max model len 2048\n",
      "INFO 01-19 12:32:49 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 01-19 12:32:49 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-19 12:32:50 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct', speculative_config=None, tokenizer='/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 01-19 12:32:50 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-19 12:32:50 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-19 12:32:50 [gpu_model_runner.py:2602] Starting to load model /root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct...\n",
      "INFO 01-19 12:32:50 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 01-19 12:32:50 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef34a5920b2450a988d454014aae64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:32:51 [default_loader.py:267] Loading weights took 0.48 seconds\n",
      "INFO 01-19 12:32:51 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-19 12:32:51 [gpu_model_runner.py:2653] Model loading took 0.9600 GiB and 0.571146 seconds\n",
      "INFO 01-19 12:32:55 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/531365d0af/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-19 12:32:55 [backends.py:559] Dynamo bytecode transform time: 3.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23.71it/s, triton_poi_fused_view_4]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:32:56 [backends.py:197] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 43.18it/s, triton_poi_fused_view_8]                            \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1404.03it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1420.19it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1215.11it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1371.14it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1137.28it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1433.13it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1469.62it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1446.76it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1298.86it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1507.12it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1460.58it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1354.36it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1293.56it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1287.91it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1418.27it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1456.02it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1286.16it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1461.94it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1491.22it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1491.63it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1457.82it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1470.48it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 45.45it/s, triton_per_fused__to_copy_add_mean_mul_pow_rsqrt_4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:33:07 [backends.py:218] Compiling a graph for dynamic shape takes 11.86 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:33:11 [monitor.py:34] torch.compile takes 15.07 s in total\n",
      "INFO 01-19 12:33:12 [gpu_worker.py:298] Available KV cache memory: 13.67 GiB\n",
      "INFO 01-19 12:33:12 [kv_cache_utils.py:1087] GPU KV cache size: 1,194,768 tokens\n",
      "INFO 01-19 12:33:12 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 583.38x\n",
      "INFO 01-19 12:33:12 [vllm_utils.py:698] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 01-19 12:33:12 [gpu_model_runner.py:3643] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:06<00:00, 10.01it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 25.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:33:21 [gpu_model_runner.py:3480] Graph capturing finished in 8 secs, took 0.69 GiB\n",
      "INFO 01-19 12:33:21 [vllm_utils.py:705] Unsloth: Patched vLLM v1 graph capture finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 12:33:21 [core.py:210] init engine (profile, create kv cache, warmup model) took 30.01 seconds\n",
      "INFO 01-19 12:33:21 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at /root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Just some info: will skip parsing ['k_norm', 'layer_norm2', 'ffn_norm', 'pre_feedforward_layernorm', 'norm2', 'norm1', 'post_attention_layernorm', 'q_norm', 'attention_norm', 'input_layernorm', 'post_layernorm', 'post_feedforward_layernorm', 'layer_norm1']\n",
      "Unsloth: Just some info: will skip parsing ['cross_attn_post_attention_layernorm', 'k_norm', 'layer_norm2', 'ffn_norm', 'cross_attn_input_layernorm', 'pre_feedforward_layernorm', 'norm2', 'norm1', 'post_attention_layernorm', 'q_norm', 'attention_norm', 'input_layernorm', 'post_layernorm', 'post_feedforward_layernorm', 'layer_norm1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.5 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 samples for SFT.\n",
      "Train samples: 1900\n",
      "Eval samples: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6cf283d7974a20969a99dd976f9b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=28):   0%|          | 0/1900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd83cc0278ee4172a6ebfbeecb6d17e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=28):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 151654}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT Training with Early Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,900 | Num Epochs = 3 | Total steps = 714\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 17,596,416 of 511,629,184 (3.44% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='714' max='714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [714/714 12:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.377344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.087436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.042884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.036309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.034673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.035713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.032839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.032580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.032082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.031827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.031633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.031411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.031001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.030954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.031121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.030625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.030801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.030560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.030407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.030508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.030264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to checkpoints/sft_tsc_synthetic\n",
      "Done.\n",
      "Memory cleaned up for GRPO stage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# ==================== Config ====================\n",
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "# model_name = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "output_dir = \"checkpoints/sft_tsc_synthetic\"\n",
    "dataset_path = \"sft_dataset_synthetic\"\n",
    "\n",
    "# ==================== Load Model ====================\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.7,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "# ==================== Load Dataset ====================\n",
    "if not os.path.isdir(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}. Run generate_synthetic_sft_dataset.py first.\")\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(f\"Loaded {len(dataset)} samples for SFT.\")\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# Limit eval dataset to 200 samples for speed\n",
    "if len(eval_dataset) > 500:\n",
    "    eval_dataset = eval_dataset.select(range(500))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# ==================== Configure Trainer ====================\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\", \n",
    ")\n",
    "\n",
    "# Formatting function for chat \n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    \n",
    "    # Logic to handle both batched and non-batched inputs\n",
    "    if isinstance(convos, list) and len(convos) > 0 and isinstance(convos[0], dict):\n",
    "        # Single conversation (list of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convos, tokenize=False, add_generation_prompt=False)]\n",
    "    else:\n",
    "        # Batch of conversations (list of lists of dicts)\n",
    "        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "        \n",
    "    return texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False, \n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3, # Increase epochs relying on early stopping\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=30, # Evaluate every 10 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=30,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 eval steps (90 steps)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting SFT Training with Early Stopping...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# Clean up memory for the next stage\n",
    "import gc\n",
    "try:\n",
    "    del model, tokenizer, trainer\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleaned up for GRPO stage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd3a3",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a017314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-19 13:36:53 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "âœ“ ä» SFT æ¨¡å‹å¼€å§‹è®­ç»ƒ: checkpoints/sft_tsc_synthetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM\n",
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM to enable standby.\n",
      "INFO:unsloth_zoo.log: Unsloth: Enabling vLLM standby mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 13:37:03 [vllm_utils.py:693] Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'\n",
      "==((====))==  Unsloth 2025.10.5: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Standby mode is enabled. Increasing `gpu_memory_utilization` to 0.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading /root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct with actual GPU utilization = 87.63%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.56 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
      "Unsloth: vLLM's KV Cache can use up to 19.69 GB. Also swap space = 6 GB.\n",
      "WARNING 01-19 13:37:08 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-19 13:37:09 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8762611176024063, 'max_num_batched_tokens': 2048, 'max_num_seqs': 288, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'enable_sleep_mode': True, 'model': '/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct'}\n",
      "INFO 01-19 13:37:09 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 13:37:09 [model.py:1510] Using max model len 2048\n",
      "INFO 01-19 13:37:10 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 01-19 13:37:10 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-19 13:37:10 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct', speculative_config=None, tokenizer='/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":24,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 01-19 13:37:10 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-19 13:37:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-19 13:37:11 [gpu_model_runner.py:2602] Starting to load model /root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct...\n",
      "INFO 01-19 13:37:11 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 01-19 13:37:11 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fda02603d64785a96ffda4d594c049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 13:37:11 [default_loader.py:267] Loading weights took 0.49 seconds\n",
      "INFO 01-19 13:37:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-19 13:37:12 [gpu_model_runner.py:2653] Model loading took 0.9606 GiB and 0.598131 seconds\n",
      "INFO 01-19 13:37:15 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/531365d0af/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-19 13:37:15 [backends.py:559] Dynamo bytecode transform time: 3.29 s\n",
      "INFO 01-19 13:37:17 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.996 s\n",
      "INFO 01-19 13:37:17 [monitor.py:34] torch.compile takes 3.29 s in total\n",
      "INFO 01-19 13:37:18 [gpu_worker.py:298] Available KV cache memory: 18.10 GiB\n",
      "INFO 01-19 13:37:18 [kv_cache_utils.py:1087] GPU KV cache size: 1,581,824 tokens\n",
      "INFO 01-19 13:37:18 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 772.38x\n",
      "INFO 01-19 13:37:18 [vllm_utils.py:698] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 01-19 13:37:18 [gpu_model_runner.py:3643] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.70it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:01<00:00, 26.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 13:37:22 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.71 GiB\n",
      "INFO 01-19 13:37:22 [vllm_utils.py:705] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 13:37:23 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.11 seconds\n",
      "INFO 01-19 13:37:23 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at /root/.cache/modelscope/hub/models/unsloth/Qwen2___5-0___5B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Just some info: will skip parsing ['q_norm', 'attention_norm', 'ffn_norm', 'post_layernorm', 'norm2', 'input_layernorm', 'post_attention_layernorm', 'k_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'layer_norm1', 'norm1', 'layer_norm2']\n",
      "Unsloth: Just some info: will skip parsing ['cross_attn_post_attention_layernorm', 'q_norm', 'attention_norm', 'cross_attn_input_layernorm', 'ffn_norm', 'post_layernorm', 'norm2', 'input_layernorm', 'post_attention_layernorm', 'k_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'layer_norm1', 'norm1', 'layer_norm2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.5 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied qwen-2.5 chat template to match SFT training\n",
      "è®¾ç½® bos_token_id: 151644\n",
      "Tokenizer padding_side: left\n",
      "Tokenizer pad_token_id: 151654\n",
      "Tokenizer bos_token_id: 151644\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"model\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"model\"\n",
    "\n",
    "# BASE_MODEL_DIR = \"model/models/qwen3-4B-SFT\"\n",
    "BASE_MODEL_DIR = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# BASE_MODEL_DIR = \"rd211/Qwen3-0.6B-Instruct\"\n",
    "CHECKPOINT_DIR = \"checkpoints/grpo_tsc_two_scenarios_latest\"\n",
    "\n",
    "\n",
    "def _looks_like_checkpoint(path: str) -> bool:\n",
    "    if not os.path.isdir(path):\n",
    "        return False\n",
    "    marker_files = [\n",
    "        \"adapter_config.json\",\n",
    "        \"adapter_model.safetensors\",\n",
    "        \"adapter_model.bin\",\n",
    "        \"config.json\",\n",
    "    ]\n",
    "    return any(os.path.isfile(os.path.join(path, f)) for f in marker_files)\n",
    "\n",
    "\n",
    "\n",
    "SFT_CHECKPOINT_DIR = \"checkpoints/sft_tsc_synthetic\"\n",
    "\n",
    "resume_from = CHECKPOINT_DIR if _looks_like_checkpoint(CHECKPOINT_DIR) else BASE_MODEL_DIR\n",
    "\n",
    "if _looks_like_checkpoint(CHECKPOINT_DIR):\n",
    "    print(f\"âœ“ ä» checkpoint ç»§ç»­è®­ç»ƒ: {CHECKPOINT_DIR}\")\n",
    "    resume_from = CHECKPOINT_DIR\n",
    "elif _looks_like_checkpoint(SFT_CHECKPOINT_DIR):\n",
    "    print(f\"âœ“ ä» SFT æ¨¡å‹å¼€å§‹è®­ç»ƒ: {SFT_CHECKPOINT_DIR}\")\n",
    "    resume_from = SFT_CHECKPOINT_DIR\n",
    "else:\n",
    "    print(f\"â„¹ ä»åŸºç¡€æ¨¡å‹å¼€å§‹: {BASE_MODEL_DIR}\")\n",
    "    resume_from = BASE_MODEL_DIR\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=resume_from,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "if resume_from == BASE_MODEL_DIR:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=lora_rank * 2,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    _trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(_trainable) == 0:\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"lora\" in name.lower():\n",
    "                p.requires_grad = True\n",
    "        print(\"âš ï¸ checkpoint æœªæ£€æµ‹åˆ°å¯è®­ç»ƒå‚æ•°ï¼Œå·²å¼ºåˆ¶å¯ç”¨ LoRA å‚æ•°è®­ç»ƒ\")\n",
    "\n",
    "# Fix for GRPO generation: Must use left padding (applies to ALL cases)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Apply the same chat template as SFT training\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",  # Must match SFT training template\n",
    ")\n",
    "print(\"âœ“ Applied qwen-2.5 chat template to match SFT training\")\n",
    "\n",
    "# Ensure BOS token is preserved - this is critical for generation\n",
    "if tokenizer.bos_token_id is None:\n",
    "    # For Qwen, <|im_start|> acts as BOS in chat context\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "    print(f\"è®¾ç½® bos_token_id: {tokenizer.bos_token_id}\")\n",
    "\n",
    "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")\n",
    "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"Tokenizer bos_token_id: {tokenizer.bos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0673f7d",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43de6b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset åŠ è½½æˆåŠŸ: grpo_dataset_two_scenarios\n",
      "æ ·æœ¬æ•°: 2000\n",
      "ç¤ºä¾‹ Prompt (message list):\n",
      "[{'content': 'You are a traffic signal control expert. Output only valid JSON without any explanation.', 'role': 'system'}, {'content': 'ä½ æ˜¯äº¤é€šä¿¡å·æ§åˆ¶ä¼˜åŒ–ä¸“å®¶ã€‚ã€signal_step_input_jsonã€‘{\"crossing_id\":4154746361,\"as_of\":\"2026-01-19 10:50:30\",\"scenario\":{\"phase_ids\":[1,3],\"phase_lane_map\":{\"1\":[\"-281244893#0_0\",\"-351972805#1_1\",\"351972805#0_1\",\"351972805#0_0\",\"-351972805#1_0\"],\"3\":[\"-281244893#0_0\",\"-351972805#1_0\"]}},\"state\":{\"current_phase_id\":1,\"current_phase_elapsed_sec\":18,\"current_phase_planned_green_sec\":28,\"phase_metrics_now\":[{\"phase_id\":1,\"avg_queue_veh\":0.6,\"avg_passed_veh_in_current_green\":4.0},{\"phase_id\":3,\"avg_queue_veh\":1.5,\"avg_passed_veh_in_current_green\":0.0}]}}ã€/signal_step_input_jsonã€‘\\n\\nå­—æ®µå«ä¹‰ï¼ˆä»…è¯´æ˜å«ä¹‰ï¼‰ï¼š\\n* state.phase_metrics_now[*].avg_queue_vehï¼šè¯¥ç›¸ä½åœ¨å½“å‰æ—¶åˆ»çš„å¹³å‡æ’é˜Ÿè½¦è¾†æ•°ï¼ˆè¾†ï¼‰ï¼›å¯ç”±è¯¥ç›¸ä½æ‰€æ§åˆ¶è½¦é“çš„æ’é˜Ÿè½¦è¾†æ•°å–å¹³å‡å¾—åˆ°ã€‚\\n* state.phase_metrics_now[*].avg_passed_veh_in_current_greenï¼šè¯¥ç›¸ä½åœ¨\"å½“å‰æ­£åœ¨æ‰§è¡Œçš„ç»¿ç¯ç›¸ä½\"å†…è‡³å½“å‰æ—¶åˆ»ç´¯è®¡é€šè¿‡è½¦è¾†æ•°ï¼ˆè¾†ï¼‰ï¼›åªæœ‰å½“å‰ç›¸ä½é€šå¸¸ä¼š >0ï¼Œéå½“å‰ç›¸ä½ä¸€èˆ¬ä¸º 0ã€‚\\n\\nä»»åŠ¡ï¼ˆå¿…é¡»å®Œæˆï¼‰ï¼š\\n1. åŸºäºè¾“å…¥ JSON çš„ scenario ä¸ stateï¼Œè‡ªè¡Œå†³å®šå†³ç­–ç­–ç•¥/å‚æ•°ï¼Œè¯„ä¼°å„ç›¸ä½å½“å‰éœ€æ±‚å¼ºåº¦å¹¶åšå‡º\"ä¸‹ä¸€æ­¥åŠ¨ä½œ\"ï¼ˆä»…åœ¨å†…éƒ¨ä½¿ç”¨ï¼Œä¸è¾“å‡ºä»»ä½•é¢„æµ‹è¿‡ç¨‹/ä¸­é—´å€¼ï¼‰ã€‚\\n2. è¾“å‡ºï¼šä¸‹ä¸€ä¸ªä¿¡å·ç›¸ä½ next_phase_idï¼Œä»¥åŠè¯¥ç›¸ä½ç»¿ç¯æŒç»­æ—¶é—´ green_secï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚\\n\\nè¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š\\n1. ä½ å¿…é¡»æ˜¾å¼åˆ©ç”¨ avg_queue_veh ä¸ passed_veh_in_current_green æ¥å†³ç­–ï¼›ä¸å¾—æ— è§†è¾“å…¥éšæ„ç»™å‡ºç­”æ¡ˆã€‚\\n2. åœ¨ç¼ºå°‘ä»»ä½•é¢å¤–ç¡¬çº¦æŸï¼ˆä¸æä¾› cycle_constraints / phase_order / phase_limits / historyï¼‰çš„æƒ…å†µä¸‹ï¼š\\n   * next_phase_id å¿…é¡»æ¥è‡ª scenario.phase_idsï¼›\\n   * green_sec å¿…é¡»ä¸ºæ­£æ•´æ•°ç§’ï¼Œä¸”å¿…é¡»åœ¨ [1, 120] èŒƒå›´å†…ï¼›\\n   * green_sec å¿…é¡»\"åˆç†\"ï¼šé˜Ÿåˆ—æ›´å¤§/é€šè¿‡æ›´å°‘çš„ç›¸ä½å€¾å‘ç»™æ›´é•¿ç»¿ï¼›é˜Ÿåˆ—æ›´å°/é€šè¿‡æ›´å¤šçš„ç›¸ä½å€¾å‘ç»™æ›´çŸ­ç»¿ï¼›\\n   * è‹¥å¤šç›¸ä½éœ€æ±‚æ¥è¿‘ï¼Œå¯ä¼˜å…ˆåˆ‡æ¢åˆ°éå½“å‰ç›¸ä½ä»¥é™ä½å…¶ä»–ç›¸ä½ç­‰å¾…çš„ç´¯ç§¯é£é™©ã€‚\\n\\nè¾“å‡ºè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\\n1. åªè¾“å‡ºæœ€ç»ˆ JSONï¼ˆä¸è¦ä»»ä½•è¯´æ˜ã€ä¸è¦ Markdown ä»£ç å—ã€ä¸è¦é¢å¤–æ–‡æœ¬ã€ä¸è¦å¤è¿°è§„åˆ™ã€ä¸è¦è¾“å‡ºæ¨ç†è¿‡ç¨‹ï¼‰ã€‚\\n2. JSON å¿…é¡»ä¸ºå¯¹è±¡ï¼Œä¸”ä»…åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š\\n   {\"next_phase_id\": <int>, \"green_sec\": <int>}\\n3. green_sec å¿…é¡»åœ¨ [1, 120] èŒƒå›´å†…ã€‚\\n4. ä¸å…è®¸è¾“å‡ºå…¶å®ƒå­—æ®µï¼Œä¸å…è®¸æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡æœ¬ã€‚', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "DATASET_PATH = \"grpo_dataset_two_scenarios\"\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset ä¸å­˜åœ¨: {DATASET_PATH}\\n\"\n",
    "        \"è¯·å…ˆè¿è¡Œ generate_grpo_dataset.py ç”Ÿæˆç¦»çº¿ dataset\"\n",
    "    )\n",
    "\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "print(f\"âœ“ Dataset åŠ è½½æˆåŠŸ: {DATASET_PATH}\")\n",
    "print(f\"æ ·æœ¬æ•°: {len(dataset)}\")\n",
    "\n",
    "# NOTE: Do NOT apply chat template here!\n",
    "# GRPOTrainer expects prompt to be a list of messages, not a formatted string.\n",
    "# It will apply the chat template internally using processing_class (tokenizer).\n",
    "print(\"ç¤ºä¾‹ Prompt (message list):\")\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c127c",
   "metadata": {},
   "source": [
    "## 4. å¯¼å…¥ Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08728fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Reward function åŠ è½½æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "from tsc_reward_function import tsc_reward_fn, cleanup_global_pool\n",
    "\n",
    "print(\"âœ“ Reward function åŠ è½½æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f371e",
   "metadata": {},
   "source": [
    "## 5. é…ç½® GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe254525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n",
      "âœ“ GRPOConfig é…ç½®å®Œæˆ (æ¨¡å¼: æ­£å¼è®­ç»ƒ)\n",
      "  - max_steps: -1 (å…¨epoch)\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥ä½¿ç”¨çŸ­è®­ç»ƒ\n",
    "# QUICK_VERIFY = True\n",
    "QUICK_VERIFY = False  # æ­£å¼è®­ç»ƒè®¾ä¸º False\n",
    "\n",
    "config = GRPOConfig(\n",
    "    output_dir=\"checkpoints/grpo_tsc_two_scenarios\",\n",
    "\n",
    "    # æ‰¹æ¬¡é…ç½®\n",
    "    per_device_train_batch_size=1,\n",
    "    num_generations=4,  # Keep at 4 for proper GRPO\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # ç”Ÿæˆé…ç½®\n",
    "    max_completion_length=128,  \n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    use_vllm=False,  # DGX Spark ä¸Š vLLM LoRA å…¼å®¹æ€§é—®é¢˜ï¼Œå·²ç¦ç”¨\n",
    "\n",
    "    # è®­ç»ƒé…ç½®\n",
    "    learning_rate=2e-6,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=20 if QUICK_VERIFY else -1,  # å¿«é€ŸéªŒè¯ï¼š20æ­¥ï¼›æ­£å¼è®­ç»ƒï¼šå…¨epoch\n",
    "\n",
    "    # GRPO ç‰¹å®š\n",
    "    scale_rewards=True,\n",
    "\n",
    "    # æ—¥å¿—ä¸ä¿å­˜\n",
    "    logging_steps=5,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # ä¼˜åŒ–å™¨\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "\n",
    "    # å…¶ä»–\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ GRPOConfig é…ç½®å®Œæˆ (æ¨¡å¼: {'å¿«é€ŸéªŒè¯' if QUICK_VERIFY else 'æ­£å¼è®­ç»ƒ'})\")\n",
    "if QUICK_VERIFY:\n",
    "    print(f\"  - max_steps: {config.max_steps} (éªŒè¯æ¨¡å¼)\")\n",
    "else:\n",
    "    print(f\"  - max_steps: {config.max_steps} (å…¨epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5b7e",
   "metadata": {},
   "source": [
    "## 6. åˆ›å»º GRPOTrainer å¹¶å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc3c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 151644, 'pad_token_id': 151654}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GRPOTrainer åˆ›å»ºæˆåŠŸ\n",
      "è®­ç»ƒæ ·æœ¬æ•°: 2000\n",
      "æ¯ epoch steps: 2000\n",
      "\n",
      "============================================================\n",
      "å¼€å§‹ GRPO è®­ç»ƒ\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 17,596,416 of 511,629,184 (3.44% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-19 13:38:47 [processor.py:215] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n",
      "\n",
      "======================================================================\n",
      "[DEBUG] Completion #1\n",
      "======================================================================\n",
      "åŸå§‹è¾“å‡º:\n",
      "{\"next_phase_id\": 3, \"green_sec\": 24}\n",
      "======================================================================\n",
      "\n",
      "[SimulatorPool] åˆ›å»ºæ–° simulator: chengdu\n",
      "DEBUG: ä½¿ç”¨çš„ SUMO è·¯å¾„æ˜¯: /usr/share/sumo/bin/sumo\n",
      "Starting SUMO with command: /usr/share/sumo/bin/sumo -c /root/SCU_TSC/sumo_simulation/environments/chengdu/chengdu.sumocfg --step-length 1.0 --no-warnings true --start --device.rerouting.probability 0\n",
      " Retrying in 1 seconds\n",
      "Successfully connected to SUMO\n",
      "Starting warmup phase...\n",
      "Warmup progress: 0/300\n",
      "Warmup progress: 100/300\n",
      "Warmup progress: 200/300\n",
      "Warmup completed. Starting real-time simulation.\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:17:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / tsc_reward_fn / mean</th>\n",
       "      <th>rewards / tsc_reward_fn / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>-1.242247</td>\n",
       "      <td>0.304247</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.837450</td>\n",
       "      <td>-1.242247</td>\n",
       "      <td>0.976971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>-1.317905</td>\n",
       "      <td>0.121985</td>\n",
       "      <td>17.837500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.837500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.020939</td>\n",
       "      <td>-1.317905</td>\n",
       "      <td>0.820363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-1.110767</td>\n",
       "      <td>0.334231</td>\n",
       "      <td>17.487500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.487500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.686810</td>\n",
       "      <td>-1.110767</td>\n",
       "      <td>1.021396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-1.269077</td>\n",
       "      <td>0.481846</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.672973</td>\n",
       "      <td>-1.269077</td>\n",
       "      <td>1.552282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>-1.492206</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.911977</td>\n",
       "      <td>-1.492206</td>\n",
       "      <td>1.263346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.167238</td>\n",
       "      <td>0.043937</td>\n",
       "      <td>17.987500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.987500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.983629</td>\n",
       "      <td>-1.167238</td>\n",
       "      <td>0.810258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>-1.546386</td>\n",
       "      <td>0.151284</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.031429</td>\n",
       "      <td>-1.546386</td>\n",
       "      <td>0.771252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-2.028966</td>\n",
       "      <td>0.936223</td>\n",
       "      <td>17.312500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.312500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.561493</td>\n",
       "      <td>-2.028966</td>\n",
       "      <td>2.632656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>-1.982039</td>\n",
       "      <td>0.639717</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.914928</td>\n",
       "      <td>-1.982039</td>\n",
       "      <td>1.484519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-1.228838</td>\n",
       "      <td>0.201652</td>\n",
       "      <td>17.512500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.512500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.675461</td>\n",
       "      <td>-1.228838</td>\n",
       "      <td>1.273919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>-2.342169</td>\n",
       "      <td>0.570109</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.775172</td>\n",
       "      <td>-2.342169</td>\n",
       "      <td>2.163387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.470962</td>\n",
       "      <td>0.197565</td>\n",
       "      <td>17.937500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.937500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.967323</td>\n",
       "      <td>-1.470962</td>\n",
       "      <td>0.672481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>-1.521126</td>\n",
       "      <td>0.623093</td>\n",
       "      <td>17.412500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.412500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.628847</td>\n",
       "      <td>-1.521126</td>\n",
       "      <td>2.052095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-2.138780</td>\n",
       "      <td>0.777843</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.805891</td>\n",
       "      <td>-2.138780</td>\n",
       "      <td>2.184484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.750062</td>\n",
       "      <td>0.483663</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.979049</td>\n",
       "      <td>-1.750062</td>\n",
       "      <td>1.508436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.471278</td>\n",
       "      <td>0.220086</td>\n",
       "      <td>17.675000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.675000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.984423</td>\n",
       "      <td>-1.471278</td>\n",
       "      <td>1.395379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-1.636321</td>\n",
       "      <td>0.383489</td>\n",
       "      <td>17.862500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.862500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.240437</td>\n",
       "      <td>-1.636321</td>\n",
       "      <td>1.064296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-1.483325</td>\n",
       "      <td>0.268338</td>\n",
       "      <td>17.775000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.775000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.240549</td>\n",
       "      <td>-1.483325</td>\n",
       "      <td>1.065606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>-1.524629</td>\n",
       "      <td>0.219750</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.210232</td>\n",
       "      <td>-1.524629</td>\n",
       "      <td>0.925038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>-1.795894</td>\n",
       "      <td>0.174035</td>\n",
       "      <td>17.787500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.787500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.138048</td>\n",
       "      <td>-1.795894</td>\n",
       "      <td>1.577629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>-1.398365</td>\n",
       "      <td>0.135675</td>\n",
       "      <td>18.012500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.012500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.230383</td>\n",
       "      <td>-1.398365</td>\n",
       "      <td>0.817257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-1.187190</td>\n",
       "      <td>0.198108</td>\n",
       "      <td>17.675000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.675000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.057351</td>\n",
       "      <td>-1.187190</td>\n",
       "      <td>1.038938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-1.683245</td>\n",
       "      <td>0.306882</td>\n",
       "      <td>17.712500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.712500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.000283</td>\n",
       "      <td>-1.683245</td>\n",
       "      <td>1.290559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>-2.010892</td>\n",
       "      <td>0.645213</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.206866</td>\n",
       "      <td>-2.010892</td>\n",
       "      <td>2.130264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-1.270803</td>\n",
       "      <td>0.428997</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.248170</td>\n",
       "      <td>-1.270803</td>\n",
       "      <td>1.110093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>-1.732749</td>\n",
       "      <td>0.354994</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.146077</td>\n",
       "      <td>-1.732749</td>\n",
       "      <td>1.156306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-1.946955</td>\n",
       "      <td>0.325953</td>\n",
       "      <td>17.887500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.887500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.061883</td>\n",
       "      <td>-1.946955</td>\n",
       "      <td>1.235197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-1.666203</td>\n",
       "      <td>0.424582</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.050747</td>\n",
       "      <td>-1.666203</td>\n",
       "      <td>1.325976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-1.899693</td>\n",
       "      <td>0.678174</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.992569</td>\n",
       "      <td>-1.899693</td>\n",
       "      <td>2.342403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>-0.924242</td>\n",
       "      <td>0.390432</td>\n",
       "      <td>17.462500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.462500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.870492</td>\n",
       "      <td>-0.924242</td>\n",
       "      <td>1.137765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-1.667478</td>\n",
       "      <td>0.284233</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.988842</td>\n",
       "      <td>-1.667478</td>\n",
       "      <td>1.648188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>-1.561888</td>\n",
       "      <td>0.311245</td>\n",
       "      <td>17.462500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.462500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.853967</td>\n",
       "      <td>-1.561888</td>\n",
       "      <td>1.907760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>-1.432388</td>\n",
       "      <td>0.095509</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.036200</td>\n",
       "      <td>-1.432388</td>\n",
       "      <td>0.854449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-1.996160</td>\n",
       "      <td>0.646703</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.002522</td>\n",
       "      <td>-1.996160</td>\n",
       "      <td>1.896075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-1.689829</td>\n",
       "      <td>0.204869</td>\n",
       "      <td>18.037500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.037500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.127947</td>\n",
       "      <td>-1.689829</td>\n",
       "      <td>0.643798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>-2.345922</td>\n",
       "      <td>0.721506</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.440616</td>\n",
       "      <td>-2.345922</td>\n",
       "      <td>3.050868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>-1.954137</td>\n",
       "      <td>0.816533</td>\n",
       "      <td>17.862500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.862500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.535847</td>\n",
       "      <td>-1.954137</td>\n",
       "      <td>1.787628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>-1.152548</td>\n",
       "      <td>0.342805</td>\n",
       "      <td>17.525000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.525000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.769062</td>\n",
       "      <td>-1.152548</td>\n",
       "      <td>1.268657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>-1.733754</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>18.137500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.137500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.831262</td>\n",
       "      <td>-1.733754</td>\n",
       "      <td>0.551244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>-1.698954</td>\n",
       "      <td>0.293160</td>\n",
       "      <td>17.712500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.712500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>2.361152</td>\n",
       "      <td>-1.698954</td>\n",
       "      <td>1.638325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>-1.342313</td>\n",
       "      <td>0.306772</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.143163</td>\n",
       "      <td>-1.342313</td>\n",
       "      <td>1.301170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>-1.086606</td>\n",
       "      <td>0.193847</td>\n",
       "      <td>17.675000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.675000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.868678</td>\n",
       "      <td>-1.086606</td>\n",
       "      <td>0.994193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>-1.439224</td>\n",
       "      <td>0.622086</td>\n",
       "      <td>17.637500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.637500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.246343</td>\n",
       "      <td>-1.439224</td>\n",
       "      <td>1.473807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>-2.014612</td>\n",
       "      <td>0.531206</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>2.075806</td>\n",
       "      <td>-2.014612</td>\n",
       "      <td>1.249650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>-1.147095</td>\n",
       "      <td>0.047753</td>\n",
       "      <td>17.925000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.925000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.832344</td>\n",
       "      <td>-1.147095</td>\n",
       "      <td>0.826410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>-2.024478</td>\n",
       "      <td>0.498311</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>16.333108</td>\n",
       "      <td>-2.024478</td>\n",
       "      <td>2.068262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>-2.169629</td>\n",
       "      <td>0.808061</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>6.241050</td>\n",
       "      <td>-2.169629</td>\n",
       "      <td>2.708580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>-1.493682</td>\n",
       "      <td>0.534672</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>5.527152</td>\n",
       "      <td>-1.493682</td>\n",
       "      <td>1.770236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>-1.623606</td>\n",
       "      <td>0.188926</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>2.620942</td>\n",
       "      <td>-1.623606</td>\n",
       "      <td>1.373530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>-1.074606</td>\n",
       "      <td>0.417404</td>\n",
       "      <td>17.512500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.512500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>4.596846</td>\n",
       "      <td>-1.074606</td>\n",
       "      <td>0.999052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>-1.339212</td>\n",
       "      <td>0.201121</td>\n",
       "      <td>17.787500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.787500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.637759</td>\n",
       "      <td>-1.339212</td>\n",
       "      <td>1.026392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>-1.496735</td>\n",
       "      <td>0.175522</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.690796</td>\n",
       "      <td>-1.496735</td>\n",
       "      <td>0.895214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>-1.564976</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>8.289404</td>\n",
       "      <td>-1.564976</td>\n",
       "      <td>1.377924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>-1.601722</td>\n",
       "      <td>0.244952</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>9.821011</td>\n",
       "      <td>-1.601722</td>\n",
       "      <td>1.281254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>-1.190405</td>\n",
       "      <td>0.326637</td>\n",
       "      <td>17.362500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.362500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>7.328795</td>\n",
       "      <td>-1.190405</td>\n",
       "      <td>1.022068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>-1.101324</td>\n",
       "      <td>0.571308</td>\n",
       "      <td>17.612500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.612500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>2.297170</td>\n",
       "      <td>-1.101324</td>\n",
       "      <td>1.482154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>-1.041232</td>\n",
       "      <td>0.159001</td>\n",
       "      <td>17.425000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.425000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650013</td>\n",
       "      <td>-1.041232</td>\n",
       "      <td>1.194288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>-1.314586</td>\n",
       "      <td>0.611672</td>\n",
       "      <td>17.487500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.487500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.450649</td>\n",
       "      <td>-1.314586</td>\n",
       "      <td>1.976531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-1.419523</td>\n",
       "      <td>0.273524</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.130801</td>\n",
       "      <td>-1.419523</td>\n",
       "      <td>1.238986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.022001</td>\n",
       "      <td>0.092320</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.975811</td>\n",
       "      <td>-1.022001</td>\n",
       "      <td>0.994407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>-1.900936</td>\n",
       "      <td>0.227034</td>\n",
       "      <td>17.837500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.837500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.191716</td>\n",
       "      <td>-1.900936</td>\n",
       "      <td>1.329787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>-1.339267</td>\n",
       "      <td>0.455135</td>\n",
       "      <td>17.612500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.612500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.962273</td>\n",
       "      <td>-1.339267</td>\n",
       "      <td>1.106697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-1.868966</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.118145</td>\n",
       "      <td>-1.868966</td>\n",
       "      <td>1.390341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.456143</td>\n",
       "      <td>0.389332</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.968233</td>\n",
       "      <td>-1.456143</td>\n",
       "      <td>0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>-1.371049</td>\n",
       "      <td>0.552335</td>\n",
       "      <td>17.762500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.762500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.928684</td>\n",
       "      <td>-1.371049</td>\n",
       "      <td>1.646852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-1.529179</td>\n",
       "      <td>0.211642</td>\n",
       "      <td>17.762500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.762500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.963556</td>\n",
       "      <td>-1.529179</td>\n",
       "      <td>1.391861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>-1.380683</td>\n",
       "      <td>0.427736</td>\n",
       "      <td>17.712500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.712500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.279062</td>\n",
       "      <td>-1.380683</td>\n",
       "      <td>1.214053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-1.445715</td>\n",
       "      <td>0.266737</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.055688</td>\n",
       "      <td>-1.445715</td>\n",
       "      <td>0.959960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>-2.396145</td>\n",
       "      <td>0.892305</td>\n",
       "      <td>17.962500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.962500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.300279</td>\n",
       "      <td>-2.396145</td>\n",
       "      <td>2.486158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-1.366677</td>\n",
       "      <td>0.082551</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.037727</td>\n",
       "      <td>-1.366677</td>\n",
       "      <td>0.834208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>-1.020367</td>\n",
       "      <td>0.391019</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.092619</td>\n",
       "      <td>-1.020367</td>\n",
       "      <td>1.408023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-1.188406</td>\n",
       "      <td>0.277576</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.055909</td>\n",
       "      <td>-1.188406</td>\n",
       "      <td>1.052253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>-1.935710</td>\n",
       "      <td>0.391188</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.296065</td>\n",
       "      <td>-1.935710</td>\n",
       "      <td>1.586836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-2.281493</td>\n",
       "      <td>1.487528</td>\n",
       "      <td>17.562500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.562500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.125990</td>\n",
       "      <td>-2.281493</td>\n",
       "      <td>3.478593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-1.395885</td>\n",
       "      <td>0.081275</td>\n",
       "      <td>17.912500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.912500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.124370</td>\n",
       "      <td>-1.395885</td>\n",
       "      <td>1.033376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>-1.281193</td>\n",
       "      <td>0.659850</td>\n",
       "      <td>17.612500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.612500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.159760</td>\n",
       "      <td>-1.281193</td>\n",
       "      <td>1.443968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>-1.890944</td>\n",
       "      <td>0.726651</td>\n",
       "      <td>17.937500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.937500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.157507</td>\n",
       "      <td>-1.890944</td>\n",
       "      <td>1.410525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>-1.525757</td>\n",
       "      <td>0.135618</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.183063</td>\n",
       "      <td>-1.525757</td>\n",
       "      <td>0.742250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>-1.524417</td>\n",
       "      <td>0.585658</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.627262</td>\n",
       "      <td>-1.524417</td>\n",
       "      <td>1.565869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>-1.426073</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>17.887500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.887500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.397316</td>\n",
       "      <td>-1.426073</td>\n",
       "      <td>1.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>-1.415430</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>17.912500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.912500</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.846726</td>\n",
       "      <td>-1.415430</td>\n",
       "      <td>0.786165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-1.339876</td>\n",
       "      <td>0.339369</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.256182</td>\n",
       "      <td>-1.339876</td>\n",
       "      <td>1.070456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>-1.918041</td>\n",
       "      <td>0.464733</td>\n",
       "      <td>18.037500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.037500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.198819</td>\n",
       "      <td>-1.918041</td>\n",
       "      <td>1.781068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>-1.513566</td>\n",
       "      <td>0.342214</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.679614</td>\n",
       "      <td>-1.513566</td>\n",
       "      <td>1.321682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>-1.462263</td>\n",
       "      <td>0.517316</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.737500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.363666</td>\n",
       "      <td>-1.462263</td>\n",
       "      <td>1.438723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>-1.060477</td>\n",
       "      <td>0.402707</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>13.274668</td>\n",
       "      <td>-1.060477</td>\n",
       "      <td>0.892214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>-1.605474</td>\n",
       "      <td>0.592074</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.662500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.067639</td>\n",
       "      <td>-1.605474</td>\n",
       "      <td>1.551406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>-2.376194</td>\n",
       "      <td>0.987444</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.472829</td>\n",
       "      <td>-2.376194</td>\n",
       "      <td>2.668648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>-1.280322</td>\n",
       "      <td>0.108163</td>\n",
       "      <td>17.962500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.962500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.815358</td>\n",
       "      <td>-1.280322</td>\n",
       "      <td>0.966950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>-1.520961</td>\n",
       "      <td>0.239198</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.170234</td>\n",
       "      <td>-1.520961</td>\n",
       "      <td>1.291269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>-1.972110</td>\n",
       "      <td>0.476644</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.151132</td>\n",
       "      <td>-1.972110</td>\n",
       "      <td>2.156506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>-1.380131</td>\n",
       "      <td>0.305814</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.158438</td>\n",
       "      <td>-1.380131</td>\n",
       "      <td>0.913313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>-1.879265</td>\n",
       "      <td>0.208419</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.660270</td>\n",
       "      <td>-1.879265</td>\n",
       "      <td>1.525137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>-2.003479</td>\n",
       "      <td>0.328179</td>\n",
       "      <td>17.862500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.862500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.776778</td>\n",
       "      <td>-2.003479</td>\n",
       "      <td>1.503115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-1.381077</td>\n",
       "      <td>0.271465</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.825000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.246927</td>\n",
       "      <td>-1.381077</td>\n",
       "      <td>0.824474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>-1.445959</td>\n",
       "      <td>0.413978</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.230450</td>\n",
       "      <td>-1.445959</td>\n",
       "      <td>1.300498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>-1.441469</td>\n",
       "      <td>0.630147</td>\n",
       "      <td>17.637500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.637500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>3.865132</td>\n",
       "      <td>-1.441469</td>\n",
       "      <td>1.476191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>-1.899254</td>\n",
       "      <td>0.525392</td>\n",
       "      <td>17.787500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.787500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>9.976022</td>\n",
       "      <td>-1.899254</td>\n",
       "      <td>1.938826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>-1.537800</td>\n",
       "      <td>0.215449</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.229536</td>\n",
       "      <td>-1.537800</td>\n",
       "      <td>0.791260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>-1.832478</td>\n",
       "      <td>0.546566</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.812500</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.430091</td>\n",
       "      <td>-1.832478</td>\n",
       "      <td>1.653126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[DEBUG] Completion #2\n",
      "======================================================================\n",
      "åŸå§‹è¾“å‡º:\n",
      "{\"next_phase_id\": 3, \"green_sec\": 36}\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[DEBUG] Completion #3\n",
      "======================================================================\n",
      "åŸå§‹è¾“å‡º:\n",
      "{\"next_phase_id\": 3, \"green_sec\": 11}\n",
      "======================================================================\n",
      "\n",
      "Step #1633.00 (17ms ~= 11.11*RT, ~263111.11UPS, TraCI: 0ms, vehicles TOT 4072 ACT 2368 BUFFp #1287.00 (12ms ~= 83.33*RT, ~237250.00UPS, TraCI: 1ms, vehicles TOT 4973 ACT 2847 BUFp #1528.00 (14ms ~= 71.43*RT, ~261357.14UPS, TraCI: 0ms, vehicles TOT 6362 ACT 3659 BUF #1028.00 (10ms ~= 100.00*RT, ~220900.00UPS, TraCI: 0ms, vehicles TOT 3672 ACT 2209 BU#1103.00 (9ms ~= 111.11*RT, ~271000.00UPS, TraCI: 1ms, vehicles TOT 4091 ACT 2439 BUF1206.00 (13ms ~= 76.92*RT, ~207769.23UPS, TraCI: 0ms, vehicles TOT 4538 ACT 2701 BUF382.00 (16ms ~= 62.50*RT, ~201312.50UPS, TraCI: 0ms, vehicles TOT 5559 ACT 3221 BUF1.00 (4ms ~= 250.00*RT, ~193500.00UPS, TraCI: 0ms, vehicles TOT 1018 ACT 774 BUF 81.00 (11ms ~= 90.91*RT, ~232000.00UPS, TraCI: 0ms, vehicles TOT 4325 ACT 2552 BUF00 (6ms ~= 166.67*RT, ~148666.67UPS, TraCI: 1ms, vehicles TOT 1240 ACT 892 BUF 80 (6ms ~= 166.67*RT, ~249333.33UPS, TraCI: 0ms, vehicles TOT 2328 ACT 1496 BUF 0 (17ms ~= 58.82*RT, ~215294.12UPS, TraCI: 0ms, vehicles TOT 6558 ACT 3660 BUF (14ms ~= 71.43*RT, ~209428.57UPS, TraCI: 0ms, vehicles TOT 5124 ACT 2932 BUF3ms ~= 333.33*RT, ~321000.00UPS, TraCI: 0ms, vehicles TOT 1360 ACT 963 BUF 917ms ~= 58.82*RT, ~217117.65UPS, TraCI: 0ms, vehicles TOT 6477 ACT 3691 BUFs ~= 250.00*RT, ~220250.00UPS, TraCI: 0ms, vehicles TOT 1212 ACT 881 BUF 9 ~= 142.86*RT, ~266000.00UPS, TraCI: 0ms, vehicles TOT 2995 ACT 1862 BUF ~= 142.86*RT, ~243571.43UPS, TraCI: 1ms, vehicles TOT 2711 ACT 1705 BUF ~= 166.67*RT, ~196666.67UPS, TraCI: 0ms, vehicles TOT 1798 ACT 1180 BUF  ~= 90.91*RT, ~253363.64UPS, TraCI: 1ms, vehicles TOT 4772 ACT 2787 BUF125.00*RT, ~206250.00UPS, TraCI: 0ms, vehicles TOT 2617 ACT 1650 BUF 33.33*RT, ~358666.67UPS, TraCI: 1ms, vehicles TOT 1608 ACT 1076 BUF \n",
      "============================================================\n",
      "è®­ç»ƒå®Œæˆ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=dataset,\n",
    "    reward_funcs=tsc_reward_fn,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ“ GRPOTrainer åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(dataset)}\")\n",
    "print(f\"æ¯ epoch steps: {len(trainer.get_train_dataloader())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"å¼€å§‹ GRPO è®­ç»ƒ\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"è®­ç»ƒå®Œæˆ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222355eb",
   "metadata": {},
   "source": [
    "## 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad711919",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = \"checkpoints/grpo_tsc_two_scenarios_final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c1138",
   "metadata": {},
   "source": [
    "## 8. æ¸…ç†èµ„æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_global_pool()\n",
    "print(\"âœ“ Simulator æ± å·²æ¸…ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7aa60b",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•æ¨ç†ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4886c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_sample = dataset[0]\n",
    "test_prompt = test_sample[\"prompt\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"æµ‹è¯•ç”Ÿæˆ:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
