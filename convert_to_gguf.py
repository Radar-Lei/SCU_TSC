#!/usr/bin/env python3
"""
å°† Unsloth LoRA checkpoint è½¬æ¢ä¸º GGUF æ ¼å¼ã€‚

ç”¨æ³•:
    python convert_to_gguf.py [--checkpoint CHECKPOINT_PATH] [--output OUTPUT_DIR] [--quantization METHOD]

ç¤ºä¾‹:
    python convert_to_gguf.py
    python convert_to_gguf.py --checkpoint checkpoints/grpo_tsc_two_scenarios/checkpoint-4000
    python convert_to_gguf.py --quantization q4_k_m
"""

import os
import sys
import argparse

# ç¡®ä¿ç¯å¢ƒå˜é‡åœ¨å¯¼å…¥å‰è®¾ç½®
os.environ["UNSLOTH_USE_MODELSCOPE"] = "1"
os.environ["MODELSCOPE_CACHE"] = "model"
os.environ["HF_HOME"] = "model"


def main():
    parser = argparse.ArgumentParser(description="å°† Unsloth LoRA checkpoint è½¬æ¢ä¸º GGUF æ ¼å¼")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="checkpoints/grpo_tsc_two_scenarios/checkpoint-4000",
        help="è¦è½¬æ¢çš„ checkpoint è·¯å¾„ (é»˜è®¤: checkpoints/grpo_tsc_two_scenarios/checkpoint-4000)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="GGUF è¾“å‡ºç›®å½• (é»˜è®¤: {checkpoint}_gguf)",
    )
    parser.add_argument(
        "--quantization",
        type=str,
        default="f16",
        choices=["f16", "q4_k_m", "q5_k_m", "q8_0", "q4_0", "q5_0", "q5_1", "q8_1"],
        help="é‡åŒ–æ–¹æ³• (é»˜è®¤: f16)",
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=2048,
        help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 2048)",
    )
    args = parser.parse_args()

    checkpoint_path = args.checkpoint
    output_dir = args.output or f"{checkpoint_path}_gguf"
    quantization_method = args.quantization

    # éªŒè¯ checkpoint å­˜åœ¨
    if not os.path.isdir(checkpoint_path):
        print(f"âŒ é”™è¯¯: checkpoint ç›®å½•ä¸å­˜åœ¨: {checkpoint_path}")
        sys.exit(1)

    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    weights_files = ["adapter_model.safetensors", "adapter_model.bin"]
    
    has_config = os.path.isfile(os.path.join(checkpoint_path, "adapter_config.json"))
    has_weights = any(os.path.isfile(os.path.join(checkpoint_path, f)) for f in weights_files)
    
    if not has_config or not has_weights:
        print(f"âŒ é”™è¯¯: æ— æ•ˆçš„ checkpoint ç›®å½•: {checkpoint_path}")
        print(f"   éœ€è¦ adapter_config.json å’Œ adapter_model.safetensors/bin")
        sys.exit(1)

    print(f"ğŸ“¦ Checkpoint è·¯å¾„: {checkpoint_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ é‡åŒ–æ–¹æ³•: {quantization_method}")
    print()

    # å¯¼å…¥ä¾èµ–
    print("â³ åŠ è½½ä¾èµ–...")
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    import json

    # è¯»å– adapter_config è·å– base_model è·¯å¾„
    adapter_config_path = os.path.join(checkpoint_path, "adapter_config.json")
    with open(adapter_config_path, "r") as f:
        adapter_config = json.load(f)
    
    base_model_name = adapter_config.get("base_model_name_or_path", "")
    print(f"ğŸ“Œ åŸºç¡€æ¨¡å‹: {base_model_name}")

    # åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"â³ åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)

    # åŠ è½½ LoRA é€‚é…å™¨
    print(f"â³ åŠ è½½ LoRA é€‚é…å™¨...")
    model = PeftModel.from_pretrained(base_model, checkpoint_path)

    # åˆå¹¶ LoRA æƒé‡
    print(f"â³ åˆå¹¶ LoRA æƒé‡...")
    model = model.merge_and_unload()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    merged_dir = output_dir + "_merged"
    os.makedirs(merged_dir, exist_ok=True)

    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"â³ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {merged_dir}")
    model.save_pretrained(merged_dir, safe_serialization=True)
    tokenizer.save_pretrained(merged_dir)
    print(f"âœ… åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {merged_dir}")

    # ä½¿ç”¨ llama.cpp è½¬æ¢ä¸º GGUF
    print()
    print(f"â³ æ­£åœ¨è½¬æ¢ä¸º GGUF ({quantization_method})...")
    
    # å°è¯•å¯¼å…¥å¹¶ä½¿ç”¨ unsloth çš„ GGUF è½¬æ¢ï¼ˆä»åˆå¹¶åçš„æ¨¡å‹ï¼‰
    try:
        from unsloth import FastLanguageModel
        
        # é‡æ–°åŠ è½½åˆå¹¶åçš„æ¨¡å‹ï¼ˆä½¿ç”¨ unslothï¼‰
        print("â³ ä½¿ç”¨ Unsloth åŠ è½½åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œ GGUF è½¬æ¢...")
        merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(
            model_name=merged_dir,
            max_seq_length=args.max_seq_length,
            load_in_4bit=False,
            fast_inference=False,
        )
        
        os.makedirs(output_dir, exist_ok=True)
        merged_model.save_pretrained_gguf(
            output_dir,
            merged_tokenizer,
            quantization_method=quantization_method,
        )
        print(f"âœ… GGUF è½¬æ¢å®Œæˆï¼")
        
    except Exception as e:
        print(f"âš ï¸ Unsloth GGUF è½¬æ¢å¤±è´¥: {e}")
        print()
        print("è¯·æ‰‹åŠ¨ä½¿ç”¨ llama.cpp è¿›è¡Œè½¬æ¢:")
        print(f"  1. git clone https://github.com/ggerganov/llama.cpp")
        print(f"  2. cd llama.cpp && pip install -r requirements.txt")
        print(f"  3. python convert_hf_to_gguf.py {merged_dir} --outtype {quantization_method}")
        print()
        print(f"âœ… åˆå¹¶åçš„ HuggingFace æ¨¡å‹å·²ä¿å­˜åˆ°: {merged_dir}")
        return

    # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
    print()
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    for f in os.listdir(output_dir):
        filepath = os.path.join(output_dir, f)
        if os.path.isfile(filepath):
            size_mb = os.path.getsize(filepath) / (1024 * 1024)
            print(f"   - {f} ({size_mb:.2f} MB)")


if __name__ == "__main__":
    main()
