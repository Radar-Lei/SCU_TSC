Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
/root/autodl-tmp/SCU_TSC/upload_model_to_hub.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.

Please restructure your imports with 'import unsloth' at the top of your file.
  from unsloth import FastLanguageModel
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
INFO 10-29 09:25:23 [__init__.py:216] Automatically detected platform cuda.
🦥 Unsloth Zoo will now patch everything to make training faster!
============================================================
开始加载微调模型...
============================================================

[步骤1] 加载基础模型...
INFO 10-29 09:25:29 [vllm_utils.py:694] Unsloth: Patching vLLM v1 graph capture
Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'
==((====))==  Unsloth 2025.10.10: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.0.
   \\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.357 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+00a7a5f.d20251027. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: vLLM loading unsloth/Qwen3-4B-Instruct-2507 with actual GPU utilization = 88.33%
Unsloth: Your GPU has CUDA compute capability 12.0 with VRAM = 31.36 GB.
Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 288.
Unsloth: vLLM's KV Cache can use up to 20.75 GB. Also swap space = 6 GB.
WARNING 10-29 09:25:36 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.
Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.
INFO 10-29 09:25:36 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 1024, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8832892780117879, 'max_num_batched_tokens': 2048, 'max_num_seqs': 288, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {"level":3,"debug_dump_path":"","cache_dir":"","backend":"inductor","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":null,"inductor_compile_config":{"epilogue_fusion":true,"max_autotune":false,"shape_padding":true,"trace.enabled":false,"triton.cudagraphs":true,"debug":false,"dce":true,"memory_planning":true,"coordinate_descent_tuning":false,"trace.graph_diagram":false,"compile_threads":32,"group_fusion":true,"disable_progress":false,"verbose_progress":true,"triton.multi_kernel":0,"triton.use_block_ptr":true,"triton.enable_persistent_tma_matmul":true,"triton.autotune_at_compile_time":false,"triton.cooperative_reductions":false,"cuda.compile_opt_level":"-O2","cuda.enable_cuda_lto":true,"combo_kernels":false,"benchmark_combo_kernel":true,"combo_kernel_foreach_dynamic_shapes":true,"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":2,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":null,"cudagraph_copy_inputs":false,"full_cuda_graph":true,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":null,"local_cache_dir":null}, 'model': 'unsloth/Qwen3-4B-Instruct-2507'}
INFO 10-29 09:25:38 [model.py:547] Resolved architecture: Qwen3ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 10-29 09:25:38 [model.py:1510] Using max model len 1024
INFO 10-29 09:25:38 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 10-29 09:25:38 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.
INFO 10-29 09:25:40 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='unsloth/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"inductor","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"epilogue_fusion":true,"max_autotune":false,"shape_padding":true,"trace.enabled":false,"triton.cudagraphs":true,"debug":false,"dce":true,"memory_planning":true,"coordinate_descent_tuning":false,"trace.graph_diagram":false,"compile_threads":32,"group_fusion":true,"disable_progress":false,"verbose_progress":true,"triton.multi_kernel":0,"triton.use_block_ptr":true,"triton.enable_persistent_tma_matmul":true,"triton.autotune_at_compile_time":false,"triton.cooperative_reductions":false,"cuda.compile_opt_level":"-O2","cuda.enable_cuda_lto":true,"combo_kernels":false,"benchmark_combo_kernel":true,"combo_kernel_foreach_dynamic_shapes":true,"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":2,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":true,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 10-29 09:25:40 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 10-29 09:25:40 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 10-29 09:25:40 [gpu_model_runner.py:2602] Starting to load model unsloth/Qwen3-4B-Instruct-2507...
INFO 10-29 09:25:41 [gpu_model_runner.py:2634] Loading model from scratch...
INFO 10-29 09:25:41 [cuda.py:366] Using Flash Attention backend on V1 engine.
INFO 10-29 09:25:41 [weight_utils.py:392] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.02it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]

INFO 10-29 09:25:44 [default_loader.py:267] Loading weights took 1.56 seconds
INFO 10-29 09:25:44 [punica_selector.py:19] Using PunicaWrapperGPU.
INFO 10-29 09:25:44 [gpu_model_runner.py:2653] Model loading took 7.7296 GiB and 2.978054 seconds
INFO 10-29 09:25:52 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/74e02e75d9/rank_0_0/backbone for vLLM's torch.compile
INFO 10-29 09:25:52 [backends.py:559] Dynamo bytecode transform time: 7.59 s
INFO 10-29 09:25:55 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.486 s
INFO 10-29 09:25:57 [monitor.py:34] torch.compile takes 7.59 s in total
INFO 10-29 09:25:58 [gpu_worker.py:298] Available KV cache memory: 18.38 GiB
INFO 10-29 09:25:58 [kv_cache_utils.py:1087] GPU KV cache size: 133,856 tokens
INFO 10-29 09:25:58 [kv_cache_utils.py:1091] Maximum concurrency for 1,024 tokens per request: 130.72x
INFO 10-29 09:25:58 [vllm_utils.py:699] Unsloth: Running patched vLLM v1 `capture_model`.
WARNING 10-29 09:25:58 [gpu_model_runner.py:3643] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|▏         | 1/67 [00:00<00:07,  8.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 3/67 [00:00<00:05, 11.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 5/67 [00:00<00:05, 12.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|█         | 7/67 [00:00<00:04, 12.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 9/67 [00:00<00:04, 12.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▋        | 11/67 [00:00<00:04, 12.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:01<00:04, 12.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:01<00:04, 12.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 17/67 [00:01<00:03, 12.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 19/67 [00:01<00:03, 12.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:01<00:03, 12.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:01<00:03, 12.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 25/67 [00:02<00:03, 12.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:02<00:03, 12.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 29/67 [00:02<00:03, 12.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▋     | 31/67 [00:02<00:02, 12.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:02<00:02, 12.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|█████▏    | 35/67 [00:02<00:02, 12.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▌    | 37/67 [00:02<00:02, 12.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 39/67 [00:03<00:02, 12.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 41/67 [00:03<00:02, 12.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:03<00:01, 12.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 45/67 [00:03<00:01, 12.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|███████   | 47/67 [00:03<00:01, 11.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 49/67 [00:03<00:01, 12.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▌  | 51/67 [00:04<00:01, 12.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:04<00:01, 12.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 55/67 [00:04<00:00, 12.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|████████▌ | 57/67 [00:04<00:00, 12.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 59/67 [00:04<00:00, 12.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:04<00:00, 12.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 63/67 [00:05<00:00, 12.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 65/67 [00:05<00:00, 12.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 11.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.36it/s]
Capturing CUDA graphs (decode, FULL):   0%|          | 0/39 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/39 [00:00<00:06,  6.20it/s]Capturing CUDA graphs (decode, FULL):   8%|▊         | 3/39 [00:00<00:03, 10.65it/s]Capturing CUDA graphs (decode, FULL):  13%|█▎        | 5/39 [00:00<00:02, 12.06it/s]Capturing CUDA graphs (decode, FULL):  18%|█▊        | 7/39 [00:00<00:02, 12.77it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 9/39 [00:00<00:02, 12.96it/s]Capturing CUDA graphs (decode, FULL):  28%|██▊       | 11/39 [00:00<00:02, 13.23it/s]Capturing CUDA graphs (decode, FULL):  33%|███▎      | 13/39 [00:01<00:01, 13.29it/s]Capturing CUDA graphs (decode, FULL):  38%|███▊      | 15/39 [00:01<00:01, 13.33it/s]Capturing CUDA graphs (decode, FULL):  44%|████▎     | 17/39 [00:01<00:01, 12.77it/s]Capturing CUDA graphs (decode, FULL):  49%|████▊     | 19/39 [00:01<00:01, 13.02it/s]Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 21/39 [00:01<00:01, 13.20it/s]Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 23/39 [00:01<00:01, 13.31it/s]Capturing CUDA graphs (decode, FULL):  64%|██████▍   | 25/39 [00:01<00:01, 13.41it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▉   | 27/39 [00:02<00:00, 13.34it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 29/39 [00:02<00:00, 13.50it/s]Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 31/39 [00:02<00:00, 13.61it/s]Capturing CUDA graphs (decode, FULL):  85%|████████▍ | 33/39 [00:02<00:00, 13.63it/s]Capturing CUDA graphs (decode, FULL):  90%|████████▉ | 35/39 [00:02<00:00, 13.57it/s]Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 37/39 [00:02<00:00, 13.67it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 39/39 [00:02<00:00, 13.78it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 39/39 [00:02<00:00, 13.15it/s]
INFO 10-29 09:26:06 [gpu_model_runner.py:3480] Graph capturing finished in 8 secs, took 0.68 GiB
INFO 10-29 09:26:06 [vllm_utils.py:706] Unsloth: Patched vLLM v1 graph capture finished in 8 secs.
INFO 10-29 09:26:07 [core.py:210] init engine (profile, create kv cache, warmup model) took 23.00 seconds
INFO 10-29 09:26:08 [llm.py:306] Supported_tasks: ('generate',)
Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'attention_norm', 'norm2', 'layer_norm1', 'post_feedforward_layernorm', 'k_norm', 'post_layernorm', 'input_layernorm', 'q_norm', 'norm1', 'layer_norm2', 'pre_feedforward_layernorm', 'ffn_norm']
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.16it/s]
/root/autodl-tmp/SCU_TSC/.venv/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:949: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!
  warnings.warn("Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!")
Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'attention_norm', 'norm2', 'layer_norm1', 'cross_attn_input_layernorm', 'post_feedforward_layernorm', 'k_norm', 'cross_attn_post_attention_layernorm', 'post_layernorm', 'input_layernorm', 'q_norm', 'norm1', 'layer_norm2', 'pre_feedforward_layernorm', 'ffn_norm']
✓ 基础模型加载成功

[步骤2] 加载 LoRA 权重...
✓ LoRA 权重加载成功

[步骤3] 合并 LoRA 权重到基础模型...
✓ 权重合并成功

[步骤4] 为推理优化模型...
✓ 推理优化完成

[步骤5] 发布模型到 Hugging Face Hub...
  - 正在转换为 GGUF 格式...
  - 目标 Repo: DavidRay93/Qwen3-4B-TSC-GRPO-Test
  - 量化方法: f16
Unsloth: Converting model to GGUF format...
Unsloth: Merging model weights to 16-bit format...
Unsloth: Converting to GGUF format...
==((====))==  Unsloth: Conversion from HF to GGUF information
   \\   /|    [0] Installing llama.cpp might take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GGUF bf16 might take 3 minutes.
\        /    [2] Converting GGUF bf16 to ['f16'] might take 10 minutes each.
 "-____-"     In total, you will have to wait at least 16 minutes.

Unsloth: Installing llama.cpp. This might take 3 minutes...
Unsloth: Updating system package directories
Unsloth: All required system packages already installed!
Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes
Unsloth: Cloning llama.cpp repository
Unsloth: Install GGUF and other packages
Unsloth: Successfully installed llama.cpp!
Unsloth: Preparing converter script...
Unsloth: [1] Converting model into bf16 GGUF format.
This might take 3 minutes...
✗ 模型发布失败: Failed to convert model to GGUF: Unsloth: GGUF conversion failed: Unsloth: `config.json` does not exist inside `/tmp/unsloth_gguf_uhj93ai1`.

错误排查提示:
  1. 检查 HF_TOKEN 是否正确
  2. 检查网络连接
  3. 确保有足够的磁盘空间在临时位置
[rank0]:[W1029 09:27:48.247970779 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
